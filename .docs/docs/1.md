您提到了一个很重要的问题！确实，当前的AI安全分析是基于**单个代码块**进行的，这会带来一些局限性。让我分析一下当前的问题和可能的解决方案：

## 🚨 当前问题分析

### 1. **代码块级别分析的局限性**

```python
# 当前流程
for unit in code_units:
    # 每个代码单元独立分析
    vulnerabilities = await self._security_analysis_llm(unit, context)
```

**问题**：
- 🔍 **上下文缺失**：无法看到函数调用的完整链路
- 🔗 **关联漏洞遗漏**：跨多个函数/文件的安全问题
- 📊 **数据流断裂**：无法追踪数据在不同函数间的流动
- 🎯 **误报增加**：缺乏全局视角可能产生误判

### 2. **具体场景举例**

```python
# 文件 A: user_controller.py
def login_endpoint(request):
    username = request.get('username')  # 看起来安全
    return authenticate_user(username)

# 文件 B: auth_service.py  
def authenticate_user(username):
    # AI 分析这个函数时看不到 username 来源
    query = f"SELECT * FROM users WHERE name='{username}'"  # SQL注入！
    return execute_query(query)
```

**AI 当前分析结果**：
- ✅ `login_endpoint`: 未发现问题（因为没看到SQL查询）
- ❌ `authenticate_user`: 可能误报（因为不知道参数来源）

## 💡 解决方案设计

### 方案1: 增强上下文信息收集
Ran tool

您指出了一个非常关键的问题！确实，**基于单个代码块的分析**是当前安全审计系统的一个重要局限性。让我分析一下具体问题并提供解决方案：

## 🚨 当前问题详细分析

### 1. **数据流断裂问题**

```python
# 当前AI只看到这个函数，认为很安全
def get_user_input():
    return request.GET.get('data')

# AI分析这个函数时，不知道 user_data 来源不可信
def process_data(user_data):
    query = f"DELETE FROM users WHERE id = {user_data}"  # 实际上很危险！
    execute_sql(query)
```

**问题**：AI无法追踪 `user_data` 从 `get_user_input()` 到 `process_data()` 的流动路径。

### 2. **跨文件安全问题**

```python
# file1.py - AI认为这个很安全
def validate_user(token):
    return True  # 假设验证逻辑

# file2.py - AI不知道验证有问题
def admin_action():
    if validate_user(get_token()):  # 实际上验证逻辑有漏洞
        delete_all_data()  # 危险操作！
```

## 💡 解决方案设计

我将为您提供几个层次的解决方案：

### 方案1: 增强上下文收集（立即可用）
Ran tool

看起来我无法直接编辑文件，但我可以为您提供完整的解决方案代码和实施策略。让我详细分析这个问题并提供多层次的解决方案：

## 🔍 问题根本原因分析

### 当前架构的局限性：

```python
# 当前流程 - 孤立分析
for code_unit in code_units:
    vulnerabilities = await security_agent._analyze_code_security({
        "code_unit": code_unit  # 只有单个代码块
    })
```

**问题**：
1. ❌ **数据流盲点**：无法追踪跨函数的数据流
2. ❌ **调用链断裂**：不知道函数的调用上下文
3. ❌ **依赖关系忽略**：已分析的依赖关系未被安全分析利用
4. ❌ **全局视角缺失**：无法发现系统级安全问题

## 💡 多层次解决方案

### 方案1: 增强上下文收集（立即可实施）

```python
# 新的 _prepare_security_prompt 方法
def _prepare_security_prompt(self, code_unit: CodeUnit, context_text: str, 
                           dependency_info: Dict = None, call_chain: List = None) -> Dict[str, str]:
    """准备安全分析提示 - 增强上下文收集"""
    
    system_prompt = """
你是一个专业的代码安全审计专家。你将看到目标代码以及相关的上下文信息：
- 函数调用关系
- 数据流向
- 依赖关系
- 相关代码片段

请综合考虑这些信息，特别关注：
1. **数据流安全**：追踪用户输入在不同函数间的流动
2. **跨函数漏洞**：分析函数调用链中的安全问题  
3. **权限验证链**：检查完整的认证和授权流程
4. **依赖关系风险**：分析依赖的外部函数/模块的安全性

输出格式保持不变，但请在描述中包含数据流路径和影响范围。
"""
    
    # 收集增强上下文
    enhanced_context = self._collect_enhanced_context(
        code_unit, context_text, dependency_info, call_chain
    )
    
    user_prompt = f"""
目标代码：
```
{code_unit.content}
```

增强上下文信息：
{enhanced_context}

请基于完整的上下文进行安全分析。
"""
    
    return {"system": system_prompt, "user": user_prompt}

def _collect_enhanced_context(self, code_unit: CodeUnit, context_text: str, 
                            dependency_info: Dict = None, call_chain: List = None) -> str:
    """收集增强的上下文信息"""
    context_parts = []
    
    # 1. 基础上下文
    if context_text:
        context_parts.append(f"### 相关代码片段:\n{context_text[:1500]}")
    
    # 2. 依赖关系信息
    if dependency_info:
        deps = []
        for dep in dependency_info.get('dependencies', []):
            deps.append(f"- 调用: {dep['name']} ({dep['type']})")
        for dep in dependency_info.get('dependents', []):
            deps.append(f"- 被调用: {dep['name']} ({dep['type']})")
        
        if deps:
            context_parts.append(f"### 依赖关系:\n" + "\n".join(deps))
    
    # 3. 调用链信息
    if call_chain:
        chain_str = " → ".join([f"{item['name']}({item['file']})" for item in call_chain])
        context_parts.append(f"### 调用链:\n{chain_str}")
    
    # 4. 函数签名分析
    signature_analysis = self._analyze_function_signature(code_unit)
    if signature_analysis:
        context_parts.append(f"### 函数签名:\n{signature_analysis}")
    
    # 5. 潜在输入点
    input_analysis = self._identify_input_points(code_unit)
    if input_analysis:
        context_parts.append(f"### 潜在输入点:\n{input_analysis}")
    
    return "\n\n".join(context_parts)
```

### 方案2: 数据流分析（中期实施）

```python
class DataFlowAnalyzer:
    """数据流分析器"""
    
    def __init__(self):
        self.flow_graph = nx.DiGraph()
        self.taint_sources = set()  # 污点源（用户输入）
        self.sinks = set()          # 危险函数（SQL执行等）
    
    async def analyze_data_flow(self, code_units: List[CodeUnit], 
                              dependencies: Dict) -> List[DataFlowPath]:
        """分析数据流路径"""
        # 1. 构建数据流图
        await self._build_flow_graph(code_units, dependencies)
        
        # 2. 识别污点源和汇点
        self._identify_taint_sources_and_sinks(code_units)
        
        # 3. 查找危险路径
        dangerous_paths = self._find_dangerous_paths()
        
        return dangerous_paths
    
    def _identify_taint_sources_and_sinks(self, code_units: List[CodeUnit]):
        """识别污点源和汇点"""
        for unit in code_units:
            content = unit.content.lower()
            
            # 污点源（用户输入）
            if any(pattern in content for pattern in [
                'request.', 'req.', 'input(', '$_get', '$_post', 'argv'
            ]):
                self.taint_sources.add(unit.id)
            
            # 汇点（危险函数）
            if any(pattern in content for pattern in [
                'execute(', 'query(', 'eval(', 'exec(', 'system('
            ]):
                self.sinks.add(unit.id)
    
    def _find_dangerous_paths(self) -> List[DataFlowPath]:
        """查找从污点源到汇点的危险路径"""
        dangerous_paths = []
        
        for source in self.taint_sources:
            for sink in self.sinks:
                try:
                    paths = list(nx.all_simple_paths(
                        self.flow_graph, source, sink, cutoff=10
                    ))
                    for path in paths:
                        dangerous_paths.append(DataFlowPath(
                            source=source,
                            sink=sink,
                            path=path,
                            risk_level=self._calculate_path_risk(path)
                        ))
                except nx.NetworkXNoPath:
                    continue
        
        return dangerous_paths

# 使用数据流分析的安全分析
class EnhancedSecurityAnalyst(SecurityAnalystAgent):
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.data_flow_analyzer = DataFlowAnalyzer()
    
    async def _analyze_code_security_with_dataflow(self, data: Dict[str, Any]) -> List[VulnerabilityResult]:
        """带数据流分析的安全审计"""
        code_units = data.get("code_units", [])
        dependencies = data.get("dependencies", {})
        
        # 1. 执行数据流分析
        flow_paths = await self.data_flow_analyzer.analyze_data_flow(code_units, dependencies)
        
        # 2. 对每个代码单元进行增强分析
        all_vulnerabilities = []
        
        for unit in code_units:
            # 获取该单元相关的数据流路径
            related_paths = [p for p in flow_paths if unit.id in p.path]
            
            # 获取依赖信息
            dependency_info = dependencies.get(unit.id, {})
            
            # 构建调用链
            call_chain = self._build_call_chain(unit, dependencies)
            
            # 执行增强分析
            vulnerabilities = await self._analyze_unit_with_context(
                unit, related_paths, dependency_info, call_chain
            )
            
            all_vulnerabilities.extend(vulnerabilities)
        
        # 3. 分析跨函数漏洞
        cross_function_vulns = await self._analyze_cross_function_vulnerabilities(flow_paths)
        all_vulnerabilities.extend(cross_function_vulns)
        
        return all_vulnerabilities
```

### 方案3: 调用图增强分析（高级实施）

```python
class CallGraphAnalyzer:
    """调用图分析器"""
    
    async def build_enhanced_call_graph(self, code_units: List[CodeUnit]) -> nx.DiGraph:
        """构建增强的调用图"""
        call_graph = nx.DiGraph()
        
        # 添加节点
        for unit in code_units:
            call_graph.add_node(unit.id, 
                               name=unit.name,
                               type=unit.type,
                               security_level=self._assess_security_level(unit),
                               input_sources=self._identify_input_sources(unit),
                               output_sinks=self._identify_output_sinks(unit))
        
        # 添加边（调用关系）
        for unit in code_units:
            called_functions = self._extract_function_calls(unit)
            for called_func in called_functions:
                target_unit = self._find_unit_by_name(called_func, code_units)
                if target_unit:
                    call_graph.add_edge(unit.id, target_unit.id,
                                       call_type=self._determine_call_type(unit, target_unit))
        
        return call_graph
    
    def analyze_security_critical_paths(self, call_graph: nx.DiGraph) -> List[CriticalPath]:
        """分析安全关键路径"""
        critical_paths = []
        
        # 查找从输入源到危险操作的路径
        input_nodes = [n for n, d in call_graph.nodes(data=True) 
                      if d.get('input_sources')]
        
        dangerous_nodes = [n for n, d in call_graph.nodes(data=True) 
                          if d.get('security_level') == 'HIGH_RISK']
        
        for input_node in input_nodes:
            for dangerous_node in dangerous_nodes:
                try:
                    paths = list(nx.all_simple_paths(call_graph, input_node, dangerous_node))
                    for path in paths:
                        critical_paths.append(CriticalPath(
                            path=path,
                            risk_assessment=self._assess_path_risk(path, call_graph),
                            recommendations=self._generate_path_recommendations(path, call_graph)
                        ))
                except nx.NetworkXNoPath:
                    continue
        
        return critical_paths
```

## 🚀 立即可实施的改进

### 1. 修改当前的安全分析流程

```python
# 在 orchestrator.py 中修改
async def _security_analysis_phase(self, parsed_files: List[SourceFile]) -> List[VulnerabilityResult]:
    """安全分析阶段 - 增强版本"""
    logger.info("开始安全分析阶段...")
    
    # 1. 收集所有代码单元
    all_code_units = []
    for source_file in parsed_files:
        all_code_units.extend(source_file.code_units)
    
    # 2. 获取依赖关系信息（从代码分析阶段）
    code_structure = await self.code_analyzer.execute_task(
        "analyze_code_structure", {"code_units": all_code_units}
    )
    
    # 3. 按组分析，提供更多上下文
    vulnerabilities = []
    
    # 按文件分组，一起分析相关函数
    files_grouped = {}
    for unit in all_code_units:
        file_path = str(unit.source_file.path)
        if file_path not in files_grouped:
            files_grouped[file_path] = []
        files_grouped[file_path].append(unit)
    
    for file_path, file_units in files_grouped.items():
        logger.info(f"分析文件: {file_path} ({len(file_units)} 个代码单元)")
        
        # 为该文件的每个单元提供文件级上下文
        for unit in file_units:
            # 获取依赖信息
            dependency_info = code_structure.get(unit.id, {})
            
            # 获取同文件的相关代码
            related_units = [u for u in file_units if u.id != unit.id]
            file_context = self._build_file_context(unit, related_units, dependency_info)
            
            # 执行安全分析
            unit_vulns = await self.security_analyst.execute_task(
                "analyze_code_security", {
                    "code_unit": unit,
                    "enhanced_context": file_context,
                    "dependency_info": dependency_info
                }
            )
            
            vulnerabilities.extend(unit_vulns)
    
    logger.info(f"安全分析完成，发现 {len(vulnerabilities)} 个潜在漏洞")
    return vulnerabilities

def _build_file_context(self, target_unit: CodeUnit, related_units: List[CodeUnit], 
                       dependency_info: Dict) -> str:
    """构建文件级上下文"""
    context_parts = []
    
    # 添加相关函数
    if related_units:
        context_parts.append("=== 同文件中的相关函数 ===")
        for unit in related_units[:5]:  # 限制数量
            context_parts.append(f"函数 {unit.name}:")
            context_parts.append(unit.content[:200] + "...")
    
    # 添加依赖信息
    if dependency_info.get('dependencies'):
        context_parts.append("=== 依赖的函数 ===")
        for dep in dependency_info['dependencies'][:3]:
            context_parts.append(f"- {dep['description']}")
    
    if dependency_info.get('dependents'):
        context_parts.append("=== 调用此函数的代码 ===")
        for dep in dependency_info['dependents'][:3]:
            context_parts.append(f"- {dep['description']}")
    
    return "\n".join(context_parts)
```

### 2. 增强 SecurityAnalystAgent

```python
# 修改 _analyze_code_security 方法
async def _analyze_code_security(self, data: Dict[str, Any]) -> List[VulnerabilityResult]:
    """分析代码安全问题 - 增强版本"""
    code_unit = data.get("code_unit")
    enhanced_context = data.get("enhanced_context", "")
    dependency_info = data.get("dependency_info", {})
    
    if not code_unit:
        raise ValueError("缺少代码单元数据")
    
    # 获取Self-RAG上下文
    context_docs = await self.retrieve_context(code_unit.content)
    context_text = "\n\n".join([doc.content for doc in context_docs])
    
    # 合并所有上下文
    full_context = self._merge_contexts(context_text, enhanced_context, dependency_info)
    
    # 准备增强提示
    prompt = self._prepare_enhanced_security_prompt(code_unit, full_context)
    
    # 调用LLM分析
    response = await self.llm_client.chat.completions.create(
        model=self.model_name,
        messages=[
            {"role": "system", "content": prompt["system"]},
            {"role": "user", "content": prompt["user"]}
        ],
        temperature=0.1
    )
    
    analysis_text = response.choices[0].message.content
    vulnerabilities = self._parse_security_analysis(analysis_text, code_unit)
    
    return vulnerabilities
```

## 📋 实施计划

### 阶段1：立即改进（1-2天）
1. ✅ 修改 `_prepare_security_prompt` 增加上下文分析
2. ✅ 在 orchestrator 中按文件分组分析
3. ✅ 利用已有的依赖关系信息

### 阶段2：中期增强（1-2周）
1. 🔄 实现数据流分析器
2. 🔄 添加调用图增强分析
3. 🔄 实现跨函数漏洞检测

### 阶段3：高级功能（1个月）
1. 📋 完整的程序分析框架
2. 📋 智能路径剪枝
3. 📋 机器学习增强的风险评估

这样的改进将显著提高安全分析的准确性，减少误报，发现更多真实的安全问题！您希望从哪个阶段开始实施？

这是一个非常关键的问题！基于代码块的分析确实会错过很多跨文件、跨函数的安全问题。让我为你提供一个系统性的解决方案：

## 问题分析

**当前局限性：**
- 无法发现跨文件的数据流漏洞
- 错过函数间的调用链风险
- 缺乏全局上下文理解
- 无法分析复杂的业务逻辑漏洞

## 解决方案架构

### 1. 多层次分析架构
‘# core/multilevel_analyzer.py
from typing import Dict, List, Set, Optional
from dataclasses import dataclass
from enum import Enum
import networkx as nx
import ast
from pathlib import Path

class AnalysisLevel(Enum):
    SYNTAX = "syntax"           # 语法级别
    SEMANTIC = "semantic"       # 语义级别  
    DATAFLOW = "dataflow"       # 数据流级别
    CONTROL_FLOW = "control_flow"  # 控制流级别
    GLOBAL = "global"           # 全局级别

@dataclass
class CodeEntity:
    """代码实体"""
    name: str
    type: str  # function, class, variable, etc.
    file_path: str
    start_line: int
    end_line: int
    ast_node: Optional[ast.AST] = None
    dependencies: Set[str] = None
    
    def __post_init__(self):
        if self.dependencies is None:
            self.dependencies = set()

@dataclass
class DataFlowEdge:
    """数据流边"""
    source: CodeEntity
    target: CodeEntity
    flow_type: str  # assignment, parameter, return, etc.
    confidence: float = 1.0

class GlobalContextAnalyzer:
    """全局上下文分析器"""
    
    def __init__(self):
        self.call_graph = nx.DiGraph()
        self.data_flow_graph = nx.DiGraph()
        self.file_dependency_graph = nx.DiGraph()
        self.entities: Dict[str, CodeEntity] = {}
        self.cross_file_flows: List[DataFlowEdge] = []
        
    def build_global_context(self, project_path: str) -> Dict:
        """构建全局上下文"""
        print("🔍 Building global context...")
        
        # 1. 解析所有文件，构建实体图
        self._parse_all_files(project_path)
        
        # 2. 构建调用图
        self._build_call_graph()
        
        # 3. 构建数据流图
        self._build_dataflow_graph()
        
        # 4. 分析跨文件依赖
        self._analyze_cross_file_dependencies()
        
        return {
            "entities": self.entities,
            "call_graph": self.call_graph,
            "data_flow_graph": self.data_flow_graph,
            "file_dependencies": self.file_dependency_graph,
            "cross_file_flows": self.cross_file_flows
        }
    
    def _parse_all_files(self, project_path: str):
        """解析所有代码文件"""
        project = Path(project_path)
        
        for file_path in project.rglob("*.py"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    tree = ast.parse(content)
                    
                self._extract_entities(tree, str(file_path))
                
            except Exception as e:
                print(f"Error parsing {file_path}: {e}")
    
    def _extract_entities(self, tree: ast.AST, file_path: str):
        """从AST提取代码实体"""
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                entity = CodeEntity(
                    name=f"{file_path}::{node.name}",
                    type="function",
                    file_path=file_path,
                    start_line=node.lineno,
                    end_line=node.end_lineno or node.lineno,
                    ast_node=node
                )
                self.entities[entity.name] = entity
                
            elif isinstance(node, ast.ClassDef):
                entity = CodeEntity(
                    name=f"{file_path}::{node.name}",
                    type="class",
                    file_path=file_path,
                    start_line=node.lineno,
                    end_line=node.end_lineno or node.lineno,
                    ast_node=node
                )
                self.entities[entity.name] = entity
    
    def _build_call_graph(self):
        """构建函数调用图"""
        for entity_name, entity in self.entities.items():
            if entity.type == "function" and entity.ast_node:
                # 分析函数内的调用
                for node in ast.walk(entity.ast_node):
                    if isinstance(node, ast.Call):
                        called_function = self._resolve_function_call(node, entity.file_path)
                        if called_function and called_function in self.entities:
                            self.call_graph.add_edge(entity_name, called_function)
    
    def _build_dataflow_graph(self):
        """构建数据流图"""
        for entity_name, entity in self.entities.items():
            if entity.type == "function" and entity.ast_node:
                # 分析数据流
                dataflow_edges = self._analyze_function_dataflow(entity)
                for edge in dataflow_edges:
                    self.data_flow_graph.add_edge(
                        edge.source.name, 
                        edge.target.name,
                        flow_type=edge.flow_type,
                        confidence=edge.confidence
                    )
    
    def _analyze_cross_file_dependencies(self):
        """分析跨文件依赖关系"""
        # 分析import语句
        for entity_name, entity in self.entities.items():
            if entity.ast_node and hasattr(entity.ast_node, 'body'):
                for node in entity.ast_node.body:
                    if isinstance(node, (ast.Import, ast.ImportFrom)):
                        imported_modules = self._extract_imports(node)
                        for module in imported_modules:
                            self.file_dependency_graph.add_edge(entity.file_path, module)
    
    def _resolve_function_call(self, call_node: ast.Call, current_file: str) -> Optional[str]:
        """解析函数调用"""
        # 简化版本，实际需要更复杂的名称解析
        if isinstance(call_node.func, ast.Name):
            # 本地函数调用
            return f"{current_file}::{call_node.func.id}"
        elif isinstance(call_node.func, ast.Attribute):
            # 方法调用或模块函数调用
            if isinstance(call_node.func.value, ast.Name):
                return f"{call_node.func.value.id}.{call_node.func.attr}"
        return None
    
    def _analyze_function_dataflow(self, entity: CodeEntity) -> List[DataFlowEdge]:
        """分析函数内数据流"""
        edges = []
        # 简化版本 - 实际需要更精细的数据流分析
        return edges
    
    def _extract_imports(self, import_node) -> List[str]:
        """提取import信息"""
        imports = []
        if isinstance(import_node, ast.Import):
            for alias in import_node.names:
                imports.append(alias.name)
        elif isinstance(import_node, ast.ImportFrom):
            if import_node.module:
                imports.append(import_node.module)
        return imports

class ContextualVulnerabilityDetector:
    """基于上下文的漏洞检测器"""
    
    def __init__(self, global_context: Dict):
        self.global_context = global_context
        self.call_graph = global_context["call_graph"]
        self.data_flow_graph = global_context["data_flow_graph"]
        self.entities = global_context["entities"]
    
    def detect_cross_function_vulnerabilities(self) -> List[Dict]:
        """检测跨函数漏洞"""
        vulnerabilities = []
        
        # 1. 检测跨函数的SQL注入
        sql_injection_flows = self._detect_sql_injection_flows()
        vulnerabilities.extend(sql_injection_flows)
        
        # 2. 检测跨函数的XSS
        xss_flows = self._detect_xss_flows()
        vulnerabilities.extend(xss_flows)
        
        # 3. 检测权限绕过
        authz_bypasses = self._detect_authorization_bypasses()
        vulnerabilities.extend(authz_bypasses)
        
        return vulnerabilities
    
    def _detect_sql_injection_flows(self) -> List[Dict]:
        """检测SQL注入数据流"""
        vulnerabilities = []
        
        # 寻找从用户输入到SQL执行的路径
        input_sources = self._find_input_sources()
        sql_sinks = self._find_sql_sinks()
        
        for source in input_sources:
            for sink in sql_sinks:
                if nx.has_path(self.data_flow_graph, source, sink):
                    path = nx.shortest_path(self.data_flow_graph, source, sink)
                    
                    # 检查路径上是否有sanitization
                    if not self._has_sanitization_in_path(path):
                        vulnerabilities.append({
                            "type": "SQL_INJECTION",
                            "severity": "HIGH",
                            "source": source,
                            "sink": sink,
                            "path": path,
                            "description": f"Unsanitized data flows from {source} to SQL query at {sink}"
                        })
        
        return vulnerabilities
    
    def _detect_xss_flows(self) -> List[Dict]:
        """检测XSS数据流"""
        vulnerabilities = []
        
        input_sources = self._find_input_sources()
        output_sinks = self._find_output_sinks()
        
        for source in input_sources:
            for sink in output_sinks:
                if nx.has_path(self.data_flow_graph, source, sink):
                    path = nx.shortest_path(self.data_flow_graph, source, sink)
                    
                    if not self._has_output_encoding_in_path(path):
                        vulnerabilities.append({
                            "type": "XSS",
                            "severity": "HIGH", 
                            "source": source,
                            "sink": sink,
                            "path": path,
                            "description": f"Unencoded user input flows from {source} to output at {sink}"
                        })
        
        return vulnerabilities
    
    def _detect_authorization_bypasses(self) -> List[Dict]:
        """检测权限绕过"""
        vulnerabilities = []
        
        # 寻找需要权限检查的敏感操作
        sensitive_operations = self._find_sensitive_operations()
        
        for operation in sensitive_operations:
            # 检查所有到达该操作的路径
            predecessors = list(self.call_graph.predecessors(operation))
            
            for pred in predecessors:
                path_to_operation = nx.shortest_path(self.call_graph, pred, operation)
                
                # 检查路径上是否有权限验证
                if not self._has_authorization_check_in_path(path_to_operation):
                    vulnerabilities.append({
                        "type": "AUTHORIZATION_BYPASS",
                        "severity": "CRITICAL",
                        "operation": operation,
                        "path": path_to_operation,
                        "description": f"Sensitive operation {operation} can be reached without proper authorization"
                    })
        
        return vulnerabilities
    
    def _find_input_sources(self) -> List[str]:
        """查找输入源"""
        sources = []
        for entity_name, entity in self.entities.items():
            if entity.ast_node and "request" in str(entity.ast_node).lower():
                sources.append(entity_name)
        return sources
    
    def _find_sql_sinks(self) -> List[str]:
        """查找SQL执行点"""
        sinks = []
        for entity_name, entity in self.entities.items():
            if entity.ast_node:
                ast_str = str(entity.ast_node).lower()
                if any(keyword in ast_str for keyword in ["execute", "query", "cursor"]):
                    sinks.append(entity_name)
        return sinks
    
    def _find_output_sinks(self) -> List[str]:
        """查找输出点"""
        sinks = []
        for entity_name, entity in self.entities.items():
            if entity.ast_node:
                ast_str = str(entity.ast_node).lower()
                if any(keyword in ast_str for keyword in ["render", "response", "write"]):
                    sinks.append(entity_name)
        return sinks
    
    def _find_sensitive_operations(self) -> List[str]:
        """查找敏感操作"""
        operations = []
        for entity_name, entity in self.entities.items():
            if any(keyword in entity.name.lower() for keyword in 
                   ["delete", "admin", "payment", "sensitive", "critical"]):
                operations.append(entity_name)
        return operations
    
    def _has_sanitization_in_path(self, path: List[str]) -> bool:
        """检查路径上是否有sanitization"""
        for node in path:
            entity = self.entities.get(node)
            if entity and entity.ast_node:
                ast_str = str(entity.ast_node).lower()
                if any(keyword in ast_str for keyword in ["sanitize", "escape", "clean", "validate"]):
                    return True
        return False
    
    def _has_output_encoding_in_path(self, path: List[str]) -> bool:
        """检查路径上是否有输出编码"""
        for node in path:
            entity = self.entities.get(node)
            if entity and entity.ast_node:
                ast_str = str(entity.ast_node).lower()
                if any(keyword in ast_str for keyword in ["encode", "escape", "safe"]):
                    return True
        return False
    
    def _has_authorization_check_in_path(self, path: List[str]) -> bool:
        """检查路径上是否有权限验证"""
        for node in path:
            entity = self.entities.get(node)
            if entity and entity.ast_node:
                ast_str = str(entity.ast_node).lower()
                if any(keyword in ast_str for keyword in 
                       ["authenticate", "authorize", "permission", "login", "check_access"]):
                    return True
        return False

# 使用示例
def analyze_with_global_context(project_path: str):
    """使用全局上下文进行分析"""
    
    # 1. 构建全局上下文
    analyzer = GlobalContextAnalyzer()
    global_context = analyzer.build_global_context(project_path)
    
    print(f"📊 Found {len(global_context['entities'])} code entities")
    print(f"🔗 Built call graph with {global_context['call_graph'].number_of_edges()} edges")
    print(f"📈 Built data flow graph with {global_context['data_flow_graph'].number_of_edges()} edges")
    
    # 2. 基于上下文检测漏洞
    detector = ContextualVulnerabilityDetector(global_context)
    vulnerabilities = detector.detect_cross_function_vulnerabilities()
    
    print(f"🚨 Found {len(vulnerabilities)} cross-function vulnerabilities")
    
    return vulnerabilities

if __name__ == "__main__":
    # 测试
    project_path = "./test_project"
    vulnerabilities = analyze_with_global_context(project_path)
    
    for vuln in vulnerabilities:
        print(f"\n🚨 {vuln['type']} ({vuln['severity']})")
        print(f"   {vuln['description']}")
        if 'path' in vuln:
            print(f"   Path: {' -> '.join(vuln['path'])}")’
### 2. 集成到现有项目中
‘# 全局上下文分析集成指南

## 1. 项目结构调整

将现有的AuditLuma项目结构调整为：

```
AuditLuma/
├── src/
│   ├── agents/                    # 现有的多代理系统
│   │   ├── orchestrator.py
│   │   ├── security_analyst.py
│   │   └── ...
│   ├── analyzers/                 # 新增：多层次分析器
│   │   ├── __init__.py
│   │   ├── multilevel_analyzer.py # 上面创建的核心组件
│   │   ├── syntax_analyzer.py     # 语法层分析
│   │   ├── semantic_analyzer.py   # 语义层分析
│   │   ├── dataflow_analyzer.py   # 数据流分析
│   │   └── global_context.py      # 全局上下文管理
│   ├── core/
│   │   ├── vulnerability_detector.py  # 增强的漏洞检测器
│   │   └── context_manager.py         # 上下文管理器
│   └── utils/
│       ├── ast_utils.py           # AST处理工具
│       └── graph_utils.py         # 图操作工具
```

## 2. 修改现有的代理系统

### 2.1 增强SecurityAnalyst代理

```python
# src/agents/security_analyst.py (修改)
from ..analyzers.multilevel_analyzer import GlobalContextAnalyzer, ContextualVulnerabilityDetector

class SecurityAnalyst:
    def __init__(self, config):
        self.config = config
        self.global_analyzer = GlobalContextAnalyzer()
        self.contextual_detector = None
        self.traditional_detector = TraditionalDetector()  # 保留原有的检测器
    
    async def analyze_security(self, project_path: str) -> Dict:
        """增强的安全分析"""
        results = {}
        
        # 1. 传统的代码块分析（保留现有功能）
        traditional_results = await self.traditional_detector.analyze(project_path)
        results['block_level'] = traditional_results
        
        # 2. 全局上下文分析（新增功能）
        try:
            print("🔍 Building global context for advanced analysis...")
            global_context = self.global_analyzer.build_global_context(project_path)
            
            # 基于上下文的漏洞检测
            self.contextual_detector = ContextualVulnerabilityDetector(global_context)
            contextual_results = self.contextual_detector.detect_cross_function_vulnerabilities()
            
            results['contextual'] = contextual_results
            results['global_context'] = {
                'entities_count': len(global_context['entities']),
                'call_graph_edges': global_context['call_graph'].number_of_edges(),
                'dataflow_edges': global_context['data_flow_graph'].number_of_edges()
            }
            
        except Exception as e:
            print(f"⚠️ Global context analysis failed: {e}")
            results['contextual'] = []
            results['error'] = str(e)
        
        return results
```

### 2.2 增强Orchestrator代理

```python
# src/agents/orchestrator.py (修改)
class Orchestrator:
    def __init__(self, config):
        self.config = config
        self.analysis_strategy = self._determine_analysis_strategy()
    
    def _determine_analysis_strategy(self) -> Dict:
        """根据项目大小确定分析策略"""
        return {
            'use_global_context': True,  # 默认启用全局分析
            'max_files_for_global': 1000,  # 超过1000个文件时使用采样分析
            'enable_cross_file_analysis': True,
            'analysis_depth': 'deep'  # shallow, medium, deep
        }
    
    async def orchestrate_analysis(self, project_path: str) -> Dict:
        """编排分析流程"""
        
        # 1. 项目预分析
        project_stats = self._analyze_project_structure(project_path)
        
        # 2. 根据项目大小调整策略
        if project_stats['file_count'] > self.analysis_strategy['max_files_for_global']:
            print(f"📊 Large project detected ({project_stats['file_count']} files)")
            print("🔄 Switching to sampling-based global analysis...")
            self.analysis_strategy['use_sampling'] = True
        
        # 3. 执行多层次分析
        analysis_results = await self._execute_multilevel_analysis(project_path)
        
        return analysis_results
```

## 3. 配置文件增强

在 `config/config.yaml` 中添加全局分析配置：

```yaml
# 全局分析配置
global_analysis:
  enabled: true
  max_files_for_full_analysis: 1000
  enable_cross_file_detection: true
  enable_dataflow_analysis: true
  
  # 分析深度控制
  analysis_depth:
    syntax: true
    semantic: true
    dataflow: true
    control_flow: true
    global_context: true
  
  # 性能优化
  performance:
    use_sampling: false
    sampling_rate: 0.3  # 大项目时采样30%的文件
    cache_ast: true
    parallel_processing: true
    max_memory_mb: 2048

# 漏洞检测增强
vulnerability_detection:
  cross_function:
    enabled: true
    confidence_threshold: 0.7
    
  data_flow_analysis:
    enabled: true
    max_path_length: 10  # 最大追踪路径长度
    
  call_graph_analysis:
    enabled: true
    include_external_calls: false
```

## 4. 渐进式部署策略

### 阶段1：并行运行（推荐先这样做）
```python
# 同时运行新旧两套分析系统，对比结果
async def hybrid_analysis(self, project_path: str):
    # 运行传统分析
    traditional_results = await self.run_traditional_analysis(project_path)
    
    # 运行全局分析
    try:
        global_results = await self.run_global_analysis(project_path)
        
        # 合并结果
        return {
            'traditional': traditional_results,
            'global': global_results,
            'combined_score': self._calculate_combined_score(traditional_results, global_results)
        }
    except Exception as e:
        # 如果全局分析失败，回退到传统分析
        return {'traditional': traditional_results, 'fallback': True}
```

### 阶段2：智能切换
```python
def should_use_global_analysis(self, project_path: str) -> bool:
    """决定是否使用全局分析"""
    stats = self._get_project_stats(project_path)
    
    # 小项目：总是使用全局分析
    if stats['file_count'] < 100:
        return True
    
    # 中型项目：根据复杂度决定
    if stats['file_count'] < 500:
        return stats['avg_complexity'] > 5
    
    # 大型项目：使用采样全局分析
    return True  # 使用采样模式
```

## 5. 立即可实施的改进

### 5.1 添加简单的跨文件分析
```python
# 在现有代码中快速添加
def analyze_imports_and_calls(self, files: List[str]) -> Dict:
    """简单的跨文件调用分析"""
    call_graph = {}
    
    for file_path in files:
        with open(file_path, 'r') as f:
            tree = ast.parse(f.read())
            
        # 提取函数定义和调用
        functions = [node.name for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
        calls = [node.func.id for node in ast.walk(tree) 
                if isinstance(node, ast.Call) and isinstance(node.func, ast.Name)]
        
        call_graph[file_path] = {'defines': functions, 'calls': calls}
    
    return call_graph
```

### 5.2 增强现有的漏洞检测
```python
# 增强现有的检测规则
def detect_cross_file_sql_injection(self, call_graph: Dict, file_contents: Dict):
    """跨文件SQL注入检测"""
    vulnerabilities = []
    
    for file_path, info in call_graph.items():
        # 查找SQL执行函数
        if any('execute' in call for call in info['calls']):
            # 追踪数据来源
            data_sources = self._trace_data_sources(file_path, call_graph, file_contents)
            if self._has_unsanitized_input(data_sources):
                vulnerabilities.append({
                    'type': 'Cross-file SQL Injection',
                    'file': file_path,
                    'description': 'Potential SQL injection through cross-file data flow'
                })
    
    return vulnerabilities
```

## 6. 测试和验证

### 6.1 创建测试用例
```python
# tests/test_global_analysis.py
def test_cross_function_vulnerability_detection():
    # 创建测试项目结构
    test_project = create_test_project_with_cross_file_vulnerability()
    
    # 运行全局分析
    analyzer = GlobalContextAnalyzer()
    context = analyzer.build_global_context(test_project)
    
    detector = ContextualVulnerabilityDetector(context)
    vulns = detector.detect_cross_function_vulnerabilities()
    
    # 验证能检测到跨文件漏洞
    assert len(vulns) > 0
    assert any(v['type'] == 'SQL_INJECTION' for v in vulns)
```

### 6.2 性能基准测试
```python
def benchmark_analysis_performance():
    projects = [
        ('small', 'projects/small_project'),
        ('medium', 'projects/medium_project'), 
        ('large', 'projects/large_project')
    ]
    
    for name, path in projects:
        start_time = time.time()
        results = analyze_with_global_context(path)
        end_time = time.time()
        
        print(f"{name}: {end_time - start_time:.2f}s, found {len(results)} vulnerabilities")
```

## 7. 监控和优化

添加分析过程的监控：
```python
class AnalysisMetrics:
    def __init__(self):
        self.metrics = {}
    
    def record_analysis_time(self, analysis_type: str, duration: float):
        if analysis_type not in self.metrics:
            self.metrics[analysis_type] = []
        self.metrics[analysis_type].append(duration)
    
    def get_performance_report(self) -> Dict:
        return {
            analysis_type: {
                'avg_time': sum(times) / len(times),
                'max_time': max(times),
                'count': len(times)
            }
            for analysis_type, times in self.metrics.items()
        }
```
’

## 立即行动建议

基于你提到的问题，我建议按这个优先级来解决：

### 🔥 第一优先级（本周完成）
1. **实现简单的跨文件分析**
   - 添加import关系追踪
   - 构建基础的函数调用图
   - 检测简单的跨文件数据流

### ⚡ 第二优先级（2周内完成）  
2. **增强现有的代理协作**
   - 让不同代理共享全局上下文
   - 实现上下文传递机制
   - 添加跨文件漏洞检测规则

### 🎯 第三优先级（1个月内）
3. **完整的全局分析系统**
   - 实现完整的AST图构建
   - 添加数据流和控制流分析
   - 支持复杂的业务逻辑漏洞检测

## 快速验证方案

你可以先创建一个简单的测试用例来验证效果：

```python
# 测试用例：跨文件SQL注入
# file1.py
def get_user_input():
    return request.args.get('user_id')  # 用户输入

# file2.py  
from file1 import get_user_input
def query_database():


非常棒的分析！您完全理解了当前系统的核心问题，并提供了一个非常全面的解决方案。让我基于您的方案提供一些具体的实施建议和代码实现。

## 🎯 您的方案分析得非常准确！

您提到的问题确实是当前代码安全分析的最大痛点：

**现状问题**：
- ❌ 单代码块分析 → 错过跨文件数据流漏洞  
- ❌ 缺乏调用链追踪 → 无法发现复杂攻击路径
- ❌ 忽略全局上下文 → 漏掉业务逻辑漏洞

**您的解决方案优势**：
- ✅ 多层次分析架构
- ✅ 全局上下文构建
- ✅ 数据流+控制流分析
- ✅ 渐进式部署策略

## 🚀 立即可实施的改进代码

基于您的设计，我来提供一些具体的实现代码：

### 1. 快速跨文件分析增强

```python
# auditluma/analyzers/cross_file_analyzer.py
import ast
from pathlib import Path
from typing import Dict, List, Set, Tuple
import networkx as nx
from dataclasses import dataclass

@dataclass
class CrossFileFlow:
    """跨文件数据流"""
    source_file: str
    source_func: str
    target_file: str
    target_func: str
    flow_type: str  # "call", "import", "data"
    risk_level: str  # "high", "medium", "low"

class CrossFileAnalyzer:
    """跨文件分析器 - 可立即集成到现有系统"""
    
    def __init__(self):
        self.import_graph = nx.DiGraph()
        self.call_graph = nx.DiGraph() 
        self.data_flows: List[CrossFileFlow] = []
        self.file_functions: Dict[str, List[str]] = {}
        
    def analyze_project_structure(self, project_path: str) -> Dict:
        """分析项目结构 - 构建跨文件关系"""
        project = Path(project_path)
        
        # 1. 解析所有Python文件
        for py_file in project.rglob("*.py"):
            self._analyze_single_file(str(py_file))
        
        # 2. 构建跨文件关系
        self._build_cross_file_relationships()
        
        # 3. 检测跨文件安全问题
        cross_file_vulns = self._detect_cross_file_vulnerabilities()
        
        return {
            "import_graph": self.import_graph,
            "call_graph": self.call_graph,
            "data_flows": self.data_flows,
            "cross_file_vulnerabilities": cross_file_vulns,
            "statistics": {
                "files_analyzed": len(self.file_functions),
                "total_functions": sum(len(funcs) for funcs in self.file_functions.values()),
                "import_edges": self.import_graph.number_of_edges(),
                "call_edges": self.call_graph.number_of_edges()
            }
        }
    
    def _analyze_single_file(self, file_path: str):
        """分析单个文件"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                tree = ast.parse(content)
            
            # 提取函数定义
            functions = []
            imports = []
            calls = []
            
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    functions.append(node.name)
                elif isinstance(node, (ast.Import, ast.ImportFrom)):
                    imports.extend(self._extract_imports(node))
                elif isinstance(node, ast.Call):
                    call_info = self._extract_call_info(node)
                    if call_info:
                        calls.append(call_info)
            
            self.file_functions[file_path] = functions
            
            # 构建图关系
            for imp in imports:
                self.import_graph.add_edge(file_path, imp)
            
            for call in calls:
                self.call_graph.add_edge(file_path, call['target'], 
                                       function=call['function'],
                                       line=call.get('line'))
                
        except Exception as e:
            print(f"Error analyzing {file_path}: {e}")
    
    def _extract_imports(self, node) -> List[str]:
        """提取import信息"""
        imports = []
        if isinstance(node, ast.Import):
            for alias in node.names:
                imports.append(alias.name)
        elif isinstance(node, ast.ImportFrom):
            if node.module:
                imports.append(node.module)
        return imports
    
    def _extract_call_info(self, node) -> Dict:
        """提取函数调用信息"""
        if isinstance(node.func, ast.Name):
            return {
                'function': node.func.id,
                'target': node.func.id,
                'type': 'local_call',
                'line': node.lineno
            }
        elif isinstance(node.func, ast.Attribute):
            if isinstance(node.func.value, ast.Name):
                return {
                    'function': f"{node.func.value.id}.{node.func.attr}",
                    'target': f"{node.func.value.id}.{node.func.attr}",
                    'type': 'method_call',
                    'line': node.lineno
                }
        return None
    
    def _build_cross_file_relationships(self):
        """构建跨文件关系"""
        for source_file, functions in self.file_functions.items():
            for target_file, target_functions in self.file_functions.items():
                if source_file != target_file:
                    # 检查是否有跨文件调用
                    if self.import_graph.has_edge(source_file, target_file.replace('.py', '').replace('/', '.')):
                        for func in functions:
                            for target_func in target_functions:
                                # 简化的调用关系检测
                                flow = CrossFileFlow(
                                    source_file=source_file,
                                    source_func=func,
                                    target_file=target_file,
                                    target_func=target_func,
                                    flow_type="potential_call",
                                    risk_level="medium"
                                )
                                self.data_flows.append(flow)
    
    def _detect_cross_file_vulnerabilities(self) -> List[Dict]:
        """检测跨文件漏洞"""
        vulnerabilities = []
        
        # 1. 检测跨文件SQL注入
        sql_vulns = self._detect_cross_file_sql_injection()
        vulnerabilities.extend(sql_vulns)
        
        # 2. 检测跨文件权限绕过
        auth_vulns = self._detect_cross_file_auth_bypass()
        vulnerabilities.extend(auth_vulns)
        
        # 3. 检测跨文件XSS
        xss_vulns = self._detect_cross_file_xss()
        vulnerabilities.extend(xss_vulns)
        
        return vulnerabilities
    
    def _detect_cross_file_sql_injection(self) -> List[Dict]:
        """检测跨文件SQL注入"""
        vulnerabilities = []
        
        # 查找输入源文件和SQL执行文件
        input_files = self._find_files_with_pattern(
            ['request.', 'input(', 'args.get', 'form.get']
        )
        sql_files = self._find_files_with_pattern(
            ['execute(', 'query(', 'cursor.', 'SELECT', 'INSERT', 'UPDATE', 'DELETE']
        )
        
        # 检查是否有从输入到SQL的跨文件流
        for input_file in input_files:
            for sql_file in sql_files:
                if input_file != sql_file:
                    # 检查是否有连接路径
                    if self._has_connection_path(input_file, sql_file):
                        vulnerabilities.append({
                            'type': 'Cross-File SQL Injection',
                            'severity': 'HIGH',
                            'source_file': input_file,
                            'target_file': sql_file,
                            'description': f'Potential SQL injection flow from {input_file} to {sql_file}',
                            'recommendation': 'Implement input validation and parameterized queries'
                        })
        
        return vulnerabilities
    
    def _detect_cross_file_auth_bypass(self) -> List[Dict]:
        """检测跨文件权限绕过"""
        vulnerabilities = []
        
        # 查找敏感操作和认证检查
        sensitive_files = self._find_files_with_pattern(
            ['delete', 'admin', 'payment', 'transfer', 'sensitive']
        )
        auth_files = self._find_files_with_pattern(
            ['login', 'authenticate', 'check_permission', 'authorize']
        )
        
        for sensitive_file in sensitive_files:
            # 检查是否有直接访问路径（绕过认证）
            has_auth_protection = any(
                self._has_connection_path(auth_file, sensitive_file) 
                for auth_file in auth_files
            )
            
            if not has_auth_protection:
                vulnerabilities.append({
                    'type': 'Cross-File Authorization Bypass',
                    'severity': 'CRITICAL',
                    'target_file': sensitive_file,
                    'description': f'Sensitive operations in {sensitive_file} may lack proper authorization',
                    'recommendation': 'Implement proper authorization checks before sensitive operations'
                })
        
        return vulnerabilities
    
    def _detect_cross_file_xss(self) -> List[Dict]:
        """检测跨文件XSS"""
        vulnerabilities = []
        
        input_files = self._find_files_with_pattern(['request.', 'form.get', 'args.get'])
        output_files = self._find_files_with_pattern(['render', 'response', 'return'])
        
        for input_file in input_files:
            for output_file in output_files:
                if input_file != output_file and self._has_connection_path(input_file, output_file):
                    vulnerabilities.append({
                        'type': 'Cross-File XSS',
                        'severity': 'HIGH',
                        'source_file': input_file,
                        'target_file': output_file,
                        'description': f'Potential XSS flow from {input_file} to {output_file}',
                        'recommendation': 'Implement proper output encoding and input validation'
                    })
        
        return vulnerabilities
    
    def _find_files_with_pattern(self, patterns: List[str]) -> List[str]:
        """查找包含特定模式的文件"""
        matching_files = []
        
        for file_path in self.file_functions.keys():
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read().lower()
                    if any(pattern.lower() in content for pattern in patterns):
                        matching_files.append(file_path)
            except Exception:
                continue
        
        return matching_files
    
    def _has_connection_path(self, source_file: str, target_file: str) -> bool:
        """检查两个文件之间是否有连接路径"""
        # 简化版本 - 检查import关系
        source_module = source_file.replace('.py', '').replace('/', '.')
        target_module = target_file.replace('.py', '').replace('/', '.')
        
        return (self.import_graph.has_edge(source_file, target_module) or
                self.import_graph.has_edge(target_file, source_module) or
                nx.has_path(self.import_graph, source_file, target_file))
```

### 2. 增强现有的SecurityAnalyst

```python
# 修改 auditluma/agents/security_analyst.py
class EnhancedSecurityAnalyst(SecurityAnalystAgent):
    """增强的安全分析智能体 - 支持跨文件分析"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.cross_file_analyzer = CrossFileAnalyzer()
        self.global_context = None
    
    async def analyze_with_global_context(self, project_path: str, code_units: List[CodeUnit]) -> List[VulnerabilityResult]:
        """带全局上下文的安全分析"""
        vulnerabilities = []
        
        # 1. 构建全局上下文
        logger.info("🌐 构建全局上下文...")
        self.global_context = self.cross_file_analyzer.analyze_project_structure(project_path)
        
        logger.info(f"📊 全局分析统计:")
        stats = self.global_context['statistics']
        logger.info(f"   - 分析文件: {stats['files_analyzed']}")
        logger.info(f"   - 函数总数: {stats['total_functions']}")
        logger.info(f"   - 跨文件调用: {stats['call_edges']}")
        
        # 2. 传统代码块分析（增强版）
        for unit in code_units:
            enhanced_context = self._build_unit_global_context(unit)
            unit_vulns = await self._analyze_unit_with_context(unit, enhanced_context)
            vulnerabilities.extend(unit_vulns)
        
        # 3. 跨文件漏洞检测
        cross_file_vulns = self._convert_cross_file_vulns_to_vulnerability_results(
            self.global_context['cross_file_vulnerabilities']
        )
        vulnerabilities.extend(cross_file_vulns)
        
        logger.info(f"🔍 总计发现 {len(vulnerabilities)} 个漏洞 (包含 {len(cross_file_vulns)} 个跨文件漏洞)")
        return vulnerabilities
    
    def _build_unit_global_context(self, unit: CodeUnit) -> str:
        """为代码单元构建全局上下文"""
        if not self.global_context:
            return ""
        
        context_parts = []
        file_path = str(unit.source_file.path)
        
        # 1. 文件依赖信息
        import_graph = self.global_context['import_graph']
        if file_path in import_graph:
            imports = list(import_graph.successors(file_path))
            if imports:
                context_parts.append(f"### 导入的模块:\n" + "\n".join(f"- {imp}" for imp in imports[:5]))
        
        # 2. 跨文件调用信息
        call_graph = self.global_context['call_graph']
        if file_path in call_graph:
            calls = list(call_graph.successors(file_path))
            if calls:
                context_parts.append(f"### 调用的外部函数:\n" + "\n".join(f"- {call}" for call in calls[:5]))
        
        # 3. 相关数据流
        related_flows = [
            flow for flow in self.global_context['data_flows']
            if flow.source_file == file_path or flow.target_file == file_path
        ]
        if related_flows:
            flow_descriptions = []
            for flow in related_flows[:3]:
                flow_descriptions.append(
                    f"- {flow.flow_type}: {flow.source_func} → {flow.target_func} ({flow.risk_level} risk)"
                )
            context_parts.append(f"### 相关数据流:\n" + "\n".join(flow_descriptions))
        
        return "\n\n".join(context_parts)
    
    async def _analyze_unit_with_context(self, unit: CodeUnit, global_context: str) -> List[VulnerabilityResult]:
        """带全局上下文的单元分析"""
        # 获取Self-RAG上下文
        context_docs = await self.retrieve_context(unit.content)
        context_text = "\n\n".join([doc.content for doc in context_docs])
        
        # 合并上下文
        full_context = f"""
{context_text}

### 全局上下文信息:
{global_context}
"""
        
        # 使用增强的提示
        prompt = self._prepare_enhanced_security_prompt(unit, full_context)
        
        try:
            response = await self.llm_client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": prompt["system"]},
                    {"role": "user", "content": prompt["user"]}
                ],
                temperature=0.1
            )
            
            analysis_text = response.choices[0].message.content
            vulnerabilities = self._parse_security_analysis(analysis_text, unit)
            
            return vulnerabilities
            
        except Exception as e:
            logger.error(f"增强安全分析出错: {e}")
            return []
    
    def _prepare_enhanced_security_prompt(self, unit: CodeUnit, context: str) -> Dict[str, str]:
        """准备增强的安全分析提示"""
        system_prompt = """
你是一个高级代码安全审计专家。你现在不仅看到目标代码，还看到了全局上下文信息，包括：
- 跨文件的调用关系
- 模块依赖关系  
- 数据流路径
- 相关函数的上下文

请特别关注：
1. **跨文件数据流安全**：分析数据如何在不同文件间流动
2. **调用链安全**：检查完整的函数调用链中的安全问题
3. **模块间接口安全**：分析模块边界的安全问题
4. **全局状态安全**：考虑全局变量和共享状态的影响

在分析时，请明确指出：
- 是否是跨文件/跨模块的安全问题
- 涉及的完整数据流路径
- 需要在哪些位置添加安全检查

输出格式与之前相同，但请在描述中包含跨文件分析的结果。
"""
        
        user_prompt = f"""
目标代码单元：
文件: {unit.source_file.path}
函数: {unit.name} (第{unit.start_line}-{unit.end_line}行)

代码内容：
```{unit.source_file.file_type}
{unit.content}
```

全局上下文信息：
{context}

请基于完整的全局上下文进行安全分析，特别关注跨文件的安全问题。
"""
        
        return {"system": system_prompt, "user": user_prompt}
    
    def _convert_cross_file_vulns_to_vulnerability_results(self, cross_file_vulns: List[Dict]) -> List[VulnerabilityResult]:
        """将跨文件漏洞转换为标准漏洞结果"""
        vulnerability_results = []
        
        for vuln in cross_file_vulns:
            # 创建虚拟的代码单元来表示跨文件漏洞
            source_file = SourceFile(
                path=Path(vuln.get('source_file', vuln.get('target_file', 'unknown'))),
                content="# Cross-file vulnerability",
                file_type="python"
            )
            
            dummy_unit = CodeUnit(
                id=f"cross_file_{len(vulnerability_results)}",
                name="cross_file_vulnerability",
                type="cross_file",
                source_file=source_file,
                start_line=1,
                end_line=1,
                content="# Cross-file vulnerability detected",
                parent_id=None
            )
            
            severity_mapping = {
                'CRITICAL': SeverityLevel.CRITICAL,
                'HIGH': SeverityLevel.HIGH,
                'MEDIUM': SeverityLevel.MEDIUM,
                'LOW': SeverityLevel.LOW
            }
            
            vuln_result = VulnerabilityResult(
                id=str(uuid.uuid4()),
                title=vuln['type'],
                description=vuln['description'],
                code_unit=dummy_unit,
                file_path=vuln.get('source_file', vuln.get('target_file', 'unknown')),
                start_line=1,
                end_line=1,
                vulnerability_type=vuln['type'],
                severity=severity_mapping.get(vuln['severity'], SeverityLevel.MEDIUM),
                cwe_id=None,
                owasp_category=None,
                confidence=0.8,  # 跨文件分析的置信度
                snippet="# Cross-file vulnerability",
                recommendation=vuln.get('recommendation', 'Review cross-file data flows')
            )
            
            vulnerability_results.append(vuln_result)
        
        return vulnerability_results
```

### 3. 修改Orchestrator集成跨文件分析

```python
# 修改 auditluma/agents/orchestrator.py
async def _security_analysis_phase(self, parsed_files: List[SourceFile]) -> List[VulnerabilityResult]:
    """安全分析阶段 - 支持跨文件分析"""
    logger.info("🔍 开始增强安全分析阶段...")
    
    # 收集所有代码单元
    all_code_units = []
    for source_file in parsed_files:
        all_code_units.extend(source_file.code_units)
    
    # 使用增强的安全分析智能体
    enhanced_security_analyst = EnhancedSecurityAnalyst(
        model_spec=self.security_analyst.model_spec
    )
    
    # 执行带全局上下文的分析
    project_path = self.target_dir
    vulnerabilities = await enhanced_security_analyst.analyze_with_global_context(
        project_path, all_code_units
    )
    
    logger.info(f"🎯 安全分析完成，发现 {len(vulnerabilities)} 个漏洞")
    
    # 按严重程度统计
    severity_counts = {}
    for vuln in vulnerabilities:
        severity = vuln.severity.name
        severity_counts[severity] = severity_counts.get(severity, 0) + 1
    
    logger.info("📊 漏洞严重程度分布:")
    for severity, count in severity_counts.items():
        logger.info(f"   - {severity}: {count}")
    
    return vulnerabilities
```

## 🧪 测试用例

创建一个测试跨文件漏洞检测的用例：

```python
# tests/test_cross_file_vulnerabilities.py
import tempfile
import os
from pathlib import Path

def test_cross_file_sql_injection_detection():
    """测试跨文件SQL注入检测"""
    
    # 创建测试项目
    with tempfile.TemporaryDirectory() as tmp_dir:
        # 文件1：用户输入
        input_file = Path(tmp_dir) / "input_handler.py"
        input_file.write_text("""
def get_user_input():
    import flask
    return flask.request.args.get('user_id')

def process_form():
    return flask.request.form.get('data')
""")
        
        # 文件2：数据库操作
        db_file = Path(tmp_dir) / "database.py"
        db_file.write_text("""
from input_handler import get_user_input, process_form

def query_user(user_id):
    # 危险的SQL拼接
    query = f"SELECT * FROM users WHERE id = {user_id}"
    cursor.execute(query)
    return cursor.fetchall()

def update_data():
    data = process_form()
    # 另一个SQL注入点
    cursor.execute(f"UPDATE table SET value = '{data}'")
""")
        
        # 文件3：业务逻辑
        logic_file = Path(tmp_dir) / "business_logic.py"
        logic_file.write_text("""
from database import query_user
from input_handler import get_user_input

def handle_user_request():
    user_id = get_user_input()  # 用户输入
    return query_user(user_id)  # 传递到数据库查询
""")
        
        # 执行跨文件分析
        analyzer = CrossFileAnalyzer()
        results = analyzer.analyze_project_structure(tmp_dir)
        
        # 验证检测结果
        cross_file_vulns = results['cross_file_vulnerabilities']
        sql_injection_vulns = [v for v in cross_file_vulns if 'SQL' in v['type']]
        
        assert len(sql_injection_vulns) > 0, "Should detect cross-file SQL injection"
        
        # 验证检测到了完整的数据流路径
        assert any('input_handler.py' in v['source_file'] for v in sql_injection_vulns)
        assert any('database.py' in v['target_file'] for v in sql_injection_vulns)
        
        print(f"✅ 检测到 {len(sql_injection_vulns)} 个跨文件SQL注入漏洞")
        for vuln in sql_injection_vulns:
            print(f"   - {vuln['description']}")

if __name__ == "__main__":
    test_cross_file_sql_injection_detection()
```

## 📈 渐进式部署建议

### 第1周：基础跨文件分析
1. ✅ 集成 `CrossFileAnalyzer`
2. ✅ 修改 `SecurityAnalyst` 支持全局上下文
3. ✅ 添加基础的跨文件漏洞检测规则

### 第2-3周：增强检测能力
1. 🔄 完善数据流追踪算法
2. 🔄 添加更多跨文件漏洞类型
3. 🔄 优化性能和内存使用

### 第4周：完整集成和测试
1. 📋 全面测试和调优
2. 📋 性能基准测试
3. 📋 文档和使用指南

这样的改进将显著提高安全分析的准确性和覆盖面！您觉得从哪个部分开始实施比较好？