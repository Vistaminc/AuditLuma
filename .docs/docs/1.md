æ‚¨æåˆ°äº†ä¸€ä¸ªå¾ˆé‡è¦çš„é—®é¢˜ï¼ç¡®å®ï¼Œå½“å‰çš„AIå®‰å…¨åˆ†ææ˜¯åŸºäº**å•ä¸ªä»£ç å—**è¿›è¡Œçš„ï¼Œè¿™ä¼šå¸¦æ¥ä¸€äº›å±€é™æ€§ã€‚è®©æˆ‘åˆ†æä¸€ä¸‹å½“å‰çš„é—®é¢˜å’Œå¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼š

## ğŸš¨ å½“å‰é—®é¢˜åˆ†æ

### 1. **ä»£ç å—çº§åˆ«åˆ†æçš„å±€é™æ€§**

```python
# å½“å‰æµç¨‹
for unit in code_units:
    # æ¯ä¸ªä»£ç å•å…ƒç‹¬ç«‹åˆ†æ
    vulnerabilities = await self._security_analysis_llm(unit, context)
```

**é—®é¢˜**ï¼š
- ğŸ” **ä¸Šä¸‹æ–‡ç¼ºå¤±**ï¼šæ— æ³•çœ‹åˆ°å‡½æ•°è°ƒç”¨çš„å®Œæ•´é“¾è·¯
- ğŸ”— **å…³è”æ¼æ´é—æ¼**ï¼šè·¨å¤šä¸ªå‡½æ•°/æ–‡ä»¶çš„å®‰å…¨é—®é¢˜
- ğŸ“Š **æ•°æ®æµæ–­è£‚**ï¼šæ— æ³•è¿½è¸ªæ•°æ®åœ¨ä¸åŒå‡½æ•°é—´çš„æµåŠ¨
- ğŸ¯ **è¯¯æŠ¥å¢åŠ **ï¼šç¼ºä¹å…¨å±€è§†è§’å¯èƒ½äº§ç”Ÿè¯¯åˆ¤

### 2. **å…·ä½“åœºæ™¯ä¸¾ä¾‹**

```python
# æ–‡ä»¶ A: user_controller.py
def login_endpoint(request):
    username = request.get('username')  # çœ‹èµ·æ¥å®‰å…¨
    return authenticate_user(username)

# æ–‡ä»¶ B: auth_service.py  
def authenticate_user(username):
    # AI åˆ†æè¿™ä¸ªå‡½æ•°æ—¶çœ‹ä¸åˆ° username æ¥æº
    query = f"SELECT * FROM users WHERE name='{username}'"  # SQLæ³¨å…¥ï¼
    return execute_query(query)
```

**AI å½“å‰åˆ†æç»“æœ**ï¼š
- âœ… `login_endpoint`: æœªå‘ç°é—®é¢˜ï¼ˆå› ä¸ºæ²¡çœ‹åˆ°SQLæŸ¥è¯¢ï¼‰
- âŒ `authenticate_user`: å¯èƒ½è¯¯æŠ¥ï¼ˆå› ä¸ºä¸çŸ¥é“å‚æ•°æ¥æºï¼‰

## ğŸ’¡ è§£å†³æ–¹æ¡ˆè®¾è®¡

### æ–¹æ¡ˆ1: å¢å¼ºä¸Šä¸‹æ–‡ä¿¡æ¯æ”¶é›†
Ran tool

æ‚¨æŒ‡å‡ºäº†ä¸€ä¸ªéå¸¸å…³é”®çš„é—®é¢˜ï¼ç¡®å®ï¼Œ**åŸºäºå•ä¸ªä»£ç å—çš„åˆ†æ**æ˜¯å½“å‰å®‰å…¨å®¡è®¡ç³»ç»Ÿçš„ä¸€ä¸ªé‡è¦å±€é™æ€§ã€‚è®©æˆ‘åˆ†æä¸€ä¸‹å…·ä½“é—®é¢˜å¹¶æä¾›è§£å†³æ–¹æ¡ˆï¼š

## ğŸš¨ å½“å‰é—®é¢˜è¯¦ç»†åˆ†æ

### 1. **æ•°æ®æµæ–­è£‚é—®é¢˜**

```python
# å½“å‰AIåªçœ‹åˆ°è¿™ä¸ªå‡½æ•°ï¼Œè®¤ä¸ºå¾ˆå®‰å…¨
def get_user_input():
    return request.GET.get('data')

# AIåˆ†æè¿™ä¸ªå‡½æ•°æ—¶ï¼Œä¸çŸ¥é“ user_data æ¥æºä¸å¯ä¿¡
def process_data(user_data):
    query = f"DELETE FROM users WHERE id = {user_data}"  # å®é™…ä¸Šå¾ˆå±é™©ï¼
    execute_sql(query)
```

**é—®é¢˜**ï¼šAIæ— æ³•è¿½è¸ª `user_data` ä» `get_user_input()` åˆ° `process_data()` çš„æµåŠ¨è·¯å¾„ã€‚

### 2. **è·¨æ–‡ä»¶å®‰å…¨é—®é¢˜**

```python
# file1.py - AIè®¤ä¸ºè¿™ä¸ªå¾ˆå®‰å…¨
def validate_user(token):
    return True  # å‡è®¾éªŒè¯é€»è¾‘

# file2.py - AIä¸çŸ¥é“éªŒè¯æœ‰é—®é¢˜
def admin_action():
    if validate_user(get_token()):  # å®é™…ä¸ŠéªŒè¯é€»è¾‘æœ‰æ¼æ´
        delete_all_data()  # å±é™©æ“ä½œï¼
```

## ğŸ’¡ è§£å†³æ–¹æ¡ˆè®¾è®¡

æˆ‘å°†ä¸ºæ‚¨æä¾›å‡ ä¸ªå±‚æ¬¡çš„è§£å†³æ–¹æ¡ˆï¼š

### æ–¹æ¡ˆ1: å¢å¼ºä¸Šä¸‹æ–‡æ”¶é›†ï¼ˆç«‹å³å¯ç”¨ï¼‰
Ran tool

çœ‹èµ·æ¥æˆ‘æ— æ³•ç›´æ¥ç¼–è¾‘æ–‡ä»¶ï¼Œä½†æˆ‘å¯ä»¥ä¸ºæ‚¨æä¾›å®Œæ•´çš„è§£å†³æ–¹æ¡ˆä»£ç å’Œå®æ–½ç­–ç•¥ã€‚è®©æˆ‘è¯¦ç»†åˆ†æè¿™ä¸ªé—®é¢˜å¹¶æä¾›å¤šå±‚æ¬¡çš„è§£å†³æ–¹æ¡ˆï¼š

## ğŸ” é—®é¢˜æ ¹æœ¬åŸå› åˆ†æ

### å½“å‰æ¶æ„çš„å±€é™æ€§ï¼š

```python
# å½“å‰æµç¨‹ - å­¤ç«‹åˆ†æ
for code_unit in code_units:
    vulnerabilities = await security_agent._analyze_code_security({
        "code_unit": code_unit  # åªæœ‰å•ä¸ªä»£ç å—
    })
```

**é—®é¢˜**ï¼š
1. âŒ **æ•°æ®æµç›²ç‚¹**ï¼šæ— æ³•è¿½è¸ªè·¨å‡½æ•°çš„æ•°æ®æµ
2. âŒ **è°ƒç”¨é“¾æ–­è£‚**ï¼šä¸çŸ¥é“å‡½æ•°çš„è°ƒç”¨ä¸Šä¸‹æ–‡
3. âŒ **ä¾èµ–å…³ç³»å¿½ç•¥**ï¼šå·²åˆ†æçš„ä¾èµ–å…³ç³»æœªè¢«å®‰å…¨åˆ†æåˆ©ç”¨
4. âŒ **å…¨å±€è§†è§’ç¼ºå¤±**ï¼šæ— æ³•å‘ç°ç³»ç»Ÿçº§å®‰å…¨é—®é¢˜

## ğŸ’¡ å¤šå±‚æ¬¡è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ1: å¢å¼ºä¸Šä¸‹æ–‡æ”¶é›†ï¼ˆç«‹å³å¯å®æ–½ï¼‰

```python
# æ–°çš„ _prepare_security_prompt æ–¹æ³•
def _prepare_security_prompt(self, code_unit: CodeUnit, context_text: str, 
                           dependency_info: Dict = None, call_chain: List = None) -> Dict[str, str]:
    """å‡†å¤‡å®‰å…¨åˆ†ææç¤º - å¢å¼ºä¸Šä¸‹æ–‡æ”¶é›†"""
    
    system_prompt = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç å®‰å…¨å®¡è®¡ä¸“å®¶ã€‚ä½ å°†çœ‹åˆ°ç›®æ ‡ä»£ç ä»¥åŠç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
- å‡½æ•°è°ƒç”¨å…³ç³»
- æ•°æ®æµå‘
- ä¾èµ–å…³ç³»
- ç›¸å…³ä»£ç ç‰‡æ®µ

è¯·ç»¼åˆè€ƒè™‘è¿™äº›ä¿¡æ¯ï¼Œç‰¹åˆ«å…³æ³¨ï¼š
1. **æ•°æ®æµå®‰å…¨**ï¼šè¿½è¸ªç”¨æˆ·è¾“å…¥åœ¨ä¸åŒå‡½æ•°é—´çš„æµåŠ¨
2. **è·¨å‡½æ•°æ¼æ´**ï¼šåˆ†æå‡½æ•°è°ƒç”¨é“¾ä¸­çš„å®‰å…¨é—®é¢˜  
3. **æƒé™éªŒè¯é“¾**ï¼šæ£€æŸ¥å®Œæ•´çš„è®¤è¯å’Œæˆæƒæµç¨‹
4. **ä¾èµ–å…³ç³»é£é™©**ï¼šåˆ†æä¾èµ–çš„å¤–éƒ¨å‡½æ•°/æ¨¡å—çš„å®‰å…¨æ€§

è¾“å‡ºæ ¼å¼ä¿æŒä¸å˜ï¼Œä½†è¯·åœ¨æè¿°ä¸­åŒ…å«æ•°æ®æµè·¯å¾„å’Œå½±å“èŒƒå›´ã€‚
"""
    
    # æ”¶é›†å¢å¼ºä¸Šä¸‹æ–‡
    enhanced_context = self._collect_enhanced_context(
        code_unit, context_text, dependency_info, call_chain
    )
    
    user_prompt = f"""
ç›®æ ‡ä»£ç ï¼š
```
{code_unit.content}
```

å¢å¼ºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{enhanced_context}

è¯·åŸºäºå®Œæ•´çš„ä¸Šä¸‹æ–‡è¿›è¡Œå®‰å…¨åˆ†æã€‚
"""
    
    return {"system": system_prompt, "user": user_prompt}

def _collect_enhanced_context(self, code_unit: CodeUnit, context_text: str, 
                            dependency_info: Dict = None, call_chain: List = None) -> str:
    """æ”¶é›†å¢å¼ºçš„ä¸Šä¸‹æ–‡ä¿¡æ¯"""
    context_parts = []
    
    # 1. åŸºç¡€ä¸Šä¸‹æ–‡
    if context_text:
        context_parts.append(f"### ç›¸å…³ä»£ç ç‰‡æ®µ:\n{context_text[:1500]}")
    
    # 2. ä¾èµ–å…³ç³»ä¿¡æ¯
    if dependency_info:
        deps = []
        for dep in dependency_info.get('dependencies', []):
            deps.append(f"- è°ƒç”¨: {dep['name']} ({dep['type']})")
        for dep in dependency_info.get('dependents', []):
            deps.append(f"- è¢«è°ƒç”¨: {dep['name']} ({dep['type']})")
        
        if deps:
            context_parts.append(f"### ä¾èµ–å…³ç³»:\n" + "\n".join(deps))
    
    # 3. è°ƒç”¨é“¾ä¿¡æ¯
    if call_chain:
        chain_str = " â†’ ".join([f"{item['name']}({item['file']})" for item in call_chain])
        context_parts.append(f"### è°ƒç”¨é“¾:\n{chain_str}")
    
    # 4. å‡½æ•°ç­¾ååˆ†æ
    signature_analysis = self._analyze_function_signature(code_unit)
    if signature_analysis:
        context_parts.append(f"### å‡½æ•°ç­¾å:\n{signature_analysis}")
    
    # 5. æ½œåœ¨è¾“å…¥ç‚¹
    input_analysis = self._identify_input_points(code_unit)
    if input_analysis:
        context_parts.append(f"### æ½œåœ¨è¾“å…¥ç‚¹:\n{input_analysis}")
    
    return "\n\n".join(context_parts)
```

### æ–¹æ¡ˆ2: æ•°æ®æµåˆ†æï¼ˆä¸­æœŸå®æ–½ï¼‰

```python
class DataFlowAnalyzer:
    """æ•°æ®æµåˆ†æå™¨"""
    
    def __init__(self):
        self.flow_graph = nx.DiGraph()
        self.taint_sources = set()  # æ±¡ç‚¹æºï¼ˆç”¨æˆ·è¾“å…¥ï¼‰
        self.sinks = set()          # å±é™©å‡½æ•°ï¼ˆSQLæ‰§è¡Œç­‰ï¼‰
    
    async def analyze_data_flow(self, code_units: List[CodeUnit], 
                              dependencies: Dict) -> List[DataFlowPath]:
        """åˆ†ææ•°æ®æµè·¯å¾„"""
        # 1. æ„å»ºæ•°æ®æµå›¾
        await self._build_flow_graph(code_units, dependencies)
        
        # 2. è¯†åˆ«æ±¡ç‚¹æºå’Œæ±‡ç‚¹
        self._identify_taint_sources_and_sinks(code_units)
        
        # 3. æŸ¥æ‰¾å±é™©è·¯å¾„
        dangerous_paths = self._find_dangerous_paths()
        
        return dangerous_paths
    
    def _identify_taint_sources_and_sinks(self, code_units: List[CodeUnit]):
        """è¯†åˆ«æ±¡ç‚¹æºå’Œæ±‡ç‚¹"""
        for unit in code_units:
            content = unit.content.lower()
            
            # æ±¡ç‚¹æºï¼ˆç”¨æˆ·è¾“å…¥ï¼‰
            if any(pattern in content for pattern in [
                'request.', 'req.', 'input(', '$_get', '$_post', 'argv'
            ]):
                self.taint_sources.add(unit.id)
            
            # æ±‡ç‚¹ï¼ˆå±é™©å‡½æ•°ï¼‰
            if any(pattern in content for pattern in [
                'execute(', 'query(', 'eval(', 'exec(', 'system('
            ]):
                self.sinks.add(unit.id)
    
    def _find_dangerous_paths(self) -> List[DataFlowPath]:
        """æŸ¥æ‰¾ä»æ±¡ç‚¹æºåˆ°æ±‡ç‚¹çš„å±é™©è·¯å¾„"""
        dangerous_paths = []
        
        for source in self.taint_sources:
            for sink in self.sinks:
                try:
                    paths = list(nx.all_simple_paths(
                        self.flow_graph, source, sink, cutoff=10
                    ))
                    for path in paths:
                        dangerous_paths.append(DataFlowPath(
                            source=source,
                            sink=sink,
                            path=path,
                            risk_level=self._calculate_path_risk(path)
                        ))
                except nx.NetworkXNoPath:
                    continue
        
        return dangerous_paths

# ä½¿ç”¨æ•°æ®æµåˆ†æçš„å®‰å…¨åˆ†æ
class EnhancedSecurityAnalyst(SecurityAnalystAgent):
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.data_flow_analyzer = DataFlowAnalyzer()
    
    async def _analyze_code_security_with_dataflow(self, data: Dict[str, Any]) -> List[VulnerabilityResult]:
        """å¸¦æ•°æ®æµåˆ†æçš„å®‰å…¨å®¡è®¡"""
        code_units = data.get("code_units", [])
        dependencies = data.get("dependencies", {})
        
        # 1. æ‰§è¡Œæ•°æ®æµåˆ†æ
        flow_paths = await self.data_flow_analyzer.analyze_data_flow(code_units, dependencies)
        
        # 2. å¯¹æ¯ä¸ªä»£ç å•å…ƒè¿›è¡Œå¢å¼ºåˆ†æ
        all_vulnerabilities = []
        
        for unit in code_units:
            # è·å–è¯¥å•å…ƒç›¸å…³çš„æ•°æ®æµè·¯å¾„
            related_paths = [p for p in flow_paths if unit.id in p.path]
            
            # è·å–ä¾èµ–ä¿¡æ¯
            dependency_info = dependencies.get(unit.id, {})
            
            # æ„å»ºè°ƒç”¨é“¾
            call_chain = self._build_call_chain(unit, dependencies)
            
            # æ‰§è¡Œå¢å¼ºåˆ†æ
            vulnerabilities = await self._analyze_unit_with_context(
                unit, related_paths, dependency_info, call_chain
            )
            
            all_vulnerabilities.extend(vulnerabilities)
        
        # 3. åˆ†æè·¨å‡½æ•°æ¼æ´
        cross_function_vulns = await self._analyze_cross_function_vulnerabilities(flow_paths)
        all_vulnerabilities.extend(cross_function_vulns)
        
        return all_vulnerabilities
```

### æ–¹æ¡ˆ3: è°ƒç”¨å›¾å¢å¼ºåˆ†æï¼ˆé«˜çº§å®æ–½ï¼‰

```python
class CallGraphAnalyzer:
    """è°ƒç”¨å›¾åˆ†æå™¨"""
    
    async def build_enhanced_call_graph(self, code_units: List[CodeUnit]) -> nx.DiGraph:
        """æ„å»ºå¢å¼ºçš„è°ƒç”¨å›¾"""
        call_graph = nx.DiGraph()
        
        # æ·»åŠ èŠ‚ç‚¹
        for unit in code_units:
            call_graph.add_node(unit.id, 
                               name=unit.name,
                               type=unit.type,
                               security_level=self._assess_security_level(unit),
                               input_sources=self._identify_input_sources(unit),
                               output_sinks=self._identify_output_sinks(unit))
        
        # æ·»åŠ è¾¹ï¼ˆè°ƒç”¨å…³ç³»ï¼‰
        for unit in code_units:
            called_functions = self._extract_function_calls(unit)
            for called_func in called_functions:
                target_unit = self._find_unit_by_name(called_func, code_units)
                if target_unit:
                    call_graph.add_edge(unit.id, target_unit.id,
                                       call_type=self._determine_call_type(unit, target_unit))
        
        return call_graph
    
    def analyze_security_critical_paths(self, call_graph: nx.DiGraph) -> List[CriticalPath]:
        """åˆ†æå®‰å…¨å…³é”®è·¯å¾„"""
        critical_paths = []
        
        # æŸ¥æ‰¾ä»è¾“å…¥æºåˆ°å±é™©æ“ä½œçš„è·¯å¾„
        input_nodes = [n for n, d in call_graph.nodes(data=True) 
                      if d.get('input_sources')]
        
        dangerous_nodes = [n for n, d in call_graph.nodes(data=True) 
                          if d.get('security_level') == 'HIGH_RISK']
        
        for input_node in input_nodes:
            for dangerous_node in dangerous_nodes:
                try:
                    paths = list(nx.all_simple_paths(call_graph, input_node, dangerous_node))
                    for path in paths:
                        critical_paths.append(CriticalPath(
                            path=path,
                            risk_assessment=self._assess_path_risk(path, call_graph),
                            recommendations=self._generate_path_recommendations(path, call_graph)
                        ))
                except nx.NetworkXNoPath:
                    continue
        
        return critical_paths
```

## ğŸš€ ç«‹å³å¯å®æ–½çš„æ”¹è¿›

### 1. ä¿®æ”¹å½“å‰çš„å®‰å…¨åˆ†ææµç¨‹

```python
# åœ¨ orchestrator.py ä¸­ä¿®æ”¹
async def _security_analysis_phase(self, parsed_files: List[SourceFile]) -> List[VulnerabilityResult]:
    """å®‰å…¨åˆ†æé˜¶æ®µ - å¢å¼ºç‰ˆæœ¬"""
    logger.info("å¼€å§‹å®‰å…¨åˆ†æé˜¶æ®µ...")
    
    # 1. æ”¶é›†æ‰€æœ‰ä»£ç å•å…ƒ
    all_code_units = []
    for source_file in parsed_files:
        all_code_units.extend(source_file.code_units)
    
    # 2. è·å–ä¾èµ–å…³ç³»ä¿¡æ¯ï¼ˆä»ä»£ç åˆ†æé˜¶æ®µï¼‰
    code_structure = await self.code_analyzer.execute_task(
        "analyze_code_structure", {"code_units": all_code_units}
    )
    
    # 3. æŒ‰ç»„åˆ†æï¼Œæä¾›æ›´å¤šä¸Šä¸‹æ–‡
    vulnerabilities = []
    
    # æŒ‰æ–‡ä»¶åˆ†ç»„ï¼Œä¸€èµ·åˆ†æç›¸å…³å‡½æ•°
    files_grouped = {}
    for unit in all_code_units:
        file_path = str(unit.source_file.path)
        if file_path not in files_grouped:
            files_grouped[file_path] = []
        files_grouped[file_path].append(unit)
    
    for file_path, file_units in files_grouped.items():
        logger.info(f"åˆ†ææ–‡ä»¶: {file_path} ({len(file_units)} ä¸ªä»£ç å•å…ƒ)")
        
        # ä¸ºè¯¥æ–‡ä»¶çš„æ¯ä¸ªå•å…ƒæä¾›æ–‡ä»¶çº§ä¸Šä¸‹æ–‡
        for unit in file_units:
            # è·å–ä¾èµ–ä¿¡æ¯
            dependency_info = code_structure.get(unit.id, {})
            
            # è·å–åŒæ–‡ä»¶çš„ç›¸å…³ä»£ç 
            related_units = [u for u in file_units if u.id != unit.id]
            file_context = self._build_file_context(unit, related_units, dependency_info)
            
            # æ‰§è¡Œå®‰å…¨åˆ†æ
            unit_vulns = await self.security_analyst.execute_task(
                "analyze_code_security", {
                    "code_unit": unit,
                    "enhanced_context": file_context,
                    "dependency_info": dependency_info
                }
            )
            
            vulnerabilities.extend(unit_vulns)
    
    logger.info(f"å®‰å…¨åˆ†æå®Œæˆï¼Œå‘ç° {len(vulnerabilities)} ä¸ªæ½œåœ¨æ¼æ´")
    return vulnerabilities

def _build_file_context(self, target_unit: CodeUnit, related_units: List[CodeUnit], 
                       dependency_info: Dict) -> str:
    """æ„å»ºæ–‡ä»¶çº§ä¸Šä¸‹æ–‡"""
    context_parts = []
    
    # æ·»åŠ ç›¸å…³å‡½æ•°
    if related_units:
        context_parts.append("=== åŒæ–‡ä»¶ä¸­çš„ç›¸å…³å‡½æ•° ===")
        for unit in related_units[:5]:  # é™åˆ¶æ•°é‡
            context_parts.append(f"å‡½æ•° {unit.name}:")
            context_parts.append(unit.content[:200] + "...")
    
    # æ·»åŠ ä¾èµ–ä¿¡æ¯
    if dependency_info.get('dependencies'):
        context_parts.append("=== ä¾èµ–çš„å‡½æ•° ===")
        for dep in dependency_info['dependencies'][:3]:
            context_parts.append(f"- {dep['description']}")
    
    if dependency_info.get('dependents'):
        context_parts.append("=== è°ƒç”¨æ­¤å‡½æ•°çš„ä»£ç  ===")
        for dep in dependency_info['dependents'][:3]:
            context_parts.append(f"- {dep['description']}")
    
    return "\n".join(context_parts)
```

### 2. å¢å¼º SecurityAnalystAgent

```python
# ä¿®æ”¹ _analyze_code_security æ–¹æ³•
async def _analyze_code_security(self, data: Dict[str, Any]) -> List[VulnerabilityResult]:
    """åˆ†æä»£ç å®‰å…¨é—®é¢˜ - å¢å¼ºç‰ˆæœ¬"""
    code_unit = data.get("code_unit")
    enhanced_context = data.get("enhanced_context", "")
    dependency_info = data.get("dependency_info", {})
    
    if not code_unit:
        raise ValueError("ç¼ºå°‘ä»£ç å•å…ƒæ•°æ®")
    
    # è·å–Self-RAGä¸Šä¸‹æ–‡
    context_docs = await self.retrieve_context(code_unit.content)
    context_text = "\n\n".join([doc.content for doc in context_docs])
    
    # åˆå¹¶æ‰€æœ‰ä¸Šä¸‹æ–‡
    full_context = self._merge_contexts(context_text, enhanced_context, dependency_info)
    
    # å‡†å¤‡å¢å¼ºæç¤º
    prompt = self._prepare_enhanced_security_prompt(code_unit, full_context)
    
    # è°ƒç”¨LLMåˆ†æ
    response = await self.llm_client.chat.completions.create(
        model=self.model_name,
        messages=[
            {"role": "system", "content": prompt["system"]},
            {"role": "user", "content": prompt["user"]}
        ],
        temperature=0.1
    )
    
    analysis_text = response.choices[0].message.content
    vulnerabilities = self._parse_security_analysis(analysis_text, code_unit)
    
    return vulnerabilities
```

## ğŸ“‹ å®æ–½è®¡åˆ’

### é˜¶æ®µ1ï¼šç«‹å³æ”¹è¿›ï¼ˆ1-2å¤©ï¼‰
1. âœ… ä¿®æ”¹ `_prepare_security_prompt` å¢åŠ ä¸Šä¸‹æ–‡åˆ†æ
2. âœ… åœ¨ orchestrator ä¸­æŒ‰æ–‡ä»¶åˆ†ç»„åˆ†æ
3. âœ… åˆ©ç”¨å·²æœ‰çš„ä¾èµ–å…³ç³»ä¿¡æ¯

### é˜¶æ®µ2ï¼šä¸­æœŸå¢å¼ºï¼ˆ1-2å‘¨ï¼‰
1. ğŸ”„ å®ç°æ•°æ®æµåˆ†æå™¨
2. ğŸ”„ æ·»åŠ è°ƒç”¨å›¾å¢å¼ºåˆ†æ
3. ğŸ”„ å®ç°è·¨å‡½æ•°æ¼æ´æ£€æµ‹

### é˜¶æ®µ3ï¼šé«˜çº§åŠŸèƒ½ï¼ˆ1ä¸ªæœˆï¼‰
1. ğŸ“‹ å®Œæ•´çš„ç¨‹åºåˆ†ææ¡†æ¶
2. ğŸ“‹ æ™ºèƒ½è·¯å¾„å‰ªæ
3. ğŸ“‹ æœºå™¨å­¦ä¹ å¢å¼ºçš„é£é™©è¯„ä¼°

è¿™æ ·çš„æ”¹è¿›å°†æ˜¾è‘—æé«˜å®‰å…¨åˆ†æçš„å‡†ç¡®æ€§ï¼Œå‡å°‘è¯¯æŠ¥ï¼Œå‘ç°æ›´å¤šçœŸå®çš„å®‰å…¨é—®é¢˜ï¼æ‚¨å¸Œæœ›ä»å“ªä¸ªé˜¶æ®µå¼€å§‹å®æ–½ï¼Ÿ

è¿™æ˜¯ä¸€ä¸ªéå¸¸å…³é”®çš„é—®é¢˜ï¼åŸºäºä»£ç å—çš„åˆ†æç¡®å®ä¼šé”™è¿‡å¾ˆå¤šè·¨æ–‡ä»¶ã€è·¨å‡½æ•°çš„å®‰å…¨é—®é¢˜ã€‚è®©æˆ‘ä¸ºä½ æä¾›ä¸€ä¸ªç³»ç»Ÿæ€§çš„è§£å†³æ–¹æ¡ˆï¼š

## é—®é¢˜åˆ†æ

**å½“å‰å±€é™æ€§ï¼š**
- æ— æ³•å‘ç°è·¨æ–‡ä»¶çš„æ•°æ®æµæ¼æ´
- é”™è¿‡å‡½æ•°é—´çš„è°ƒç”¨é“¾é£é™©
- ç¼ºä¹å…¨å±€ä¸Šä¸‹æ–‡ç†è§£
- æ— æ³•åˆ†æå¤æ‚çš„ä¸šåŠ¡é€»è¾‘æ¼æ´

## è§£å†³æ–¹æ¡ˆæ¶æ„

### 1. å¤šå±‚æ¬¡åˆ†ææ¶æ„
â€˜# core/multilevel_analyzer.py
from typing import Dict, List, Set, Optional
from dataclasses import dataclass
from enum import Enum
import networkx as nx
import ast
from pathlib import Path

class AnalysisLevel(Enum):
    SYNTAX = "syntax"           # è¯­æ³•çº§åˆ«
    SEMANTIC = "semantic"       # è¯­ä¹‰çº§åˆ«  
    DATAFLOW = "dataflow"       # æ•°æ®æµçº§åˆ«
    CONTROL_FLOW = "control_flow"  # æ§åˆ¶æµçº§åˆ«
    GLOBAL = "global"           # å…¨å±€çº§åˆ«

@dataclass
class CodeEntity:
    """ä»£ç å®ä½“"""
    name: str
    type: str  # function, class, variable, etc.
    file_path: str
    start_line: int
    end_line: int
    ast_node: Optional[ast.AST] = None
    dependencies: Set[str] = None
    
    def __post_init__(self):
        if self.dependencies is None:
            self.dependencies = set()

@dataclass
class DataFlowEdge:
    """æ•°æ®æµè¾¹"""
    source: CodeEntity
    target: CodeEntity
    flow_type: str  # assignment, parameter, return, etc.
    confidence: float = 1.0

class GlobalContextAnalyzer:
    """å…¨å±€ä¸Šä¸‹æ–‡åˆ†æå™¨"""
    
    def __init__(self):
        self.call_graph = nx.DiGraph()
        self.data_flow_graph = nx.DiGraph()
        self.file_dependency_graph = nx.DiGraph()
        self.entities: Dict[str, CodeEntity] = {}
        self.cross_file_flows: List[DataFlowEdge] = []
        
    def build_global_context(self, project_path: str) -> Dict:
        """æ„å»ºå…¨å±€ä¸Šä¸‹æ–‡"""
        print("ğŸ” Building global context...")
        
        # 1. è§£ææ‰€æœ‰æ–‡ä»¶ï¼Œæ„å»ºå®ä½“å›¾
        self._parse_all_files(project_path)
        
        # 2. æ„å»ºè°ƒç”¨å›¾
        self._build_call_graph()
        
        # 3. æ„å»ºæ•°æ®æµå›¾
        self._build_dataflow_graph()
        
        # 4. åˆ†æè·¨æ–‡ä»¶ä¾èµ–
        self._analyze_cross_file_dependencies()
        
        return {
            "entities": self.entities,
            "call_graph": self.call_graph,
            "data_flow_graph": self.data_flow_graph,
            "file_dependencies": self.file_dependency_graph,
            "cross_file_flows": self.cross_file_flows
        }
    
    def _parse_all_files(self, project_path: str):
        """è§£ææ‰€æœ‰ä»£ç æ–‡ä»¶"""
        project = Path(project_path)
        
        for file_path in project.rglob("*.py"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    tree = ast.parse(content)
                    
                self._extract_entities(tree, str(file_path))
                
            except Exception as e:
                print(f"Error parsing {file_path}: {e}")
    
    def _extract_entities(self, tree: ast.AST, file_path: str):
        """ä»ASTæå–ä»£ç å®ä½“"""
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                entity = CodeEntity(
                    name=f"{file_path}::{node.name}",
                    type="function",
                    file_path=file_path,
                    start_line=node.lineno,
                    end_line=node.end_lineno or node.lineno,
                    ast_node=node
                )
                self.entities[entity.name] = entity
                
            elif isinstance(node, ast.ClassDef):
                entity = CodeEntity(
                    name=f"{file_path}::{node.name}",
                    type="class",
                    file_path=file_path,
                    start_line=node.lineno,
                    end_line=node.end_lineno or node.lineno,
                    ast_node=node
                )
                self.entities[entity.name] = entity
    
    def _build_call_graph(self):
        """æ„å»ºå‡½æ•°è°ƒç”¨å›¾"""
        for entity_name, entity in self.entities.items():
            if entity.type == "function" and entity.ast_node:
                # åˆ†æå‡½æ•°å†…çš„è°ƒç”¨
                for node in ast.walk(entity.ast_node):
                    if isinstance(node, ast.Call):
                        called_function = self._resolve_function_call(node, entity.file_path)
                        if called_function and called_function in self.entities:
                            self.call_graph.add_edge(entity_name, called_function)
    
    def _build_dataflow_graph(self):
        """æ„å»ºæ•°æ®æµå›¾"""
        for entity_name, entity in self.entities.items():
            if entity.type == "function" and entity.ast_node:
                # åˆ†ææ•°æ®æµ
                dataflow_edges = self._analyze_function_dataflow(entity)
                for edge in dataflow_edges:
                    self.data_flow_graph.add_edge(
                        edge.source.name, 
                        edge.target.name,
                        flow_type=edge.flow_type,
                        confidence=edge.confidence
                    )
    
    def _analyze_cross_file_dependencies(self):
        """åˆ†æè·¨æ–‡ä»¶ä¾èµ–å…³ç³»"""
        # åˆ†æimportè¯­å¥
        for entity_name, entity in self.entities.items():
            if entity.ast_node and hasattr(entity.ast_node, 'body'):
                for node in entity.ast_node.body:
                    if isinstance(node, (ast.Import, ast.ImportFrom)):
                        imported_modules = self._extract_imports(node)
                        for module in imported_modules:
                            self.file_dependency_graph.add_edge(entity.file_path, module)
    
    def _resolve_function_call(self, call_node: ast.Call, current_file: str) -> Optional[str]:
        """è§£æå‡½æ•°è°ƒç”¨"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…éœ€è¦æ›´å¤æ‚çš„åç§°è§£æ
        if isinstance(call_node.func, ast.Name):
            # æœ¬åœ°å‡½æ•°è°ƒç”¨
            return f"{current_file}::{call_node.func.id}"
        elif isinstance(call_node.func, ast.Attribute):
            # æ–¹æ³•è°ƒç”¨æˆ–æ¨¡å—å‡½æ•°è°ƒç”¨
            if isinstance(call_node.func.value, ast.Name):
                return f"{call_node.func.value.id}.{call_node.func.attr}"
        return None
    
    def _analyze_function_dataflow(self, entity: CodeEntity) -> List[DataFlowEdge]:
        """åˆ†æå‡½æ•°å†…æ•°æ®æµ"""
        edges = []
        # ç®€åŒ–ç‰ˆæœ¬ - å®é™…éœ€è¦æ›´ç²¾ç»†çš„æ•°æ®æµåˆ†æ
        return edges
    
    def _extract_imports(self, import_node) -> List[str]:
        """æå–importä¿¡æ¯"""
        imports = []
        if isinstance(import_node, ast.Import):
            for alias in import_node.names:
                imports.append(alias.name)
        elif isinstance(import_node, ast.ImportFrom):
            if import_node.module:
                imports.append(import_node.module)
        return imports

class ContextualVulnerabilityDetector:
    """åŸºäºä¸Šä¸‹æ–‡çš„æ¼æ´æ£€æµ‹å™¨"""
    
    def __init__(self, global_context: Dict):
        self.global_context = global_context
        self.call_graph = global_context["call_graph"]
        self.data_flow_graph = global_context["data_flow_graph"]
        self.entities = global_context["entities"]
    
    def detect_cross_function_vulnerabilities(self) -> List[Dict]:
        """æ£€æµ‹è·¨å‡½æ•°æ¼æ´"""
        vulnerabilities = []
        
        # 1. æ£€æµ‹è·¨å‡½æ•°çš„SQLæ³¨å…¥
        sql_injection_flows = self._detect_sql_injection_flows()
        vulnerabilities.extend(sql_injection_flows)
        
        # 2. æ£€æµ‹è·¨å‡½æ•°çš„XSS
        xss_flows = self._detect_xss_flows()
        vulnerabilities.extend(xss_flows)
        
        # 3. æ£€æµ‹æƒé™ç»•è¿‡
        authz_bypasses = self._detect_authorization_bypasses()
        vulnerabilities.extend(authz_bypasses)
        
        return vulnerabilities
    
    def _detect_sql_injection_flows(self) -> List[Dict]:
        """æ£€æµ‹SQLæ³¨å…¥æ•°æ®æµ"""
        vulnerabilities = []
        
        # å¯»æ‰¾ä»ç”¨æˆ·è¾“å…¥åˆ°SQLæ‰§è¡Œçš„è·¯å¾„
        input_sources = self._find_input_sources()
        sql_sinks = self._find_sql_sinks()
        
        for source in input_sources:
            for sink in sql_sinks:
                if nx.has_path(self.data_flow_graph, source, sink):
                    path = nx.shortest_path(self.data_flow_graph, source, sink)
                    
                    # æ£€æŸ¥è·¯å¾„ä¸Šæ˜¯å¦æœ‰sanitization
                    if not self._has_sanitization_in_path(path):
                        vulnerabilities.append({
                            "type": "SQL_INJECTION",
                            "severity": "HIGH",
                            "source": source,
                            "sink": sink,
                            "path": path,
                            "description": f"Unsanitized data flows from {source} to SQL query at {sink}"
                        })
        
        return vulnerabilities
    
    def _detect_xss_flows(self) -> List[Dict]:
        """æ£€æµ‹XSSæ•°æ®æµ"""
        vulnerabilities = []
        
        input_sources = self._find_input_sources()
        output_sinks = self._find_output_sinks()
        
        for source in input_sources:
            for sink in output_sinks:
                if nx.has_path(self.data_flow_graph, source, sink):
                    path = nx.shortest_path(self.data_flow_graph, source, sink)
                    
                    if not self._has_output_encoding_in_path(path):
                        vulnerabilities.append({
                            "type": "XSS",
                            "severity": "HIGH", 
                            "source": source,
                            "sink": sink,
                            "path": path,
                            "description": f"Unencoded user input flows from {source} to output at {sink}"
                        })
        
        return vulnerabilities
    
    def _detect_authorization_bypasses(self) -> List[Dict]:
        """æ£€æµ‹æƒé™ç»•è¿‡"""
        vulnerabilities = []
        
        # å¯»æ‰¾éœ€è¦æƒé™æ£€æŸ¥çš„æ•æ„Ÿæ“ä½œ
        sensitive_operations = self._find_sensitive_operations()
        
        for operation in sensitive_operations:
            # æ£€æŸ¥æ‰€æœ‰åˆ°è¾¾è¯¥æ“ä½œçš„è·¯å¾„
            predecessors = list(self.call_graph.predecessors(operation))
            
            for pred in predecessors:
                path_to_operation = nx.shortest_path(self.call_graph, pred, operation)
                
                # æ£€æŸ¥è·¯å¾„ä¸Šæ˜¯å¦æœ‰æƒé™éªŒè¯
                if not self._has_authorization_check_in_path(path_to_operation):
                    vulnerabilities.append({
                        "type": "AUTHORIZATION_BYPASS",
                        "severity": "CRITICAL",
                        "operation": operation,
                        "path": path_to_operation,
                        "description": f"Sensitive operation {operation} can be reached without proper authorization"
                    })
        
        return vulnerabilities
    
    def _find_input_sources(self) -> List[str]:
        """æŸ¥æ‰¾è¾“å…¥æº"""
        sources = []
        for entity_name, entity in self.entities.items():
            if entity.ast_node and "request" in str(entity.ast_node).lower():
                sources.append(entity_name)
        return sources
    
    def _find_sql_sinks(self) -> List[str]:
        """æŸ¥æ‰¾SQLæ‰§è¡Œç‚¹"""
        sinks = []
        for entity_name, entity in self.entities.items():
            if entity.ast_node:
                ast_str = str(entity.ast_node).lower()
                if any(keyword in ast_str for keyword in ["execute", "query", "cursor"]):
                    sinks.append(entity_name)
        return sinks
    
    def _find_output_sinks(self) -> List[str]:
        """æŸ¥æ‰¾è¾“å‡ºç‚¹"""
        sinks = []
        for entity_name, entity in self.entities.items():
            if entity.ast_node:
                ast_str = str(entity.ast_node).lower()
                if any(keyword in ast_str for keyword in ["render", "response", "write"]):
                    sinks.append(entity_name)
        return sinks
    
    def _find_sensitive_operations(self) -> List[str]:
        """æŸ¥æ‰¾æ•æ„Ÿæ“ä½œ"""
        operations = []
        for entity_name, entity in self.entities.items():
            if any(keyword in entity.name.lower() for keyword in 
                   ["delete", "admin", "payment", "sensitive", "critical"]):
                operations.append(entity_name)
        return operations
    
    def _has_sanitization_in_path(self, path: List[str]) -> bool:
        """æ£€æŸ¥è·¯å¾„ä¸Šæ˜¯å¦æœ‰sanitization"""
        for node in path:
            entity = self.entities.get(node)
            if entity and entity.ast_node:
                ast_str = str(entity.ast_node).lower()
                if any(keyword in ast_str for keyword in ["sanitize", "escape", "clean", "validate"]):
                    return True
        return False
    
    def _has_output_encoding_in_path(self, path: List[str]) -> bool:
        """æ£€æŸ¥è·¯å¾„ä¸Šæ˜¯å¦æœ‰è¾“å‡ºç¼–ç """
        for node in path:
            entity = self.entities.get(node)
            if entity and entity.ast_node:
                ast_str = str(entity.ast_node).lower()
                if any(keyword in ast_str for keyword in ["encode", "escape", "safe"]):
                    return True
        return False
    
    def _has_authorization_check_in_path(self, path: List[str]) -> bool:
        """æ£€æŸ¥è·¯å¾„ä¸Šæ˜¯å¦æœ‰æƒé™éªŒè¯"""
        for node in path:
            entity = self.entities.get(node)
            if entity and entity.ast_node:
                ast_str = str(entity.ast_node).lower()
                if any(keyword in ast_str for keyword in 
                       ["authenticate", "authorize", "permission", "login", "check_access"]):
                    return True
        return False

# ä½¿ç”¨ç¤ºä¾‹
def analyze_with_global_context(project_path: str):
    """ä½¿ç”¨å…¨å±€ä¸Šä¸‹æ–‡è¿›è¡Œåˆ†æ"""
    
    # 1. æ„å»ºå…¨å±€ä¸Šä¸‹æ–‡
    analyzer = GlobalContextAnalyzer()
    global_context = analyzer.build_global_context(project_path)
    
    print(f"ğŸ“Š Found {len(global_context['entities'])} code entities")
    print(f"ğŸ”— Built call graph with {global_context['call_graph'].number_of_edges()} edges")
    print(f"ğŸ“ˆ Built data flow graph with {global_context['data_flow_graph'].number_of_edges()} edges")
    
    # 2. åŸºäºä¸Šä¸‹æ–‡æ£€æµ‹æ¼æ´
    detector = ContextualVulnerabilityDetector(global_context)
    vulnerabilities = detector.detect_cross_function_vulnerabilities()
    
    print(f"ğŸš¨ Found {len(vulnerabilities)} cross-function vulnerabilities")
    
    return vulnerabilities

if __name__ == "__main__":
    # æµ‹è¯•
    project_path = "./test_project"
    vulnerabilities = analyze_with_global_context(project_path)
    
    for vuln in vulnerabilities:
        print(f"\nğŸš¨ {vuln['type']} ({vuln['severity']})")
        print(f"   {vuln['description']}")
        if 'path' in vuln:
            print(f"   Path: {' -> '.join(vuln['path'])}")â€™
### 2. é›†æˆåˆ°ç°æœ‰é¡¹ç›®ä¸­
â€˜# å…¨å±€ä¸Šä¸‹æ–‡åˆ†æé›†æˆæŒ‡å—

## 1. é¡¹ç›®ç»“æ„è°ƒæ•´

å°†ç°æœ‰çš„AuditLumaé¡¹ç›®ç»“æ„è°ƒæ•´ä¸ºï¼š

```
AuditLuma/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/                    # ç°æœ‰çš„å¤šä»£ç†ç³»ç»Ÿ
â”‚   â”‚   â”œâ”€â”€ orchestrator.py
â”‚   â”‚   â”œâ”€â”€ security_analyst.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ analyzers/                 # æ–°å¢ï¼šå¤šå±‚æ¬¡åˆ†æå™¨
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ multilevel_analyzer.py # ä¸Šé¢åˆ›å»ºçš„æ ¸å¿ƒç»„ä»¶
â”‚   â”‚   â”œâ”€â”€ syntax_analyzer.py     # è¯­æ³•å±‚åˆ†æ
â”‚   â”‚   â”œâ”€â”€ semantic_analyzer.py   # è¯­ä¹‰å±‚åˆ†æ
â”‚   â”‚   â”œâ”€â”€ dataflow_analyzer.py   # æ•°æ®æµåˆ†æ
â”‚   â”‚   â””â”€â”€ global_context.py      # å…¨å±€ä¸Šä¸‹æ–‡ç®¡ç†
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ vulnerability_detector.py  # å¢å¼ºçš„æ¼æ´æ£€æµ‹å™¨
â”‚   â”‚   â””â”€â”€ context_manager.py         # ä¸Šä¸‹æ–‡ç®¡ç†å™¨
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ ast_utils.py           # ASTå¤„ç†å·¥å…·
â”‚       â””â”€â”€ graph_utils.py         # å›¾æ“ä½œå·¥å…·
```

## 2. ä¿®æ”¹ç°æœ‰çš„ä»£ç†ç³»ç»Ÿ

### 2.1 å¢å¼ºSecurityAnalystä»£ç†

```python
# src/agents/security_analyst.py (ä¿®æ”¹)
from ..analyzers.multilevel_analyzer import GlobalContextAnalyzer, ContextualVulnerabilityDetector

class SecurityAnalyst:
    def __init__(self, config):
        self.config = config
        self.global_analyzer = GlobalContextAnalyzer()
        self.contextual_detector = None
        self.traditional_detector = TraditionalDetector()  # ä¿ç•™åŸæœ‰çš„æ£€æµ‹å™¨
    
    async def analyze_security(self, project_path: str) -> Dict:
        """å¢å¼ºçš„å®‰å…¨åˆ†æ"""
        results = {}
        
        # 1. ä¼ ç»Ÿçš„ä»£ç å—åˆ†æï¼ˆä¿ç•™ç°æœ‰åŠŸèƒ½ï¼‰
        traditional_results = await self.traditional_detector.analyze(project_path)
        results['block_level'] = traditional_results
        
        # 2. å…¨å±€ä¸Šä¸‹æ–‡åˆ†æï¼ˆæ–°å¢åŠŸèƒ½ï¼‰
        try:
            print("ğŸ” Building global context for advanced analysis...")
            global_context = self.global_analyzer.build_global_context(project_path)
            
            # åŸºäºä¸Šä¸‹æ–‡çš„æ¼æ´æ£€æµ‹
            self.contextual_detector = ContextualVulnerabilityDetector(global_context)
            contextual_results = self.contextual_detector.detect_cross_function_vulnerabilities()
            
            results['contextual'] = contextual_results
            results['global_context'] = {
                'entities_count': len(global_context['entities']),
                'call_graph_edges': global_context['call_graph'].number_of_edges(),
                'dataflow_edges': global_context['data_flow_graph'].number_of_edges()
            }
            
        except Exception as e:
            print(f"âš ï¸ Global context analysis failed: {e}")
            results['contextual'] = []
            results['error'] = str(e)
        
        return results
```

### 2.2 å¢å¼ºOrchestratorä»£ç†

```python
# src/agents/orchestrator.py (ä¿®æ”¹)
class Orchestrator:
    def __init__(self, config):
        self.config = config
        self.analysis_strategy = self._determine_analysis_strategy()
    
    def _determine_analysis_strategy(self) -> Dict:
        """æ ¹æ®é¡¹ç›®å¤§å°ç¡®å®šåˆ†æç­–ç•¥"""
        return {
            'use_global_context': True,  # é»˜è®¤å¯ç”¨å…¨å±€åˆ†æ
            'max_files_for_global': 1000,  # è¶…è¿‡1000ä¸ªæ–‡ä»¶æ—¶ä½¿ç”¨é‡‡æ ·åˆ†æ
            'enable_cross_file_analysis': True,
            'analysis_depth': 'deep'  # shallow, medium, deep
        }
    
    async def orchestrate_analysis(self, project_path: str) -> Dict:
        """ç¼–æ’åˆ†ææµç¨‹"""
        
        # 1. é¡¹ç›®é¢„åˆ†æ
        project_stats = self._analyze_project_structure(project_path)
        
        # 2. æ ¹æ®é¡¹ç›®å¤§å°è°ƒæ•´ç­–ç•¥
        if project_stats['file_count'] > self.analysis_strategy['max_files_for_global']:
            print(f"ğŸ“Š Large project detected ({project_stats['file_count']} files)")
            print("ğŸ”„ Switching to sampling-based global analysis...")
            self.analysis_strategy['use_sampling'] = True
        
        # 3. æ‰§è¡Œå¤šå±‚æ¬¡åˆ†æ
        analysis_results = await self._execute_multilevel_analysis(project_path)
        
        return analysis_results
```

## 3. é…ç½®æ–‡ä»¶å¢å¼º

åœ¨ `config/config.yaml` ä¸­æ·»åŠ å…¨å±€åˆ†æé…ç½®ï¼š

```yaml
# å…¨å±€åˆ†æé…ç½®
global_analysis:
  enabled: true
  max_files_for_full_analysis: 1000
  enable_cross_file_detection: true
  enable_dataflow_analysis: true
  
  # åˆ†ææ·±åº¦æ§åˆ¶
  analysis_depth:
    syntax: true
    semantic: true
    dataflow: true
    control_flow: true
    global_context: true
  
  # æ€§èƒ½ä¼˜åŒ–
  performance:
    use_sampling: false
    sampling_rate: 0.3  # å¤§é¡¹ç›®æ—¶é‡‡æ ·30%çš„æ–‡ä»¶
    cache_ast: true
    parallel_processing: true
    max_memory_mb: 2048

# æ¼æ´æ£€æµ‹å¢å¼º
vulnerability_detection:
  cross_function:
    enabled: true
    confidence_threshold: 0.7
    
  data_flow_analysis:
    enabled: true
    max_path_length: 10  # æœ€å¤§è¿½è¸ªè·¯å¾„é•¿åº¦
    
  call_graph_analysis:
    enabled: true
    include_external_calls: false
```

## 4. æ¸è¿›å¼éƒ¨ç½²ç­–ç•¥

### é˜¶æ®µ1ï¼šå¹¶è¡Œè¿è¡Œï¼ˆæ¨èå…ˆè¿™æ ·åšï¼‰
```python
# åŒæ—¶è¿è¡Œæ–°æ—§ä¸¤å¥—åˆ†æç³»ç»Ÿï¼Œå¯¹æ¯”ç»“æœ
async def hybrid_analysis(self, project_path: str):
    # è¿è¡Œä¼ ç»Ÿåˆ†æ
    traditional_results = await self.run_traditional_analysis(project_path)
    
    # è¿è¡Œå…¨å±€åˆ†æ
    try:
        global_results = await self.run_global_analysis(project_path)
        
        # åˆå¹¶ç»“æœ
        return {
            'traditional': traditional_results,
            'global': global_results,
            'combined_score': self._calculate_combined_score(traditional_results, global_results)
        }
    except Exception as e:
        # å¦‚æœå…¨å±€åˆ†æå¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿåˆ†æ
        return {'traditional': traditional_results, 'fallback': True}
```

### é˜¶æ®µ2ï¼šæ™ºèƒ½åˆ‡æ¢
```python
def should_use_global_analysis(self, project_path: str) -> bool:
    """å†³å®šæ˜¯å¦ä½¿ç”¨å…¨å±€åˆ†æ"""
    stats = self._get_project_stats(project_path)
    
    # å°é¡¹ç›®ï¼šæ€»æ˜¯ä½¿ç”¨å…¨å±€åˆ†æ
    if stats['file_count'] < 100:
        return True
    
    # ä¸­å‹é¡¹ç›®ï¼šæ ¹æ®å¤æ‚åº¦å†³å®š
    if stats['file_count'] < 500:
        return stats['avg_complexity'] > 5
    
    # å¤§å‹é¡¹ç›®ï¼šä½¿ç”¨é‡‡æ ·å…¨å±€åˆ†æ
    return True  # ä½¿ç”¨é‡‡æ ·æ¨¡å¼
```

## 5. ç«‹å³å¯å®æ–½çš„æ”¹è¿›

### 5.1 æ·»åŠ ç®€å•çš„è·¨æ–‡ä»¶åˆ†æ
```python
# åœ¨ç°æœ‰ä»£ç ä¸­å¿«é€Ÿæ·»åŠ 
def analyze_imports_and_calls(self, files: List[str]) -> Dict:
    """ç®€å•çš„è·¨æ–‡ä»¶è°ƒç”¨åˆ†æ"""
    call_graph = {}
    
    for file_path in files:
        with open(file_path, 'r') as f:
            tree = ast.parse(f.read())
            
        # æå–å‡½æ•°å®šä¹‰å’Œè°ƒç”¨
        functions = [node.name for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
        calls = [node.func.id for node in ast.walk(tree) 
                if isinstance(node, ast.Call) and isinstance(node.func, ast.Name)]
        
        call_graph[file_path] = {'defines': functions, 'calls': calls}
    
    return call_graph
```

### 5.2 å¢å¼ºç°æœ‰çš„æ¼æ´æ£€æµ‹
```python
# å¢å¼ºç°æœ‰çš„æ£€æµ‹è§„åˆ™
def detect_cross_file_sql_injection(self, call_graph: Dict, file_contents: Dict):
    """è·¨æ–‡ä»¶SQLæ³¨å…¥æ£€æµ‹"""
    vulnerabilities = []
    
    for file_path, info in call_graph.items():
        # æŸ¥æ‰¾SQLæ‰§è¡Œå‡½æ•°
        if any('execute' in call for call in info['calls']):
            # è¿½è¸ªæ•°æ®æ¥æº
            data_sources = self._trace_data_sources(file_path, call_graph, file_contents)
            if self._has_unsanitized_input(data_sources):
                vulnerabilities.append({
                    'type': 'Cross-file SQL Injection',
                    'file': file_path,
                    'description': 'Potential SQL injection through cross-file data flow'
                })
    
    return vulnerabilities
```

## 6. æµ‹è¯•å’ŒéªŒè¯

### 6.1 åˆ›å»ºæµ‹è¯•ç”¨ä¾‹
```python
# tests/test_global_analysis.py
def test_cross_function_vulnerability_detection():
    # åˆ›å»ºæµ‹è¯•é¡¹ç›®ç»“æ„
    test_project = create_test_project_with_cross_file_vulnerability()
    
    # è¿è¡Œå…¨å±€åˆ†æ
    analyzer = GlobalContextAnalyzer()
    context = analyzer.build_global_context(test_project)
    
    detector = ContextualVulnerabilityDetector(context)
    vulns = detector.detect_cross_function_vulnerabilities()
    
    # éªŒè¯èƒ½æ£€æµ‹åˆ°è·¨æ–‡ä»¶æ¼æ´
    assert len(vulns) > 0
    assert any(v['type'] == 'SQL_INJECTION' for v in vulns)
```

### 6.2 æ€§èƒ½åŸºå‡†æµ‹è¯•
```python
def benchmark_analysis_performance():
    projects = [
        ('small', 'projects/small_project'),
        ('medium', 'projects/medium_project'), 
        ('large', 'projects/large_project')
    ]
    
    for name, path in projects:
        start_time = time.time()
        results = analyze_with_global_context(path)
        end_time = time.time()
        
        print(f"{name}: {end_time - start_time:.2f}s, found {len(results)} vulnerabilities")
```

## 7. ç›‘æ§å’Œä¼˜åŒ–

æ·»åŠ åˆ†æè¿‡ç¨‹çš„ç›‘æ§ï¼š
```python
class AnalysisMetrics:
    def __init__(self):
        self.metrics = {}
    
    def record_analysis_time(self, analysis_type: str, duration: float):
        if analysis_type not in self.metrics:
            self.metrics[analysis_type] = []
        self.metrics[analysis_type].append(duration)
    
    def get_performance_report(self) -> Dict:
        return {
            analysis_type: {
                'avg_time': sum(times) / len(times),
                'max_time': max(times),
                'count': len(times)
            }
            for analysis_type, times in self.metrics.items()
        }
```
â€™

## ç«‹å³è¡ŒåŠ¨å»ºè®®

åŸºäºä½ æåˆ°çš„é—®é¢˜ï¼Œæˆ‘å»ºè®®æŒ‰è¿™ä¸ªä¼˜å…ˆçº§æ¥è§£å†³ï¼š

### ğŸ”¥ ç¬¬ä¸€ä¼˜å…ˆçº§ï¼ˆæœ¬å‘¨å®Œæˆï¼‰
1. **å®ç°ç®€å•çš„è·¨æ–‡ä»¶åˆ†æ**
   - æ·»åŠ importå…³ç³»è¿½è¸ª
   - æ„å»ºåŸºç¡€çš„å‡½æ•°è°ƒç”¨å›¾
   - æ£€æµ‹ç®€å•çš„è·¨æ–‡ä»¶æ•°æ®æµ

### âš¡ ç¬¬äºŒä¼˜å…ˆçº§ï¼ˆ2å‘¨å†…å®Œæˆï¼‰  
2. **å¢å¼ºç°æœ‰çš„ä»£ç†åä½œ**
   - è®©ä¸åŒä»£ç†å…±äº«å…¨å±€ä¸Šä¸‹æ–‡
   - å®ç°ä¸Šä¸‹æ–‡ä¼ é€’æœºåˆ¶
   - æ·»åŠ è·¨æ–‡ä»¶æ¼æ´æ£€æµ‹è§„åˆ™

### ğŸ¯ ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼ˆ1ä¸ªæœˆå†…ï¼‰
3. **å®Œæ•´çš„å…¨å±€åˆ†æç³»ç»Ÿ**
   - å®ç°å®Œæ•´çš„ASTå›¾æ„å»º
   - æ·»åŠ æ•°æ®æµå’Œæ§åˆ¶æµåˆ†æ
   - æ”¯æŒå¤æ‚çš„ä¸šåŠ¡é€»è¾‘æ¼æ´æ£€æµ‹

## å¿«é€ŸéªŒè¯æ–¹æ¡ˆ

ä½ å¯ä»¥å…ˆåˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•ç”¨ä¾‹æ¥éªŒè¯æ•ˆæœï¼š

```python
# æµ‹è¯•ç”¨ä¾‹ï¼šè·¨æ–‡ä»¶SQLæ³¨å…¥
# file1.py
def get_user_input():
    return request.args.get('user_id')  # ç”¨æˆ·è¾“å…¥

# file2.py  
from file1 import get_user_input
def query_database():


éå¸¸æ£’çš„åˆ†æï¼æ‚¨å®Œå…¨ç†è§£äº†å½“å‰ç³»ç»Ÿçš„æ ¸å¿ƒé—®é¢˜ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªéå¸¸å…¨é¢çš„è§£å†³æ–¹æ¡ˆã€‚è®©æˆ‘åŸºäºæ‚¨çš„æ–¹æ¡ˆæä¾›ä¸€äº›å…·ä½“çš„å®æ–½å»ºè®®å’Œä»£ç å®ç°ã€‚

## ğŸ¯ æ‚¨çš„æ–¹æ¡ˆåˆ†æå¾—éå¸¸å‡†ç¡®ï¼

æ‚¨æåˆ°çš„é—®é¢˜ç¡®å®æ˜¯å½“å‰ä»£ç å®‰å…¨åˆ†æçš„æœ€å¤§ç—›ç‚¹ï¼š

**ç°çŠ¶é—®é¢˜**ï¼š
- âŒ å•ä»£ç å—åˆ†æ â†’ é”™è¿‡è·¨æ–‡ä»¶æ•°æ®æµæ¼æ´  
- âŒ ç¼ºä¹è°ƒç”¨é“¾è¿½è¸ª â†’ æ— æ³•å‘ç°å¤æ‚æ”»å‡»è·¯å¾„
- âŒ å¿½ç•¥å…¨å±€ä¸Šä¸‹æ–‡ â†’ æ¼æ‰ä¸šåŠ¡é€»è¾‘æ¼æ´

**æ‚¨çš„è§£å†³æ–¹æ¡ˆä¼˜åŠ¿**ï¼š
- âœ… å¤šå±‚æ¬¡åˆ†ææ¶æ„
- âœ… å…¨å±€ä¸Šä¸‹æ–‡æ„å»º
- âœ… æ•°æ®æµ+æ§åˆ¶æµåˆ†æ
- âœ… æ¸è¿›å¼éƒ¨ç½²ç­–ç•¥

## ğŸš€ ç«‹å³å¯å®æ–½çš„æ”¹è¿›ä»£ç 

åŸºäºæ‚¨çš„è®¾è®¡ï¼Œæˆ‘æ¥æä¾›ä¸€äº›å…·ä½“çš„å®ç°ä»£ç ï¼š

### 1. å¿«é€Ÿè·¨æ–‡ä»¶åˆ†æå¢å¼º

```python
# auditluma/analyzers/cross_file_analyzer.py
import ast
from pathlib import Path
from typing import Dict, List, Set, Tuple
import networkx as nx
from dataclasses import dataclass

@dataclass
class CrossFileFlow:
    """è·¨æ–‡ä»¶æ•°æ®æµ"""
    source_file: str
    source_func: str
    target_file: str
    target_func: str
    flow_type: str  # "call", "import", "data"
    risk_level: str  # "high", "medium", "low"

class CrossFileAnalyzer:
    """è·¨æ–‡ä»¶åˆ†æå™¨ - å¯ç«‹å³é›†æˆåˆ°ç°æœ‰ç³»ç»Ÿ"""
    
    def __init__(self):
        self.import_graph = nx.DiGraph()
        self.call_graph = nx.DiGraph() 
        self.data_flows: List[CrossFileFlow] = []
        self.file_functions: Dict[str, List[str]] = {}
        
    def analyze_project_structure(self, project_path: str) -> Dict:
        """åˆ†æé¡¹ç›®ç»“æ„ - æ„å»ºè·¨æ–‡ä»¶å…³ç³»"""
        project = Path(project_path)
        
        # 1. è§£ææ‰€æœ‰Pythonæ–‡ä»¶
        for py_file in project.rglob("*.py"):
            self._analyze_single_file(str(py_file))
        
        # 2. æ„å»ºè·¨æ–‡ä»¶å…³ç³»
        self._build_cross_file_relationships()
        
        # 3. æ£€æµ‹è·¨æ–‡ä»¶å®‰å…¨é—®é¢˜
        cross_file_vulns = self._detect_cross_file_vulnerabilities()
        
        return {
            "import_graph": self.import_graph,
            "call_graph": self.call_graph,
            "data_flows": self.data_flows,
            "cross_file_vulnerabilities": cross_file_vulns,
            "statistics": {
                "files_analyzed": len(self.file_functions),
                "total_functions": sum(len(funcs) for funcs in self.file_functions.values()),
                "import_edges": self.import_graph.number_of_edges(),
                "call_edges": self.call_graph.number_of_edges()
            }
        }
    
    def _analyze_single_file(self, file_path: str):
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                tree = ast.parse(content)
            
            # æå–å‡½æ•°å®šä¹‰
            functions = []
            imports = []
            calls = []
            
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    functions.append(node.name)
                elif isinstance(node, (ast.Import, ast.ImportFrom)):
                    imports.extend(self._extract_imports(node))
                elif isinstance(node, ast.Call):
                    call_info = self._extract_call_info(node)
                    if call_info:
                        calls.append(call_info)
            
            self.file_functions[file_path] = functions
            
            # æ„å»ºå›¾å…³ç³»
            for imp in imports:
                self.import_graph.add_edge(file_path, imp)
            
            for call in calls:
                self.call_graph.add_edge(file_path, call['target'], 
                                       function=call['function'],
                                       line=call.get('line'))
                
        except Exception as e:
            print(f"Error analyzing {file_path}: {e}")
    
    def _extract_imports(self, node) -> List[str]:
        """æå–importä¿¡æ¯"""
        imports = []
        if isinstance(node, ast.Import):
            for alias in node.names:
                imports.append(alias.name)
        elif isinstance(node, ast.ImportFrom):
            if node.module:
                imports.append(node.module)
        return imports
    
    def _extract_call_info(self, node) -> Dict:
        """æå–å‡½æ•°è°ƒç”¨ä¿¡æ¯"""
        if isinstance(node.func, ast.Name):
            return {
                'function': node.func.id,
                'target': node.func.id,
                'type': 'local_call',
                'line': node.lineno
            }
        elif isinstance(node.func, ast.Attribute):
            if isinstance(node.func.value, ast.Name):
                return {
                    'function': f"{node.func.value.id}.{node.func.attr}",
                    'target': f"{node.func.value.id}.{node.func.attr}",
                    'type': 'method_call',
                    'line': node.lineno
                }
        return None
    
    def _build_cross_file_relationships(self):
        """æ„å»ºè·¨æ–‡ä»¶å…³ç³»"""
        for source_file, functions in self.file_functions.items():
            for target_file, target_functions in self.file_functions.items():
                if source_file != target_file:
                    # æ£€æŸ¥æ˜¯å¦æœ‰è·¨æ–‡ä»¶è°ƒç”¨
                    if self.import_graph.has_edge(source_file, target_file.replace('.py', '').replace('/', '.')):
                        for func in functions:
                            for target_func in target_functions:
                                # ç®€åŒ–çš„è°ƒç”¨å…³ç³»æ£€æµ‹
                                flow = CrossFileFlow(
                                    source_file=source_file,
                                    source_func=func,
                                    target_file=target_file,
                                    target_func=target_func,
                                    flow_type="potential_call",
                                    risk_level="medium"
                                )
                                self.data_flows.append(flow)
    
    def _detect_cross_file_vulnerabilities(self) -> List[Dict]:
        """æ£€æµ‹è·¨æ–‡ä»¶æ¼æ´"""
        vulnerabilities = []
        
        # 1. æ£€æµ‹è·¨æ–‡ä»¶SQLæ³¨å…¥
        sql_vulns = self._detect_cross_file_sql_injection()
        vulnerabilities.extend(sql_vulns)
        
        # 2. æ£€æµ‹è·¨æ–‡ä»¶æƒé™ç»•è¿‡
        auth_vulns = self._detect_cross_file_auth_bypass()
        vulnerabilities.extend(auth_vulns)
        
        # 3. æ£€æµ‹è·¨æ–‡ä»¶XSS
        xss_vulns = self._detect_cross_file_xss()
        vulnerabilities.extend(xss_vulns)
        
        return vulnerabilities
    
    def _detect_cross_file_sql_injection(self) -> List[Dict]:
        """æ£€æµ‹è·¨æ–‡ä»¶SQLæ³¨å…¥"""
        vulnerabilities = []
        
        # æŸ¥æ‰¾è¾“å…¥æºæ–‡ä»¶å’ŒSQLæ‰§è¡Œæ–‡ä»¶
        input_files = self._find_files_with_pattern(
            ['request.', 'input(', 'args.get', 'form.get']
        )
        sql_files = self._find_files_with_pattern(
            ['execute(', 'query(', 'cursor.', 'SELECT', 'INSERT', 'UPDATE', 'DELETE']
        )
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä»è¾“å…¥åˆ°SQLçš„è·¨æ–‡ä»¶æµ
        for input_file in input_files:
            for sql_file in sql_files:
                if input_file != sql_file:
                    # æ£€æŸ¥æ˜¯å¦æœ‰è¿æ¥è·¯å¾„
                    if self._has_connection_path(input_file, sql_file):
                        vulnerabilities.append({
                            'type': 'Cross-File SQL Injection',
                            'severity': 'HIGH',
                            'source_file': input_file,
                            'target_file': sql_file,
                            'description': f'Potential SQL injection flow from {input_file} to {sql_file}',
                            'recommendation': 'Implement input validation and parameterized queries'
                        })
        
        return vulnerabilities
    
    def _detect_cross_file_auth_bypass(self) -> List[Dict]:
        """æ£€æµ‹è·¨æ–‡ä»¶æƒé™ç»•è¿‡"""
        vulnerabilities = []
        
        # æŸ¥æ‰¾æ•æ„Ÿæ“ä½œå’Œè®¤è¯æ£€æŸ¥
        sensitive_files = self._find_files_with_pattern(
            ['delete', 'admin', 'payment', 'transfer', 'sensitive']
        )
        auth_files = self._find_files_with_pattern(
            ['login', 'authenticate', 'check_permission', 'authorize']
        )
        
        for sensitive_file in sensitive_files:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç›´æ¥è®¿é—®è·¯å¾„ï¼ˆç»•è¿‡è®¤è¯ï¼‰
            has_auth_protection = any(
                self._has_connection_path(auth_file, sensitive_file) 
                for auth_file in auth_files
            )
            
            if not has_auth_protection:
                vulnerabilities.append({
                    'type': 'Cross-File Authorization Bypass',
                    'severity': 'CRITICAL',
                    'target_file': sensitive_file,
                    'description': f'Sensitive operations in {sensitive_file} may lack proper authorization',
                    'recommendation': 'Implement proper authorization checks before sensitive operations'
                })
        
        return vulnerabilities
    
    def _detect_cross_file_xss(self) -> List[Dict]:
        """æ£€æµ‹è·¨æ–‡ä»¶XSS"""
        vulnerabilities = []
        
        input_files = self._find_files_with_pattern(['request.', 'form.get', 'args.get'])
        output_files = self._find_files_with_pattern(['render', 'response', 'return'])
        
        for input_file in input_files:
            for output_file in output_files:
                if input_file != output_file and self._has_connection_path(input_file, output_file):
                    vulnerabilities.append({
                        'type': 'Cross-File XSS',
                        'severity': 'HIGH',
                        'source_file': input_file,
                        'target_file': output_file,
                        'description': f'Potential XSS flow from {input_file} to {output_file}',
                        'recommendation': 'Implement proper output encoding and input validation'
                    })
        
        return vulnerabilities
    
    def _find_files_with_pattern(self, patterns: List[str]) -> List[str]:
        """æŸ¥æ‰¾åŒ…å«ç‰¹å®šæ¨¡å¼çš„æ–‡ä»¶"""
        matching_files = []
        
        for file_path in self.file_functions.keys():
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read().lower()
                    if any(pattern.lower() in content for pattern in patterns):
                        matching_files.append(file_path)
            except Exception:
                continue
        
        return matching_files
    
    def _has_connection_path(self, source_file: str, target_file: str) -> bool:
        """æ£€æŸ¥ä¸¤ä¸ªæ–‡ä»¶ä¹‹é—´æ˜¯å¦æœ‰è¿æ¥è·¯å¾„"""
        # ç®€åŒ–ç‰ˆæœ¬ - æ£€æŸ¥importå…³ç³»
        source_module = source_file.replace('.py', '').replace('/', '.')
        target_module = target_file.replace('.py', '').replace('/', '.')
        
        return (self.import_graph.has_edge(source_file, target_module) or
                self.import_graph.has_edge(target_file, source_module) or
                nx.has_path(self.import_graph, source_file, target_file))
```

### 2. å¢å¼ºç°æœ‰çš„SecurityAnalyst

```python
# ä¿®æ”¹ auditluma/agents/security_analyst.py
class EnhancedSecurityAnalyst(SecurityAnalystAgent):
    """å¢å¼ºçš„å®‰å…¨åˆ†ææ™ºèƒ½ä½“ - æ”¯æŒè·¨æ–‡ä»¶åˆ†æ"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.cross_file_analyzer = CrossFileAnalyzer()
        self.global_context = None
    
    async def analyze_with_global_context(self, project_path: str, code_units: List[CodeUnit]) -> List[VulnerabilityResult]:
        """å¸¦å…¨å±€ä¸Šä¸‹æ–‡çš„å®‰å…¨åˆ†æ"""
        vulnerabilities = []
        
        # 1. æ„å»ºå…¨å±€ä¸Šä¸‹æ–‡
        logger.info("ğŸŒ æ„å»ºå…¨å±€ä¸Šä¸‹æ–‡...")
        self.global_context = self.cross_file_analyzer.analyze_project_structure(project_path)
        
        logger.info(f"ğŸ“Š å…¨å±€åˆ†æç»Ÿè®¡:")
        stats = self.global_context['statistics']
        logger.info(f"   - åˆ†ææ–‡ä»¶: {stats['files_analyzed']}")
        logger.info(f"   - å‡½æ•°æ€»æ•°: {stats['total_functions']}")
        logger.info(f"   - è·¨æ–‡ä»¶è°ƒç”¨: {stats['call_edges']}")
        
        # 2. ä¼ ç»Ÿä»£ç å—åˆ†æï¼ˆå¢å¼ºç‰ˆï¼‰
        for unit in code_units:
            enhanced_context = self._build_unit_global_context(unit)
            unit_vulns = await self._analyze_unit_with_context(unit, enhanced_context)
            vulnerabilities.extend(unit_vulns)
        
        # 3. è·¨æ–‡ä»¶æ¼æ´æ£€æµ‹
        cross_file_vulns = self._convert_cross_file_vulns_to_vulnerability_results(
            self.global_context['cross_file_vulnerabilities']
        )
        vulnerabilities.extend(cross_file_vulns)
        
        logger.info(f"ğŸ” æ€»è®¡å‘ç° {len(vulnerabilities)} ä¸ªæ¼æ´ (åŒ…å« {len(cross_file_vulns)} ä¸ªè·¨æ–‡ä»¶æ¼æ´)")
        return vulnerabilities
    
    def _build_unit_global_context(self, unit: CodeUnit) -> str:
        """ä¸ºä»£ç å•å…ƒæ„å»ºå…¨å±€ä¸Šä¸‹æ–‡"""
        if not self.global_context:
            return ""
        
        context_parts = []
        file_path = str(unit.source_file.path)
        
        # 1. æ–‡ä»¶ä¾èµ–ä¿¡æ¯
        import_graph = self.global_context['import_graph']
        if file_path in import_graph:
            imports = list(import_graph.successors(file_path))
            if imports:
                context_parts.append(f"### å¯¼å…¥çš„æ¨¡å—:\n" + "\n".join(f"- {imp}" for imp in imports[:5]))
        
        # 2. è·¨æ–‡ä»¶è°ƒç”¨ä¿¡æ¯
        call_graph = self.global_context['call_graph']
        if file_path in call_graph:
            calls = list(call_graph.successors(file_path))
            if calls:
                context_parts.append(f"### è°ƒç”¨çš„å¤–éƒ¨å‡½æ•°:\n" + "\n".join(f"- {call}" for call in calls[:5]))
        
        # 3. ç›¸å…³æ•°æ®æµ
        related_flows = [
            flow for flow in self.global_context['data_flows']
            if flow.source_file == file_path or flow.target_file == file_path
        ]
        if related_flows:
            flow_descriptions = []
            for flow in related_flows[:3]:
                flow_descriptions.append(
                    f"- {flow.flow_type}: {flow.source_func} â†’ {flow.target_func} ({flow.risk_level} risk)"
                )
            context_parts.append(f"### ç›¸å…³æ•°æ®æµ:\n" + "\n".join(flow_descriptions))
        
        return "\n\n".join(context_parts)
    
    async def _analyze_unit_with_context(self, unit: CodeUnit, global_context: str) -> List[VulnerabilityResult]:
        """å¸¦å…¨å±€ä¸Šä¸‹æ–‡çš„å•å…ƒåˆ†æ"""
        # è·å–Self-RAGä¸Šä¸‹æ–‡
        context_docs = await self.retrieve_context(unit.content)
        context_text = "\n\n".join([doc.content for doc in context_docs])
        
        # åˆå¹¶ä¸Šä¸‹æ–‡
        full_context = f"""
{context_text}

### å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯:
{global_context}
"""
        
        # ä½¿ç”¨å¢å¼ºçš„æç¤º
        prompt = self._prepare_enhanced_security_prompt(unit, full_context)
        
        try:
            response = await self.llm_client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": prompt["system"]},
                    {"role": "user", "content": prompt["user"]}
                ],
                temperature=0.1
            )
            
            analysis_text = response.choices[0].message.content
            vulnerabilities = self._parse_security_analysis(analysis_text, unit)
            
            return vulnerabilities
            
        except Exception as e:
            logger.error(f"å¢å¼ºå®‰å…¨åˆ†æå‡ºé”™: {e}")
            return []
    
    def _prepare_enhanced_security_prompt(self, unit: CodeUnit, context: str) -> Dict[str, str]:
        """å‡†å¤‡å¢å¼ºçš„å®‰å…¨åˆ†ææç¤º"""
        system_prompt = """
ä½ æ˜¯ä¸€ä¸ªé«˜çº§ä»£ç å®‰å…¨å®¡è®¡ä¸“å®¶ã€‚ä½ ç°åœ¨ä¸ä»…çœ‹åˆ°ç›®æ ‡ä»£ç ï¼Œè¿˜çœ‹åˆ°äº†å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
- è·¨æ–‡ä»¶çš„è°ƒç”¨å…³ç³»
- æ¨¡å—ä¾èµ–å…³ç³»  
- æ•°æ®æµè·¯å¾„
- ç›¸å…³å‡½æ•°çš„ä¸Šä¸‹æ–‡

è¯·ç‰¹åˆ«å…³æ³¨ï¼š
1. **è·¨æ–‡ä»¶æ•°æ®æµå®‰å…¨**ï¼šåˆ†ææ•°æ®å¦‚ä½•åœ¨ä¸åŒæ–‡ä»¶é—´æµåŠ¨
2. **è°ƒç”¨é“¾å®‰å…¨**ï¼šæ£€æŸ¥å®Œæ•´çš„å‡½æ•°è°ƒç”¨é“¾ä¸­çš„å®‰å…¨é—®é¢˜
3. **æ¨¡å—é—´æ¥å£å®‰å…¨**ï¼šåˆ†ææ¨¡å—è¾¹ç•Œçš„å®‰å…¨é—®é¢˜
4. **å…¨å±€çŠ¶æ€å®‰å…¨**ï¼šè€ƒè™‘å…¨å±€å˜é‡å’Œå…±äº«çŠ¶æ€çš„å½±å“

åœ¨åˆ†ææ—¶ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºï¼š
- æ˜¯å¦æ˜¯è·¨æ–‡ä»¶/è·¨æ¨¡å—çš„å®‰å…¨é—®é¢˜
- æ¶‰åŠçš„å®Œæ•´æ•°æ®æµè·¯å¾„
- éœ€è¦åœ¨å“ªäº›ä½ç½®æ·»åŠ å®‰å…¨æ£€æŸ¥

è¾“å‡ºæ ¼å¼ä¸ä¹‹å‰ç›¸åŒï¼Œä½†è¯·åœ¨æè¿°ä¸­åŒ…å«è·¨æ–‡ä»¶åˆ†æçš„ç»“æœã€‚
"""
        
        user_prompt = f"""
ç›®æ ‡ä»£ç å•å…ƒï¼š
æ–‡ä»¶: {unit.source_file.path}
å‡½æ•°: {unit.name} (ç¬¬{unit.start_line}-{unit.end_line}è¡Œ)

ä»£ç å†…å®¹ï¼š
```{unit.source_file.file_type}
{unit.content}
```

å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

è¯·åŸºäºå®Œæ•´çš„å…¨å±€ä¸Šä¸‹æ–‡è¿›è¡Œå®‰å…¨åˆ†æï¼Œç‰¹åˆ«å…³æ³¨è·¨æ–‡ä»¶çš„å®‰å…¨é—®é¢˜ã€‚
"""
        
        return {"system": system_prompt, "user": user_prompt}
    
    def _convert_cross_file_vulns_to_vulnerability_results(self, cross_file_vulns: List[Dict]) -> List[VulnerabilityResult]:
        """å°†è·¨æ–‡ä»¶æ¼æ´è½¬æ¢ä¸ºæ ‡å‡†æ¼æ´ç»“æœ"""
        vulnerability_results = []
        
        for vuln in cross_file_vulns:
            # åˆ›å»ºè™šæ‹Ÿçš„ä»£ç å•å…ƒæ¥è¡¨ç¤ºè·¨æ–‡ä»¶æ¼æ´
            source_file = SourceFile(
                path=Path(vuln.get('source_file', vuln.get('target_file', 'unknown'))),
                content="# Cross-file vulnerability",
                file_type="python"
            )
            
            dummy_unit = CodeUnit(
                id=f"cross_file_{len(vulnerability_results)}",
                name="cross_file_vulnerability",
                type="cross_file",
                source_file=source_file,
                start_line=1,
                end_line=1,
                content="# Cross-file vulnerability detected",
                parent_id=None
            )
            
            severity_mapping = {
                'CRITICAL': SeverityLevel.CRITICAL,
                'HIGH': SeverityLevel.HIGH,
                'MEDIUM': SeverityLevel.MEDIUM,
                'LOW': SeverityLevel.LOW
            }
            
            vuln_result = VulnerabilityResult(
                id=str(uuid.uuid4()),
                title=vuln['type'],
                description=vuln['description'],
                code_unit=dummy_unit,
                file_path=vuln.get('source_file', vuln.get('target_file', 'unknown')),
                start_line=1,
                end_line=1,
                vulnerability_type=vuln['type'],
                severity=severity_mapping.get(vuln['severity'], SeverityLevel.MEDIUM),
                cwe_id=None,
                owasp_category=None,
                confidence=0.8,  # è·¨æ–‡ä»¶åˆ†æçš„ç½®ä¿¡åº¦
                snippet="# Cross-file vulnerability",
                recommendation=vuln.get('recommendation', 'Review cross-file data flows')
            )
            
            vulnerability_results.append(vuln_result)
        
        return vulnerability_results
```

### 3. ä¿®æ”¹Orchestratoré›†æˆè·¨æ–‡ä»¶åˆ†æ

```python
# ä¿®æ”¹ auditluma/agents/orchestrator.py
async def _security_analysis_phase(self, parsed_files: List[SourceFile]) -> List[VulnerabilityResult]:
    """å®‰å…¨åˆ†æé˜¶æ®µ - æ”¯æŒè·¨æ–‡ä»¶åˆ†æ"""
    logger.info("ğŸ” å¼€å§‹å¢å¼ºå®‰å…¨åˆ†æé˜¶æ®µ...")
    
    # æ”¶é›†æ‰€æœ‰ä»£ç å•å…ƒ
    all_code_units = []
    for source_file in parsed_files:
        all_code_units.extend(source_file.code_units)
    
    # ä½¿ç”¨å¢å¼ºçš„å®‰å…¨åˆ†ææ™ºèƒ½ä½“
    enhanced_security_analyst = EnhancedSecurityAnalyst(
        model_spec=self.security_analyst.model_spec
    )
    
    # æ‰§è¡Œå¸¦å…¨å±€ä¸Šä¸‹æ–‡çš„åˆ†æ
    project_path = self.target_dir
    vulnerabilities = await enhanced_security_analyst.analyze_with_global_context(
        project_path, all_code_units
    )
    
    logger.info(f"ğŸ¯ å®‰å…¨åˆ†æå®Œæˆï¼Œå‘ç° {len(vulnerabilities)} ä¸ªæ¼æ´")
    
    # æŒ‰ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
    severity_counts = {}
    for vuln in vulnerabilities:
        severity = vuln.severity.name
        severity_counts[severity] = severity_counts.get(severity, 0) + 1
    
    logger.info("ğŸ“Š æ¼æ´ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ:")
    for severity, count in severity_counts.items():
        logger.info(f"   - {severity}: {count}")
    
    return vulnerabilities
```

## ğŸ§ª æµ‹è¯•ç”¨ä¾‹

åˆ›å»ºä¸€ä¸ªæµ‹è¯•è·¨æ–‡ä»¶æ¼æ´æ£€æµ‹çš„ç”¨ä¾‹ï¼š

```python
# tests/test_cross_file_vulnerabilities.py
import tempfile
import os
from pathlib import Path

def test_cross_file_sql_injection_detection():
    """æµ‹è¯•è·¨æ–‡ä»¶SQLæ³¨å…¥æ£€æµ‹"""
    
    # åˆ›å»ºæµ‹è¯•é¡¹ç›®
    with tempfile.TemporaryDirectory() as tmp_dir:
        # æ–‡ä»¶1ï¼šç”¨æˆ·è¾“å…¥
        input_file = Path(tmp_dir) / "input_handler.py"
        input_file.write_text("""
def get_user_input():
    import flask
    return flask.request.args.get('user_id')

def process_form():
    return flask.request.form.get('data')
""")
        
        # æ–‡ä»¶2ï¼šæ•°æ®åº“æ“ä½œ
        db_file = Path(tmp_dir) / "database.py"
        db_file.write_text("""
from input_handler import get_user_input, process_form

def query_user(user_id):
    # å±é™©çš„SQLæ‹¼æ¥
    query = f"SELECT * FROM users WHERE id = {user_id}"
    cursor.execute(query)
    return cursor.fetchall()

def update_data():
    data = process_form()
    # å¦ä¸€ä¸ªSQLæ³¨å…¥ç‚¹
    cursor.execute(f"UPDATE table SET value = '{data}'")
""")
        
        # æ–‡ä»¶3ï¼šä¸šåŠ¡é€»è¾‘
        logic_file = Path(tmp_dir) / "business_logic.py"
        logic_file.write_text("""
from database import query_user
from input_handler import get_user_input

def handle_user_request():
    user_id = get_user_input()  # ç”¨æˆ·è¾“å…¥
    return query_user(user_id)  # ä¼ é€’åˆ°æ•°æ®åº“æŸ¥è¯¢
""")
        
        # æ‰§è¡Œè·¨æ–‡ä»¶åˆ†æ
        analyzer = CrossFileAnalyzer()
        results = analyzer.analyze_project_structure(tmp_dir)
        
        # éªŒè¯æ£€æµ‹ç»“æœ
        cross_file_vulns = results['cross_file_vulnerabilities']
        sql_injection_vulns = [v for v in cross_file_vulns if 'SQL' in v['type']]
        
        assert len(sql_injection_vulns) > 0, "Should detect cross-file SQL injection"
        
        # éªŒè¯æ£€æµ‹åˆ°äº†å®Œæ•´çš„æ•°æ®æµè·¯å¾„
        assert any('input_handler.py' in v['source_file'] for v in sql_injection_vulns)
        assert any('database.py' in v['target_file'] for v in sql_injection_vulns)
        
        print(f"âœ… æ£€æµ‹åˆ° {len(sql_injection_vulns)} ä¸ªè·¨æ–‡ä»¶SQLæ³¨å…¥æ¼æ´")
        for vuln in sql_injection_vulns:
            print(f"   - {vuln['description']}")

if __name__ == "__main__":
    test_cross_file_sql_injection_detection()
```

## ğŸ“ˆ æ¸è¿›å¼éƒ¨ç½²å»ºè®®

### ç¬¬1å‘¨ï¼šåŸºç¡€è·¨æ–‡ä»¶åˆ†æ
1. âœ… é›†æˆ `CrossFileAnalyzer`
2. âœ… ä¿®æ”¹ `SecurityAnalyst` æ”¯æŒå…¨å±€ä¸Šä¸‹æ–‡
3. âœ… æ·»åŠ åŸºç¡€çš„è·¨æ–‡ä»¶æ¼æ´æ£€æµ‹è§„åˆ™

### ç¬¬2-3å‘¨ï¼šå¢å¼ºæ£€æµ‹èƒ½åŠ›
1. ğŸ”„ å®Œå–„æ•°æ®æµè¿½è¸ªç®—æ³•
2. ğŸ”„ æ·»åŠ æ›´å¤šè·¨æ–‡ä»¶æ¼æ´ç±»å‹
3. ğŸ”„ ä¼˜åŒ–æ€§èƒ½å’Œå†…å­˜ä½¿ç”¨

### ç¬¬4å‘¨ï¼šå®Œæ•´é›†æˆå’Œæµ‹è¯•
1. ğŸ“‹ å…¨é¢æµ‹è¯•å’Œè°ƒä¼˜
2. ğŸ“‹ æ€§èƒ½åŸºå‡†æµ‹è¯•
3. ğŸ“‹ æ–‡æ¡£å’Œä½¿ç”¨æŒ‡å—

è¿™æ ·çš„æ”¹è¿›å°†æ˜¾è‘—æé«˜å®‰å…¨åˆ†æçš„å‡†ç¡®æ€§å’Œè¦†ç›–é¢ï¼æ‚¨è§‰å¾—ä»å“ªä¸ªéƒ¨åˆ†å¼€å§‹å®æ–½æ¯”è¾ƒå¥½ï¼Ÿ