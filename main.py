#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
AuditLuma - é«˜çº§ä»£ç å®¡è®¡AIç³»ç»Ÿ
åº”ç”¨ç¨‹åºçš„ä¸»å…¥å£ç‚¹
"""

import argparse
import os
import asyncio
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
import yaml

from loguru import logger

from auditluma.config import Config, load_config
# Import AgentOrchestrator from the orchestrator.py file (not the directory)
import importlib.util
import os
orchestrator_path = os.path.join(os.path.dirname(__file__), 'auditluma', 'orchestrator.py')
spec = importlib.util.spec_from_file_location("auditluma.orchestrator_module", orchestrator_path)
orchestrator_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(orchestrator_module)
AgentOrchestrator = orchestrator_module.AgentOrchestrator
from auditluma.orchestrator.compatibility import UnifiedOrchestrator, ArchitectureMode, create_unified_orchestrator
from auditluma.scanner import CodeScanner
from auditluma.utils import setup_logging, calculate_project_hash
from auditluma.visualizer.report_generator import ReportGenerator
from auditluma.visualizer.graph_visualizer import GraphVisualizer


def init() -> argparse.Namespace:
    """åˆå§‹åŒ–åº”ç”¨ç¨‹åºå¹¶è§£æå‘½ä»¤è¡Œå‚æ•°"""
    # è®¾ç½®æ—¥å¿—è®°å½•
    setup_logging()
    
    # é…ç½®æ–‡ä»¶è·¯å¾„
    current_dir = Path(__file__).parent
    config_path = current_dir / "config" / "config.yaml"
    
    # åŠ è½½é…ç½®
    if config_path.exists():
        load_config(str(config_path))
        logger.info(f"ä» {config_path} åŠ è½½äº†é…ç½®")
    else:
        logger.warning("æœªæ‰¾åˆ°config.yamlï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="AuditLuma - é«˜çº§ä»£ç å®¡è®¡AIç³»ç»Ÿ")
    parser.add_argument("-d", "--directory", type=str, default=Config.get_target_dir(), 
                        help=f"ç›®æ ‡é¡¹ç›®ç›®å½•è·¯å¾„ï¼ˆé»˜è®¤ï¼š{Config.get_target_dir()}ï¼‰")
    parser.add_argument("-o", "--output", type=str, default=Config.get_report_dir(),
                        help=f"æŠ¥å‘Šè¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼š{Config.get_report_dir()}ï¼‰")
    parser.add_argument("-w", "--workers", type=int, default=Config.project.max_batch_size,
                        help=f"å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°ï¼ˆé»˜è®¤ï¼š{Config.project.max_batch_size}ï¼‰")
    parser.add_argument("-f", "--format", type=str, choices=["html", "pdf", "json"], 
                        default=Config.get_report_format(),
                        help=f"æŠ¥å‘Šæ ¼å¼ï¼ˆé»˜è®¤ï¼š{Config.get_report_format()}ï¼‰")
    
    # æ¶æ„é€‰æ‹©å‚æ•°
    parser.add_argument("--architecture", type=str, choices=["traditional", "hierarchical", "auto"], 
                        default="auto",
                        help="é€‰æ‹©RAGæ¶æ„æ¨¡å¼ï¼štraditionalï¼ˆä¼ ç»Ÿï¼‰ã€hierarchicalï¼ˆå±‚çº§ï¼‰ã€autoï¼ˆè‡ªåŠ¨é€‰æ‹©ï¼Œé»˜è®¤ï¼‰")
    parser.add_argument("--force-traditional", action="store_true",
                        help="å¼ºåˆ¶ä½¿ç”¨ä¼ ç»ŸRAGæ¶æ„ï¼ˆç­‰åŒäº --architecture traditionalï¼‰")
    parser.add_argument("--force-hierarchical", action="store_true",
                        help="å¼ºåˆ¶ä½¿ç”¨å±‚çº§RAGæ¶æ„ï¼ˆç­‰åŒäº --architecture hierarchicalï¼‰")
    parser.add_argument("--enable-performance-comparison", action="store_true",
                        help="å¯ç”¨æ€§èƒ½å¯¹æ¯”æ¨¡å¼ï¼ˆåŒæ—¶è¿è¡Œä¸¤ç§æ¶æ„è¿›è¡Œå¯¹æ¯”ï¼‰")
    parser.add_argument("--auto-switch-threshold", type=int, default=100,
                        help="è‡ªåŠ¨åˆ‡æ¢æ¶æ„çš„æ–‡ä»¶æ•°é‡é˜ˆå€¼ï¼ˆé»˜è®¤ï¼š100ï¼‰")
    
    # ä¼ ç»ŸåŠŸèƒ½å‚æ•°
    parser.add_argument("--no-mcp", action="store_true",
                        help="ç¦ç”¨å¤šæ™ºèƒ½ä½“åä½œåè®®")
    parser.add_argument("--no-self-rag", action="store_true",
                        help="ç¦ç”¨Self-RAGæ£€ç´¢")
    parser.add_argument("--no-deps", action="store_true",
                        help="è·³è¿‡ä¾èµ–åˆ†æ")
    parser.add_argument("--no-remediation", action="store_true",
                        help="è·³è¿‡ç”Ÿæˆä¿®å¤å»ºè®®")
    parser.add_argument("--no-cross-file", action="store_true",
                        help="ç¦ç”¨è·¨æ–‡ä»¶æ¼æ´æ£€æµ‹")
    parser.add_argument("--enhanced-analysis", action="store_true",
                        help="å¯ç”¨å¢å¼ºçš„è·¨æ–‡ä»¶å®‰å…¨åˆ†æï¼ˆå®éªŒæ€§åŠŸèƒ½ï¼‰")
    
    # å±‚çº§RAGç‰¹å®šå‚æ•°
    parser.add_argument("--haystack-orchestrator", type=str, choices=["traditional", "ai"], 
                        default=None,
                        help="é€‰æ‹©Haystackç¼–æ’å™¨ç±»å‹ï¼štraditionalï¼ˆä¼ ç»Ÿï¼‰æˆ– aiï¼ˆHaystack-AIï¼Œé»˜è®¤ï¼‰")
    parser.add_argument("--enable-txtai", action="store_true",
                        help="å¯ç”¨txtaiçŸ¥è¯†æ£€ç´¢å±‚ï¼ˆå±‚çº§RAGæ¨¡å¼ï¼‰")
    parser.add_argument("--enable-r2r", action="store_true",
                        help="å¯ç”¨R2Rä¸Šä¸‹æ–‡å¢å¼ºå±‚ï¼ˆå±‚çº§RAGæ¨¡å¼ï¼‰")
    parser.add_argument("--enable-self-rag-validation", action="store_true",
                        help="å¯ç”¨Self-RAGéªŒè¯å±‚ï¼ˆå±‚çº§RAGæ¨¡å¼ï¼‰")
    parser.add_argument("--disable-caching", action="store_true",
                        help="ç¦ç”¨å±‚çº§ç¼“å­˜ç³»ç»Ÿ")
    parser.add_argument("--disable-monitoring", action="store_true",
                        help="ç¦ç”¨æ€§èƒ½ç›‘æ§")
    
    # å…¶ä»–å‚æ•°
    parser.add_argument("--verbose", action="store_true",
                        help="å¯ç”¨è¯¦ç»†æ—¥å¿—è®°å½•")
    parser.add_argument("--dry-run", action="store_true",
                        help="è¯•è¿è¡Œæ¨¡å¼ï¼ˆä¸æ‰§è¡Œå®é™…åˆ†æï¼‰")
    parser.add_argument("--config-migrate", action="store_true",
                        help="è¿ç§»é…ç½®åˆ°å±‚çº§RAGæ ¼å¼")
    parser.add_argument("--show-architecture-info", action="store_true",
                        help="æ˜¾ç¤ºå½“å‰æ¶æ„ä¿¡æ¯å¹¶é€€å‡º")
    
    args = parser.parse_args()
    
    # å¤„ç†æ¶æ„é€‰æ‹©å‚æ•°
    if args.force_traditional:
        args.architecture = "traditional"
    elif args.force_hierarchical:
        args.architecture = "hierarchical"
    
    # å¤„ç†é…ç½®è¿ç§»
    if args.config_migrate:
        # æ ‡è®°éœ€è¦è¿ç§»ï¼Œåœ¨mainå‡½æ•°ä¸­å¤„ç†
        args._needs_migration = True
        return args
    
    # æ˜¾ç¤ºæ¶æ„ä¿¡æ¯
    if args.show_architecture_info:
        show_architecture_info()
        return args
    
    # ä»å‚æ•°æ›´æ–°é…ç½®
    Config.project.max_batch_size = args.workers
    Config.mcp.enabled = not args.no_mcp
    Config.self_rag.enabled = not args.no_self_rag
    Config.global_config.report_dir = args.output
    Config.global_config.report_format = args.format
    
    # è®¾ç½®æ¶æ„ç›¸å…³é…ç½®
    Config.architecture_mode = args.architecture
    Config.auto_switch_threshold = args.auto_switch_threshold
    Config.enable_performance_comparison = args.enable_performance_comparison
    
    # è®¾ç½®Haystackç¼–æ’å™¨ç±»å‹
    if args.haystack_orchestrator:
        # æ›´æ–°å±‚çº§RAGæ¨¡å‹é…ç½®ä¸­çš„ç¼–æ’å™¨ç±»å‹
        if hasattr(Config, 'hierarchical_rag_models') and Config.hierarchical_rag_models:
            Config.hierarchical_rag_models.haystack["orchestrator_type"] = args.haystack_orchestrator
    
    # å¦‚æœè¾“å‡ºç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºå®ƒ
    output_dir = Path(args.output)
    if not output_dir.exists():
        output_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"åˆ›å»ºäº†è¾“å‡ºç›®å½•ï¼š{args.output}")
    
    # è®°å½•å¯åŠ¨ä¿¡æ¯
    logger.info(f"ğŸš€ åœ¨ä»¥ä¸‹ç›®å½•å¼€å§‹AuditLumaåˆ†æï¼š{args.directory}")
    logger.info(f"ğŸ“ è¾“å‡ºå°†ä¿å­˜åˆ°ï¼š{args.output}")
    logger.info(f"ğŸ“„ æŠ¥å‘Šæ ¼å¼ï¼š{args.format}")
    logger.info(f"ğŸ—ï¸ RAGæ¶æ„æ¨¡å¼ï¼š{args.architecture}")
    logger.info(f"âš™ï¸ å·¥ä½œçº¿ç¨‹æ•°ï¼š{args.workers}")
    logger.info(f"ğŸ¤– MCPå·²å¯ç”¨ï¼š{Config.mcp.enabled}")
    logger.info(f"ğŸ” Self-RAGå·²å¯ç”¨ï¼š{Config.self_rag.enabled}")
    logger.info(f"ğŸ”— ä¾èµ–åˆ†æå·²å¯ç”¨ï¼š{not args.no_deps}")
    logger.info(f"ğŸ› ï¸ ä¿®å¤å»ºè®®å·²å¯ç”¨ï¼š{not args.no_remediation}")
    logger.info(f"ğŸ“Š è·¨æ–‡ä»¶åˆ†æå·²å¯ç”¨ï¼š{not args.no_cross_file}")
    
    if args.architecture == "hierarchical":
        # æ˜¾ç¤ºHaystackç¼–æ’å™¨ç±»å‹
        orchestrator_type = args.haystack_orchestrator or (
            Config.hierarchical_rag_models.haystack.get("orchestrator_type", "ai") 
            if hasattr(Config, 'hierarchical_rag_models') and Config.hierarchical_rag_models 
            else "ai"
        )
        orchestrator_name = "Haystack-AI" if orchestrator_type == "ai" else "ä¼ ç»ŸHaystack"
        logger.info(f"ğŸŒŸ ä½¿ç”¨å±‚çº§RAGæ¶æ„ï¼ˆ{orchestrator_name} + txtai + R2R + Self-RAGï¼‰")
    elif args.architecture == "traditional":
        logger.info("ğŸ”§ ä½¿ç”¨ä¼ ç»ŸRAGæ¶æ„")
    else:
        logger.info("ğŸ¯ è‡ªåŠ¨é€‰æ‹©æ¶æ„æ¨¡å¼ï¼ˆåŸºäºé¡¹ç›®è§„æ¨¡ï¼‰")
    
    if args.enhanced_analysis:
        logger.info("âœ¨ å¢å¼ºè·¨æ–‡ä»¶åˆ†ææ¨¡å¼å·²å¯ç”¨ï¼ˆå®éªŒæ€§åŠŸèƒ½ï¼‰")
    
    if args.enable_performance_comparison:
        logger.info("ğŸ“ˆ æ€§èƒ½å¯¹æ¯”æ¨¡å¼å·²å¯ç”¨")
    
    if args.dry_run:
        logger.info("ğŸ§ª è¯•è¿è¡Œæ¨¡å¼å·²å¯ç”¨")
    
    return args


async def handle_config_migration():
    """å¤„ç†é…ç½®è¿ç§»"""
    try:
        from auditluma.migration.config_migrator import migrate_config_async
        
        logger.info("ğŸ”„ å¼€å§‹é…ç½®è¿ç§»...")
        success, migration_result = await migrate_config_async()
        
        if success:
            logger.info("âœ… é…ç½®è¿ç§»æˆåŠŸ")
            logger.info(f"ğŸ“ å¤‡ä»½æ–‡ä»¶ï¼š{migration_result.get('backup_path', 'N/A')}")
            logger.info(f"ğŸ”§ åº”ç”¨äº† {len(migration_result.get('changes', []))} ä¸ªæ›´æ”¹")
            
            if migration_result.get('warnings'):
                logger.warning("âš ï¸ è¿ç§»è­¦å‘Šï¼š")
                for warning in migration_result['warnings']:
                    logger.warning(f"  - {warning}")
        else:
            logger.error("âŒ é…ç½®è¿ç§»å¤±è´¥")
            for error in migration_result.get('errors', []):
                logger.error(f"  - {error}")
                
    except ImportError:
        logger.error("é…ç½®è¿ç§»å·¥å…·ä¸å¯ç”¨")
    except Exception as e:
        logger.error(f"é…ç½®è¿ç§»è¿‡ç¨‹ä¸­å‡ºé”™: {e}")


def show_architecture_info():
    """æ˜¾ç¤ºæ¶æ„ä¿¡æ¯"""
    logger.info("ğŸ—ï¸ AuditLumaæ¶æ„ä¿¡æ¯")
    logger.info("=" * 50)
    
    # æ˜¾ç¤ºå¯ç”¨æ¶æ„
    logger.info("ğŸ“‹ å¯ç”¨æ¶æ„æ¨¡å¼ï¼š")
    logger.info("  â€¢ traditional - ä¼ ç»ŸRAGæ¶æ„ï¼ˆå•å±‚æ™ºèƒ½ä½“åä½œï¼‰")
    logger.info("  â€¢ hierarchical - å±‚çº§RAGæ¶æ„ï¼ˆå››å±‚ï¼šHaystack + txtai + R2R + Self-RAGï¼‰")
    logger.info("  â€¢ auto - è‡ªåŠ¨é€‰æ‹©ï¼ˆåŸºäºé¡¹ç›®è§„æ¨¡å’Œå¤æ‚åº¦ï¼‰")
    
    # æ˜¾ç¤ºå½“å‰é…ç½®
    current_mode = getattr(Config, 'architecture_mode', 'auto')
    logger.info(f"\nğŸ¯ å½“å‰é…ç½®çš„æ¶æ„æ¨¡å¼ï¼š{current_mode}")
    
    # æ˜¾ç¤ºHaystackç¼–æ’å™¨é€‰æ‹©
    logger.info("\nğŸš€ Haystackç¼–æ’å™¨é€‰æ‹©ï¼š")
    try:
        if hasattr(Config, 'hierarchical_rag_models') and Config.hierarchical_rag_models:
            orchestrator_type = Config.hierarchical_rag_models.get_orchestrator_type()
            orchestrator_name = "Haystack-AIç¼–æ’å™¨" if orchestrator_type == "ai" else "ä¼ ç»ŸHaystackç¼–æ’å™¨"
            logger.info(f"  â€¢ å½“å‰é€‰æ‹©ï¼š{orchestrator_name} ({orchestrator_type})")
            logger.info(f"  â€¢ å¯é€‰ç±»å‹ï¼štraditionalï¼ˆä¼ ç»Ÿï¼‰ã€aiï¼ˆHaystack-AIï¼Œæ¨èï¼‰")
            logger.info(f"  â€¢ åˆ‡æ¢æ–¹å¼ï¼š--haystack-orchestrator [traditional|ai]")
        else:
            logger.info("  â€¢ é»˜è®¤ï¼šHaystack-AIç¼–æ’å™¨ (ai)")
    except Exception as e:
        logger.warning(f"  âš ï¸ æ— æ³•è¯»å–ç¼–æ’å™¨é…ç½®: {e}")
    
    # æ˜¾ç¤ºå±‚çº§RAGç»„ä»¶çŠ¶æ€
    logger.info("\nğŸŒŸ å±‚çº§RAGç»„ä»¶çŠ¶æ€ï¼š")
    try:
        hierarchical_config = getattr(Config, 'hierarchical_rag', {})
        if hierarchical_config:
            logger.info(f"  â€¢ Haystackç¼–æ’å±‚ï¼š{'âœ… å¯ç”¨' if hierarchical_config.get('haystack', {}).get('enabled', True) else 'âŒ ç¦ç”¨'}")
            logger.info(f"  â€¢ txtaiçŸ¥è¯†æ£€ç´¢å±‚ï¼š{'âœ… å¯ç”¨' if hierarchical_config.get('txtai', {}).get('enabled', True) else 'âŒ ç¦ç”¨'}")
            logger.info(f"  â€¢ R2Rä¸Šä¸‹æ–‡å¢å¼ºå±‚ï¼š{'âœ… å¯ç”¨' if hierarchical_config.get('r2r', {}).get('enabled', True) else 'âŒ ç¦ç”¨'}")
            logger.info(f"  â€¢ Self-RAGéªŒè¯å±‚ï¼š{'âœ… å¯ç”¨' if hierarchical_config.get('self_rag_validation', {}).get('enabled', True) else 'âŒ ç¦ç”¨'}")
            logger.info(f"  â€¢ å±‚çº§ç¼“å­˜ç³»ç»Ÿï¼š{'âœ… å¯ç”¨' if hierarchical_config.get('cache', {}).get('enabled', True) else 'âŒ ç¦ç”¨'}")
            logger.info(f"  â€¢ æ€§èƒ½ç›‘æ§ï¼š{'âœ… å¯ç”¨' if hierarchical_config.get('monitoring', {}).get('enabled', True) else 'âŒ ç¦ç”¨'}")
        else:
            logger.info("  âš ï¸ å±‚çº§RAGé…ç½®æœªæ‰¾åˆ°ï¼Œè¯·è¿è¡Œ --config-migrate è¿›è¡Œé…ç½®è¿ç§»")
    except Exception as e:
        logger.warning(f"  âš ï¸ æ— æ³•è¯»å–å±‚çº§RAGé…ç½®: {e}")
    
    # æ˜¾ç¤ºå…¼å®¹æ€§ä¿¡æ¯
    logger.info("\nğŸ”„ å…¼å®¹æ€§ä¿¡æ¯ï¼š")
    logger.info("  â€¢ æ”¯æŒä»ä¼ ç»Ÿæ¶æ„æ— ç¼åˆ‡æ¢åˆ°å±‚çº§æ¶æ„")
    logger.info("  â€¢ æ”¯æŒé…ç½®çƒ­é‡è½½å’ŒåŠ¨æ€æ¶æ„åˆ‡æ¢")
    logger.info("  â€¢ æä¾›A/Bæµ‹è¯•æ¡†æ¶è¿›è¡Œæ€§èƒ½å¯¹æ¯”")
    logger.info("  â€¢ å®Œå…¨å‘åå…¼å®¹ç°æœ‰APIå’Œé…ç½®")
    
    logger.info("\nğŸ’¡ ä½¿ç”¨å»ºè®®ï¼š")
    logger.info("  â€¢ å°é¡¹ç›®ï¼ˆ<100æ–‡ä»¶ï¼‰ï¼šæ¨èä½¿ç”¨ traditional æ¶æ„")
    logger.info("  â€¢ å¤§é¡¹ç›®ï¼ˆâ‰¥100æ–‡ä»¶ï¼‰ï¼šæ¨èä½¿ç”¨ hierarchical æ¶æ„")
    logger.info("  â€¢ ä¸ç¡®å®šæ—¶ï¼šä½¿ç”¨ auto æ¨¡å¼è®©ç³»ç»Ÿè‡ªåŠ¨é€‰æ‹©")
    logger.info("  â€¢ ç¼–æ’å™¨é€‰æ‹©ï¼šæ¨èä½¿ç”¨ aiï¼ˆHaystack-AIç¼–æ’å™¨ï¼‰")
    logger.info("  â€¢ æ€§èƒ½å¯¹æ¯”ï¼šä½¿ç”¨ --enable-performance-comparison å‚æ•°")
    
    logger.info("\nğŸ”§ å‘½ä»¤ç¤ºä¾‹ï¼š")
    logger.info("  â€¢ ä½¿ç”¨Haystack-AIç¼–æ’å™¨ï¼š--architecture hierarchical --haystack-orchestrator ai")
    logger.info("  â€¢ ä½¿ç”¨ä¼ ç»Ÿç¼–æ’å™¨ï¼š--architecture hierarchical --haystack-orchestrator traditional")
    logger.info("  â€¢ æŸ¥çœ‹æ¶æ„ä¿¡æ¯ï¼š--show-architecture-info")
    
    logger.info("=" * 50)


async def run_analysis(target_dir: str, output_dir: str, workers: int, 
                     skip_deps: bool = False, skip_remediation: bool = False,
                     skip_cross_file: bool = False, enhanced_analysis: bool = False,
                     architecture_mode: str = "auto", 
                     enable_performance_comparison: bool = False,
                     dry_run: bool = False) -> Dict[str, Any]:
    """è¿è¡Œä»£ç åˆ†æè¿‡ç¨‹
    
    Args:
        target_dir: ç›®æ ‡é¡¹ç›®ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        workers: å·¥ä½œçº¿ç¨‹æ•°
        skip_deps: æ˜¯å¦è·³è¿‡ä¾èµ–åˆ†æ
        skip_remediation: æ˜¯å¦è·³è¿‡ç”Ÿæˆä¿®å¤å»ºè®®
        skip_cross_file: æ˜¯å¦è·³è¿‡è·¨æ–‡ä»¶æ¼æ´æ£€æµ‹
        enhanced_analysis: æ˜¯å¦å¯ç”¨å¢å¼ºçš„è·¨æ–‡ä»¶åˆ†æ
        architecture_mode: RAGæ¶æ„æ¨¡å¼
        enable_performance_comparison: æ˜¯å¦å¯ç”¨æ€§èƒ½å¯¹æ¯”
        dry_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œæ¨¡å¼
        
    Returns:
        åŒ…å«åˆ†æç»“æœçš„å­—å…¸
    """
    start_time = time.time()
    
    # ç¡®ä¿ç›®æ ‡ç›®å½•ä¸ºç»å¯¹è·¯å¾„
    target_dir_path = Path(target_dir)
    if not target_dir_path.is_absolute():
        target_dir_path = Path(__file__).parent / target_dir_path
    
    # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
    if not target_dir_path.exists():
        logger.warning(f"ç›®æ ‡ç›®å½•ä¸å­˜åœ¨: {target_dir_path}ï¼Œå°è¯•åˆ›å»º")
        try:
            target_dir_path.mkdir(parents=True, exist_ok=True)
            logger.info(f"æˆåŠŸåˆ›å»ºç›®æ ‡ç›®å½•: {target_dir_path}")
        except Exception as e:
            logger.error(f"åˆ›å»ºç›®æ ‡ç›®å½•æ—¶å‡ºé”™: {e}")
    
    # è®¡ç®—é¡¹ç›®å“ˆå¸Œå€¼ç”¨äºç¼“å­˜
    project_hash = calculate_project_hash(str(target_dir_path))
    logger.info(f"é¡¹ç›®å“ˆå¸Œå€¼ï¼š{project_hash}")
    
    # åˆå§‹åŒ–ä»£ç æ‰«æå™¨ä»¥æ”¶é›†æ‰€æœ‰æºæ–‡ä»¶
    scanner = CodeScanner(str(target_dir_path))
    # ä½¿ç”¨å¼‚æ­¥æ‰«æåŠ é€Ÿæ–‡ä»¶æ”¶é›†
    logger.info("ä½¿ç”¨å¼‚æ­¥æ–¹å¼æ‰«ææ–‡ä»¶...")
    source_files = await scanner.scan_async()
    
    # ç»Ÿè®¡æ–‡ä»¶å’Œä»£ç è¡Œæ•°
    total_files = len(source_files)
    total_lines = sum(len(sf.content.splitlines()) for sf in source_files)
    logger.info(f"æ‰¾åˆ°{total_files}ä¸ªè¦åˆ†æçš„æºæ–‡ä»¶ï¼Œå…±{total_lines}è¡Œä»£ç ")
    
    # å¦‚æœæ˜¯è¯•è¿è¡Œæ¨¡å¼ï¼Œç›´æ¥è¿”å›æ¨¡æ‹Ÿç»“æœ
    if dry_run:
        logger.info("ğŸ§ª è¯•è¿è¡Œæ¨¡å¼ï¼šè·³è¿‡å®é™…åˆ†æï¼Œè¿”å›æ¨¡æ‹Ÿç»“æœ")
        return {
            "vulnerabilities": [],
            "dependency_graph": None,
            "code_structure": {},
            "remediation_data": None,
            "scan_info": {
                "project_name": Config.project.name,
                "scan_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "scanned_files": total_files,
                "scanned_lines": total_lines,
                "scan_duration": "0.00ç§’ï¼ˆè¯•è¿è¡Œï¼‰",
                "project_hash": project_hash,
                "architecture_mode": architecture_mode,
                "dry_run": True
            }
        }
    
    # åˆå§‹åŒ–ç»Ÿä¸€ç¼–æ’å™¨ï¼ˆæ”¯æŒæ¶æ„åˆ‡æ¢ï¼‰
    try:
        orchestrator = create_unified_orchestrator(
            workers=workers,
            architecture_mode=architecture_mode,
            enable_performance_comparison=enable_performance_comparison,
            auto_switch_threshold=getattr(Config, 'auto_switch_threshold', 100),
            compatibility_mode=True
        )
        await orchestrator.initialize_orchestrators()
        logger.info(f"ğŸ¯ ç»Ÿä¸€ç¼–æ’å™¨åˆå§‹åŒ–å®Œæˆï¼Œå½“å‰æ¶æ„: {orchestrator.current_mode.value if orchestrator.current_mode else 'unknown'}")
        
    except Exception as e:
        logger.warning(f"ç»Ÿä¸€ç¼–æ’å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿç¼–æ’å™¨: {e}")
        # å›é€€åˆ°ä¼ ç»Ÿç¼–æ’å™¨
        orchestrator = AgentOrchestrator(workers=workers)
        await orchestrator.initialize_agents()
        logger.info("ğŸ”§ ä½¿ç”¨ä¼ ç»Ÿç¼–æ’å™¨")
    
    # è¿è¡Œå®‰å…¨åˆ†æ
    if skip_cross_file:
        logger.info("å¼€å§‹ä¼ ç»Ÿå®‰å…¨æ¼æ´åˆ†æï¼ˆè·³è¿‡è·¨æ–‡ä»¶æ£€æµ‹ï¼‰...")
    elif enhanced_analysis:
        logger.info("å¼€å§‹å¢å¼ºå®‰å…¨æ¼æ´åˆ†æï¼ˆåŒ…å«AIå¢å¼ºçš„è·¨æ–‡ä»¶æ£€æµ‹ï¼‰...")
    else:
        logger.info("å¼€å§‹å…¨é¢å®‰å…¨æ¼æ´åˆ†æï¼ˆåŒ…å«è·¨æ–‡ä»¶æ£€æµ‹ï¼‰...")
        
    vulnerabilities = await orchestrator.run_security_analysis(
        source_files, 
        skip_cross_file=skip_cross_file, 
        enhanced_analysis=enhanced_analysis
    )
    logger.info(f"å®‰å…¨åˆ†æå®Œæˆï¼šå‘ç°{len(vulnerabilities)}ä¸ªæ½œåœ¨æ¼æ´")
    
    # è¿è¡Œä»£ç ä¾èµ–åˆ†æï¼ˆå¦‚æœæœªè·³è¿‡ï¼‰
    dependency_graph = None
    code_structure = {}
    if not skip_deps:
        logger.info("å¼€å§‹ä»£ç ä¾èµ–åˆ†æ...")
        code_units = await orchestrator.extract_code_units(source_files)
        code_structure = await orchestrator.run_code_structure_analysis(code_units)
        dependency_graph = orchestrator.get_dependency_graph()
        logger.info(f"ä»£ç ç»“æ„åˆ†æå®Œæˆï¼šåˆ†æäº†{len(code_units)}ä¸ªä»£ç å•å…ƒ")
    
    # ç”Ÿæˆä¿®å¤å»ºè®®ï¼ˆå¦‚æœæœªè·³è¿‡ï¼‰
    remediation_data = None
    if not skip_remediation and vulnerabilities:
        logger.info("å¼€å§‹ç”Ÿæˆä¿®å¤å»ºè®®...")
        remediation_data = await orchestrator.generate_remediations(vulnerabilities)
        logger.info(f"ç”Ÿæˆäº†{remediation_data.get('remediation_count', 0)}ä¸ªä¿®å¤å»ºè®®")
    
    # æ”¶é›†æ‰«æä¿¡æ¯
    end_time = time.time()
    scan_duration = end_time - start_time
    
    # è·å–æ¶æ„ä¿¡æ¯
    architecture_info = {}
    if hasattr(orchestrator, 'get_orchestrator_info'):
        try:
            architecture_info = orchestrator.get_orchestrator_info()
        except Exception as e:
            logger.debug(f"è·å–æ¶æ„ä¿¡æ¯å¤±è´¥: {e}")
    
    # è·å–æ€§èƒ½æ‘˜è¦
    performance_summary = {}
    if hasattr(orchestrator, 'get_performance_summary'):
        try:
            performance_summary = orchestrator.get_performance_summary()
        except Exception as e:
            logger.debug(f"è·å–æ€§èƒ½æ‘˜è¦å¤±è´¥: {e}")
    
    scan_info = {
        "project_name": Config.project.name,
        "scan_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "scanned_files": total_files,
        "scanned_lines": total_lines,
        "scan_duration": f"{scan_duration:.2f}ç§’",
        "project_hash": project_hash,
        "architecture_mode": architecture_mode,
        "actual_architecture": orchestrator.current_mode.value if hasattr(orchestrator, 'current_mode') and orchestrator.current_mode else "traditional",
        "architecture_info": architecture_info,
        "performance_summary": performance_summary,
        "enable_performance_comparison": enable_performance_comparison
    }
    
    return {
        "vulnerabilities": vulnerabilities,
        "dependency_graph": dependency_graph,
        "code_structure": code_structure,
        "remediation_data": remediation_data,
        "scan_info": scan_info
    }


def generate_report(analysis_results: Dict[str, Any], report_format: str) -> str:
    """ç”Ÿæˆå®¡è®¡æŠ¥å‘Š
    
    Args:
        analysis_results: åˆ†æç»“æœ
        report_format: æŠ¥å‘Šæ ¼å¼
        
    Returns:
        ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
    """
    # åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨
    report_generator = ReportGenerator()
    
    # ç”ŸæˆæŠ¥å‘Š
    report_path = report_generator.generate_report(
        vulnerabilities=analysis_results.get("vulnerabilities", []),
        dependency_graph=analysis_results.get("dependency_graph"),
        remediation_data=analysis_results.get("remediation_data"),
        scan_info=analysis_results.get("scan_info", {})
    )
    
    return report_path


def generate_dependency_visualization(dependency_graph, output_dir: str) -> Optional[str]:
    """ç”Ÿæˆä»£ç ä¾èµ–å…³ç³»å¯è§†åŒ–
    
    Args:
        dependency_graph: ä¾èµ–å…³ç³»å›¾
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        ç”Ÿæˆçš„å¯è§†åŒ–æ–‡ä»¶è·¯å¾„
    """
    if not dependency_graph:
        return None
    
    # åˆå§‹åŒ–å›¾å½¢å¯è§†åŒ–å™¨
    graph_visualizer = GraphVisualizer()
    
    # åˆ›å»ºäº¤äº’å¼ä¾èµ–å…³ç³»å›¾
    interactive_graph_path = None
    try:
        interactive_graph_path = graph_visualizer.create_interactive_graph(
            dependency_graph=dependency_graph,
            output_file=str(Path(output_dir) / "dependency_graph_interactive.html")
        )
    except Exception as e:
        logger.error(f"åˆ›å»ºäº¤äº’å¼ä¾èµ–å›¾æ—¶å‡ºé”™: {e}")
    
    return interactive_graph_path


def save_analysis_data(analysis_results: Dict[str, Any]) -> str:
    """ä¿å­˜åˆ†ææ•°æ®åˆ°historyç›®å½•
    
    Args:
        analysis_results: åˆ†æç»“æœ
        
    Returns:
        ä¿å­˜çš„æ•°æ®æ–‡ä»¶è·¯å¾„
    """
    import json
    from pathlib import Path
    
    # ç”Ÿæˆæ–‡ä»¶å - åŒ…å«é¡¹ç›®åç§°
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # è·å–é¡¹ç›®åç§°å¹¶è¿›è¡Œæ¸…ç†
    project_name = analysis_results.get("scan_info", {}).get("project_name", "æœªçŸ¥é¡¹ç›®")
    # æ¸…ç†é¡¹ç›®åç§°ï¼Œç§»é™¤ä¸é€‚åˆæ–‡ä»¶åçš„å­—ç¬¦
    safe_project_name = "".join(c for c in project_name if c.isalnum() or c in "._-").rstrip()
    if not safe_project_name:
        safe_project_name = "æœªçŸ¥é¡¹ç›®"
    
    data_filename = f"Data_{safe_project_name}_{timestamp}.txt"
    data_path = Path("history") / data_filename
    
    # ç¡®ä¿historyç›®å½•å­˜åœ¨
    data_path.parent.mkdir(exist_ok=True)
    
    # å‡†å¤‡è¦ä¿å­˜çš„æ•°æ®
    scan_info = analysis_results.get("scan_info", {})
    save_data = {
        "analysis_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "scan_info": scan_info,
        "vulnerabilities_count": len(analysis_results.get("vulnerabilities", [])),
        "vulnerabilities": [],
        "dependency_info": {
            "has_dependency_graph": analysis_results.get("dependency_graph") is not None,
            "dependency_summary": "ä¾èµ–å…³ç³»å›¾å·²ç”Ÿæˆ" if analysis_results.get("dependency_graph") else "æœªç”Ÿæˆä¾èµ–å…³ç³»å›¾"
        },
        "remediation_info": {
            "has_remediation": analysis_results.get("remediation_data") is not None,
            "remediation_count": analysis_results.get("remediation_data", {}).get("remediation_count", 0) if analysis_results.get("remediation_data") else 0
        },
        "architecture_info": {
            "requested_mode": scan_info.get("architecture_mode", "unknown"),
            "actual_mode": scan_info.get("actual_architecture", "unknown"),
            "performance_comparison_enabled": scan_info.get("enable_performance_comparison", False),
            "architecture_details": scan_info.get("architecture_info", {}),
            "performance_summary": scan_info.get("performance_summary", {})
        }
    }
    
    # å¤„ç†æ¼æ´æ•°æ®ï¼ˆåºåˆ—åŒ–VulnerabilityResultå¯¹è±¡ï¼‰
    for vuln in analysis_results.get("vulnerabilities", []):
        try:
            # æ ‡å‡†åŒ–æ•°æ®ç±»å‹ç¡®ä¿ä¸€è‡´æ€§
            vuln_dict = {
                "id": str(vuln.id) if vuln.id else f"vuln_{len(save_data['vulnerabilities'])}",
                "title": getattr(vuln, 'title', vuln.vulnerability_type or "Unknown Vulnerability"),
                "vulnerability_type": str(vuln.vulnerability_type) if vuln.vulnerability_type else "Unknown",
                "severity": str(vuln.severity.value) if hasattr(vuln.severity, 'value') else str(vuln.severity),
                "description": str(vuln.description) if vuln.description else "",
                "file_path": str(vuln.file_path) if vuln.file_path else "unknown",
                "start_line": int(vuln.start_line) if vuln.start_line else 1,
                "end_line": int(vuln.end_line) if vuln.end_line else 1,
                "snippet": str(vuln.snippet) if vuln.snippet else "",
                "metadata": dict(getattr(vuln, 'metadata', {})),
                "cwe_id": getattr(vuln, 'cwe_id', None),
                "owasp_category": getattr(vuln, 'owasp_category', None),
                "confidence": float(getattr(vuln, 'confidence', 1.0)),
                "recommendation": str(getattr(vuln, 'recommendation', "")),
                "references": list(getattr(vuln, 'references', [])),
                "cvss4_score": float(getattr(vuln, 'cvss4_score')) if getattr(vuln, 'cvss4_score') is not None else None,
                "cvss4_vector": str(getattr(vuln, 'cvss4_vector', "")),
                "cvss4_severity": str(getattr(vuln, 'cvss4_severity', ""))
            }
            save_data["vulnerabilities"].append(vuln_dict)
        except Exception as e:
            logger.error(f"åºåˆ—åŒ–æ¼æ´æ•°æ®æ—¶å‡ºé”™: {e}")
            logger.debug(f"é—®é¢˜æ¼æ´å¯¹è±¡: {vuln}")
            # æ·»åŠ æœ€å°åŒ–çš„æ¼æ´ä¿¡æ¯ä»¥ä¿æŒæ•°æ®å®Œæ•´æ€§
            fallback_vuln = {
                "id": f"error_vuln_{len(save_data['vulnerabilities'])}",
                "title": "æ•°æ®åºåˆ—åŒ–é”™è¯¯",
                "vulnerability_type": "Serialization Error",
                "severity": "medium",
                "description": f"æ¼æ´æ•°æ®åºåˆ—åŒ–å¤±è´¥: {str(e)}",
                "file_path": "unknown",
                "start_line": 1,
                "end_line": 1,
                "snippet": "",
                "metadata": {},
                "cvss4_score": None,
                "cvss4_vector": "",
                "cvss4_severity": ""
            }
            save_data["vulnerabilities"].append(fallback_vuln)
    
    # ä¿å­˜å®Œæ•´çš„åˆ†æç»“æœï¼ˆç”¨äºåç»­æŠ¥å‘Šç”Ÿæˆï¼‰
    save_data["full_analysis_results"] = {
        "vulnerabilities_serialized": save_data["vulnerabilities"],  # å·²åºåˆ—åŒ–çš„æ¼æ´æ•°æ®
        "scan_info": analysis_results.get("scan_info", {}),
        "remediation_data": analysis_results.get("remediation_data"),  # ä¿å­˜ä¿®å¤å»ºè®®æ•°æ®
        "dependency_available": analysis_results.get("dependency_graph") is not None,
        "code_structure": analysis_results.get("code_structure", {})
    }
    
    # å†™å…¥æ–‡ä»¶
    with open(data_path, 'w', encoding='utf-8') as f:
        json.dump(save_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"åˆ†ææ•°æ®å·²ä¿å­˜åˆ°ï¼š{data_path}")
    return str(data_path)


async def main() -> None:
    """ä¸»å…¥å£ç‚¹"""
    args = init()
    
    # å¤„ç†é…ç½®è¿ç§»
    if getattr(args, '_needs_migration', False):
        await handle_config_migration()
        return
    
    # å¤„ç†ç‰¹æ®Šå‘½ä»¤
    if args.show_architecture_info:
        return
    
    try:
        # è¿è¡Œåˆ†æ
        analysis_results = await run_analysis(
            target_dir=args.directory,
            output_dir=args.output,
            workers=args.workers,
            skip_deps=args.no_deps,
            skip_remediation=args.no_remediation,
            skip_cross_file=args.no_cross_file,
            enhanced_analysis=args.enhanced_analysis,
            architecture_mode=args.architecture,
            enable_performance_comparison=args.enable_performance_comparison,
            dry_run=args.dry_run
        )
        
        # ä¿å­˜åˆ†ææ•°æ®åˆ°historyç›®å½•
        data_path = save_analysis_data(analysis_results)
        
        # æ‰“å°æ‘˜è¦
        vulnerabilities = analysis_results.get("vulnerabilities", [])
        scan_info = analysis_results.get("scan_info", {})
        
        if args.dry_run:
            logger.info("ğŸ§ª è¯•è¿è¡Œå®Œæˆ")
            logger.info(f"ğŸ“ æ‰«ææ–‡ä»¶æ•°: {scan_info.get('scanned_files', 0)}")
            logger.info(f"ğŸ“ ä»£ç è¡Œæ•°: {scan_info.get('scanned_lines', 0)}")
            logger.info(f"ğŸ—ï¸ æ¶æ„æ¨¡å¼: {scan_info.get('architecture_mode', 'unknown')}")
            return
        
        # ç»Ÿè®¡æ¼æ´ä¸¥é‡ç¨‹åº¦
        severity_counts = {}
        for vuln in vulnerabilities:
            severity = getattr(vuln, 'severity', 'unknown')
            if hasattr(severity, 'lower'):
                severity = severity.lower()
            severity_counts[severity] = severity_counts.get(severity, 0) + 1
        
        logger.info("ğŸ“Š åˆ†ææ‘˜è¦ï¼š")
        logger.info(f"  ğŸ“ æ‰«ææ–‡ä»¶: {scan_info.get('scanned_files', 0)}")
        logger.info(f"  ğŸ“ ä»£ç è¡Œæ•°: {scan_info.get('scanned_lines', 0)}")
        logger.info(f"  â±ï¸ åˆ†æè€—æ—¶: {scan_info.get('scan_duration', 'N/A')}")
        logger.info(f"  ğŸ—ï¸ ä½¿ç”¨æ¶æ„: {scan_info.get('actual_architecture', 'unknown')}")
        logger.info(f"  ğŸ” å‘ç°æ¼æ´: {len(vulnerabilities)}")
        
        if severity_counts:
            logger.info("  ğŸ“ˆ ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ:")
            for severity, count in severity_counts.items():
                logger.info(f"    {severity.upper()}: {count}")
        
        # æ˜¾ç¤ºæ€§èƒ½å¯¹æ¯”ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if args.enable_performance_comparison and scan_info.get('performance_summary'):
            perf_summary = scan_info['performance_summary']
            logger.info("ğŸ“ˆ æ€§èƒ½å¯¹æ¯”:")
            for arch, stats in perf_summary.get('performance_stats', {}).items():
                if stats.get('calls', 0) > 0:
                    logger.info(f"  {arch}: å¹³å‡è€—æ—¶ {stats.get('avg_time', 0):.2f}ç§’")
        
        logger.info(f"âœ… åˆ†æå®Œæˆï¼æ•°æ®å·²ä¿å­˜åˆ°ï¼š{data_path}")
        logger.info("ğŸŒ è¯·ä½¿ç”¨Webç•Œé¢ç”Ÿæˆä¸åŒæ ¼å¼çš„æŠ¥å‘Š")
        logger.info("ğŸš€ è¿è¡Œå‘½ä»¤ï¼špython -m auditluma.web.report_server")
    
    except Exception as e:
        logger.error(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        logger.error(traceback.format_exc())
        logger.error("AuditLumaåˆ†æå¤±è´¥")


if __name__ == "__main__":
    asyncio.run(main())
