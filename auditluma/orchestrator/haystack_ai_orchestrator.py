"""
Haystack-AIä¸»ç¼–æ’æ¡†æ¶ - å±‚çº§RAGæ¶æ„ç¬¬ä¸€å±‚
åŸºäºå®˜æ–¹Haystack-AIåº“å®ç°çš„æ™ºèƒ½ç¼–æ’ç³»ç»Ÿ
è´Ÿè´£ä»»åŠ¡åˆ†å‘ã€æµç¨‹ç¼–æ’å’Œç»“æœæ±‡æ€»
"""

import asyncio
import uuid
from typing import List, Dict, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
import time
from pathlib import Path
import json

from loguru import logger
import requests
import socket

# Haystack AI imports
try:
    from haystack import Pipeline, Document
    from haystack.components.builders import PromptBuilder
    from haystack.components.generators import OpenAIGenerator
    from haystack.components.retrievers import InMemoryBM25Retriever
    from haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder
    from haystack.document_stores.in_memory import InMemoryDocumentStore
    from haystack.components.routers import ConditionalRouter
    from haystack.components.joiners import DocumentJoiner
    from haystack.components.preprocessors import DocumentSplitter
    from haystack.components.rankers import TransformersSimilarityRanker
    HAYSTACK_AVAILABLE = True
except ImportError as e:
    logger.warning(f"Haystack-AI not available: {e}")
    HAYSTACK_AVAILABLE = False
    # åˆ›å»ºå ä½ç¬¦ç±»
    class Pipeline: pass
    class Document: pass
    class PromptBuilder: pass
    class OpenAIGenerator: pass

from auditluma.config import Config
from auditluma.models.code import SourceFile, CodeUnit, VulnerabilityResult
from auditluma.rag.txtai_retriever import TxtaiRetriever
from auditluma.rag.r2r_enhancer import R2REnhancer
from auditluma.rag.self_rag_validator import SelfRAGValidator
from auditluma.orchestrator.task_decomposer import TaskDecomposer, TaskCollection, TaskType, AuditTask
from auditluma.orchestrator.parallel_executor import ParallelProcessingManager, TaskScheduler
from auditluma.orchestrator.result_integrator import ResultIntegrator, ConflictResolutionStrategy

# Import UnifiedGenerator and HaystackPipelineBuilder for enhanced support
try:
    from auditluma.components.unified_generator import UnifiedGenerator
    from auditluma.components.pipeline_builder import HaystackPipelineBuilder
    UNIFIED_GENERATOR_AVAILABLE = True
except (ImportError, AttributeError) as e:
    logger.warning(f"UnifiedGenerator not available: {e}")
    UNIFIED_GENERATOR_AVAILABLE = False
    class UnifiedGenerator: 
        def __init__(self, *args, **kwargs):
            pass
    class HaystackPipelineBuilder:
        def __init__(self, *args, **kwargs):
            pass


@dataclass
class HaystackPipelineConfig:
    """Haystackç®¡é“é…ç½®"""
    model_name: str = "gpt-3.5-turbo"
    max_tokens: int = 2000
    temperature: float = 0.1
    top_k: int = 5
    similarity_threshold: float = 0.7
    enable_embeddings: bool = True
    enable_ranking: bool = True
    
    # Ollamaç‰¹å®šé…ç½®
    ollama_base_url: str = "http://localhost:11434/api"  # é»˜è®¤å€¼ï¼Œä¼šä»é…ç½®æ–‡ä»¶è¦†ç›–
    ollama_timeout: float = 60.0
    ollama_max_retries: int = 3
    ollama_retry_delay: float = 2.0
    
    @classmethod
    def from_config(cls, config_dict: Dict[str, Any] = None) -> 'HaystackPipelineConfig':
        """ä»é…ç½®å­—å…¸åˆ›å»ºé…ç½®å¯¹è±¡"""
        if config_dict is None:
            # ç›´æ¥ä»hierarchical_rag_modelsé…ç½®è·å–Haystacké…ç½®
            try:
                haystack_config = Config.hierarchical_rag_models.haystack
                default_model = haystack_config.get("default_model", "gpt-3.5-turbo")
                
                # ä»é…ç½®æ–‡ä»¶è¯»å–Ollamaè®¾ç½®
                ollama_base_url = Config.ollama.base_url if hasattr(Config, 'ollama') else 'http://localhost:11434/api'
                
                logger.info(f"ä»é…ç½®åŠ è½½Haystackç®¡é“æ¨¡å‹: {default_model}")
                logger.info(f"ä»é…ç½®åŠ è½½OllamaæœåŠ¡åœ°å€: {ollama_base_url}")
                
                return cls(
                    model_name=default_model,
                    max_tokens=2000,
                    temperature=0.1,
                    top_k=5,
                    similarity_threshold=0.7,
                    enable_embeddings=True,
                    enable_ranking=True,
                    ollama_base_url=ollama_base_url,
                    ollama_timeout=60.0,
                    ollama_max_retries=3,
                    ollama_retry_delay=2.0
                )
            except Exception as e:
                logger.warning(f"åŠ è½½Haystacké…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return cls()  # ä½¿ç”¨é»˜è®¤å€¼
        
        return cls(
            model_name=config_dict.get("model_name", "gpt-3.5-turbo"),
            max_tokens=config_dict.get("max_tokens", 2000),
            temperature=config_dict.get("temperature", 0.1),
            top_k=config_dict.get("top_k", 5),
            similarity_threshold=config_dict.get("similarity_threshold", 0.7),
            enable_embeddings=config_dict.get("enable_embeddings", True),
            enable_ranking=config_dict.get("enable_ranking", True)
        )


@dataclass
class TaskResult:
    """ä»»åŠ¡æ‰§è¡Œç»“æœ"""
    task_id: str
    task_type: TaskType
    vulnerabilities: List[VulnerabilityResult]
    execution_time: float
    confidence: float
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class AuditResult:
    """ç»¼åˆå®¡è®¡ç»“æœ"""
    vulnerabilities: List[VulnerabilityResult]
    task_results: List[TaskResult]
    execution_summary: Dict[str, Any]
    confidence_score: float
    processing_time: float


class HaystackPipelineBuilder:
    """Haystackç®¡é“æ„å»ºå™¨"""
    
    def __init__(self, config: HaystackPipelineConfig):
        """åˆå§‹åŒ–ç®¡é“æ„å»ºå™¨"""
        self.config = config
        self.document_store = InMemoryDocumentStore()
        
        if not HAYSTACK_AVAILABLE:
            logger.error("Haystack-AI not available, pipeline functionality will be limited")
    
    def _is_openai_compatible_model(self, model_name: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å…¼å®¹OpenAI API"""
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºOllamaæ¨¡å‹ï¼Œå¦‚æœæ˜¯åˆ™ä¸å…¼å®¹OpenAI API
        if self._is_ollama_model(model_name):
            return False
        
        # æå–æä¾›å•†ä¿¡æ¯
        if "@" in model_name:
            _, provider = model_name.split("@", 1)
        else:
            # æ ¹æ®æ¨¡å‹åç§°æ¨æ–­æä¾›å•†
            if "gpt-" in model_name.lower():
                provider = "openai"
            elif "deepseek" in model_name.lower():
                provider = "deepseek"
            elif "qwen" in model_name.lower():
                provider = "qwen"
            elif "moonshot" in model_name.lower():
                provider = "moonshot"
            elif "zhipu" in model_name.lower() or "glm" in model_name.lower():
                provider = "zhipu"
            elif "baichuan" in model_name.lower():
                provider = "baichuan"
            else:
                provider = "unknown"
        
        # Haystackçš„OpenAIGeneratoræ”¯æŒOpenAIå…¼å®¹çš„API
        openai_compatible_providers = ["openai", "deepseek", "moonshot", "qwen", "zhipu", "baichuan"]
        
        return provider in openai_compatible_providers
    
    def _get_ollama_config(self, model_name: str, base_config: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–Ollamaç‰¹å®šé…ç½®"""
        # ä»é…ç½®æ–‡ä»¶é‡æ–°è¯»å–æœ€æ–°çš„Ollamaè®¾ç½®
        try:
            if hasattr(Config, 'ollama') and hasattr(Config.ollama, 'base_url'):
                base_url = Config.ollama.base_url
            else:
                base_url = self.config.ollama_base_url
        except Exception as e:
            logger.debug(f"ä»é…ç½®æ–‡ä»¶è¯»å–Ollamaè®¾ç½®å¤±è´¥: {e}")
            base_url = self.config.ollama_base_url
        
        # ç¡®ä¿base_urlæ˜¯å®Œæ•´çš„APIåœ°å€
        if not base_url.endswith('/api') and not base_url.endswith('/api/'):
            if base_url.endswith('/'):
                base_url = base_url + 'api'
            else:
                base_url = base_url + '/api'
        
        ollama_config = {
            "model": model_name.replace("@ollama", ""),  # ç§»é™¤@ollamaåç¼€
            "base_url": base_url,
            "timeout": self.config.ollama_timeout,
            "max_retries": self.config.ollama_max_retries,
            "retry_delay": self.config.ollama_retry_delay,
            "generation_kwargs": {
                "max_tokens": base_config.get("max_tokens", self.config.max_tokens),
                "temperature": base_config.get("temperature", self.config.temperature)
            }
        }
        
        # ä»ç¯å¢ƒå˜é‡è·å–è‡ªå®šä¹‰Ollamaè®¾ç½®ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        try:
            import os
            if "OLLAMA_BASE_URL" in os.environ:
                env_base_url = os.environ["OLLAMA_BASE_URL"]
                # ç¡®ä¿ç¯å¢ƒå˜é‡çš„URLä¹Ÿæ˜¯å®Œæ•´çš„APIåœ°å€
                if not env_base_url.endswith('/api') and not env_base_url.endswith('/api/'):
                    if env_base_url.endswith('/'):
                        env_base_url = env_base_url + 'api'
                    else:
                        env_base_url = env_base_url + '/api'
                ollama_config["base_url"] = env_base_url
                logger.debug(f"ä½¿ç”¨ç¯å¢ƒå˜é‡OLLAMA_BASE_URL: {env_base_url}")
            if "OLLAMA_TIMEOUT" in os.environ:
                ollama_config["timeout"] = float(os.environ["OLLAMA_TIMEOUT"])
        except Exception as e:
            logger.debug(f"è·å–Ollamaç¯å¢ƒå˜é‡é…ç½®æ—¶å‡ºé”™: {e}")
        
        return ollama_config
    
    def _get_openai_config(self, model_name: str) -> Dict[str, Any]:
        """è·å–OpenAIå…¼å®¹æ¨¡å‹çš„é…ç½®"""
        # ä»é…ç½®æ–‡ä»¶è¯»å–OpenAIè®¾ç½®
        try:
            openai_config = getattr(Config, 'openai', {})
            api_key = openai_config.api_key if hasattr(openai_config, 'api_key') else ""
            base_url = openai_config.base_url if hasattr(openai_config, 'base_url') else ""
            
            config = {
                "api_key": api_key,
                "base_url": base_url
            }
            
            # ä»ç¯å¢ƒå˜é‡è·å–è¦†ç›–è®¾ç½®ï¼Œæˆ–å°†é…ç½®è®¾ç½®ä¸ºç¯å¢ƒå˜é‡
            import os
            if "OPENAI_API_KEY" in os.environ:
                config["api_key"] = os.environ["OPENAI_API_KEY"]
            elif config["api_key"]:
                # å¦‚æœé…ç½®æ–‡ä»¶ä¸­æœ‰API keyä½†ç¯å¢ƒå˜é‡ä¸­æ²¡æœ‰ï¼Œè®¾ç½®ç¯å¢ƒå˜é‡
                os.environ["OPENAI_API_KEY"] = config["api_key"]
                
            if "OPENAI_BASE_URL" in os.environ:
                config["base_url"] = os.environ["OPENAI_BASE_URL"]
            elif config["base_url"]:
                # å¦‚æœé…ç½®æ–‡ä»¶ä¸­æœ‰base_urlä½†ç¯å¢ƒå˜é‡ä¸­æ²¡æœ‰ï¼Œè®¾ç½®ç¯å¢ƒå˜é‡
                os.environ["OPENAI_BASE_URL"] = config["base_url"]
            
            logger.debug(f"OpenAIé…ç½®: api_key={'***' if config['api_key'] else 'None'}, base_url={config['base_url']}")
            return config
            
        except Exception as e:
            logger.debug(f"è·å–OpenAIé…ç½®å¤±è´¥: {e}")
            return {"api_key": "", "base_url": ""}
    
    def _handle_ollama_error(self, error: Exception, model_name: str, operation: str) -> bool:
        """
        å¤„ç†Ollamaç‰¹å®šé”™è¯¯
        
        Args:
            error: å‘ç”Ÿçš„å¼‚å¸¸
            model_name: æ¨¡å‹åç§°
            operation: æ“ä½œç±»å‹
            
        Returns:
            bool: æ˜¯å¦åº”è¯¥å›é€€åˆ°ä¼ ç»Ÿæ‰§è¡Œæ–¹å¼
        """
        error_type = type(error).__name__
        error_msg = str(error)
        
        # è¿æ¥é”™è¯¯å¤„ç†
        if isinstance(error, (requests.ConnectionError, socket.error)):
            logger.error(f"ğŸ”Œ OllamaæœåŠ¡è¿æ¥å¤±è´¥ - æ¨¡å‹: {model_name}, æ“ä½œ: {operation}")
            logger.error(f"   é”™è¯¯è¯¦æƒ…: {error_msg}")
            logger.info(f"   è¯·æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦åœ¨ {self.config.ollama_base_url} è¿è¡Œ")
            logger.info(f"   å»ºè®®: å¯åŠ¨OllamaæœåŠ¡æˆ–æ£€æŸ¥ç½‘ç»œè¿æ¥")
            return True  # å›é€€åˆ°ä¼ ç»Ÿæ‰§è¡Œ
        
        # è¶…æ—¶é”™è¯¯å¤„ç†
        elif isinstance(error, requests.Timeout):
            logger.error(f"â° Ollamaè¯·æ±‚è¶…æ—¶ - æ¨¡å‹: {model_name}, æ“ä½œ: {operation}")
            logger.error(f"   è¶…æ—¶æ—¶é—´: {self.config.ollama_timeout}ç§’")
            logger.info(f"   å»ºè®®: å¢åŠ è¶…æ—¶æ—¶é—´æˆ–æ£€æŸ¥æ¨¡å‹æ˜¯å¦éœ€è¦ä¸‹è½½")
            return True  # å›é€€åˆ°ä¼ ç»Ÿæ‰§è¡Œ
        
        # HTTPé”™è¯¯å¤„ç†
        elif isinstance(error, requests.HTTPError):
            status_code = getattr(error.response, 'status_code', 'unknown')
            if status_code == 404:
                logger.error(f"ğŸš« Ollamaæ¨¡å‹æœªæ‰¾åˆ° - æ¨¡å‹: {model_name}")
                logger.info(f"   å»ºè®®: è¿è¡Œ 'ollama pull {model_name}' ä¸‹è½½æ¨¡å‹")
            elif status_code == 500:
                logger.error(f"ğŸ’¥ OllamaæœåŠ¡å†…éƒ¨é”™è¯¯ - æ¨¡å‹: {model_name}")
                logger.info(f"   å»ºè®®: æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€å’Œæ—¥å¿—")
            else:
                logger.error(f"ğŸŒ Ollama HTTPé”™è¯¯ - çŠ¶æ€ç : {status_code}, æ¨¡å‹: {model_name}")
            return True  # å›é€€åˆ°ä¼ ç»Ÿæ‰§è¡Œ
        
        # Haystacké›†æˆé”™è¯¯å¤„ç†
        elif "haystack" in error_msg.lower() or "__haystack_" in error_msg:
            logger.warning(f"ğŸ”§ Haystacké›†æˆé—®é¢˜ - æ¨¡å‹: {model_name}, æ“ä½œ: {operation}")
            logger.warning(f"   é”™è¯¯è¯¦æƒ…: {error_msg}")
            logger.info(f"   ç³»ç»Ÿå°†å›é€€åˆ°ä¼ ç»Ÿæ‰§è¡Œæ–¹å¼")
            return True  # å›é€€åˆ°ä¼ ç»Ÿæ‰§è¡Œ
        
        # å…¶ä»–UnifiedGeneratoré”™è¯¯
        elif "UnifiedGenerator" in error_msg:
            logger.error(f"âš™ï¸ UnifiedGeneratoré”™è¯¯ - æ¨¡å‹: {model_name}, æ“ä½œ: {operation}")
            logger.error(f"   é”™è¯¯è¯¦æƒ…: {error_msg}")
            logger.info(f"   ç³»ç»Ÿå°†å›é€€åˆ°ä¼ ç»Ÿæ‰§è¡Œæ–¹å¼")
            return True  # å›é€€åˆ°ä¼ ç»Ÿæ‰§è¡Œ
        
        # æœªçŸ¥é”™è¯¯
        else:
            logger.error(f"â“ æœªçŸ¥Ollamaé”™è¯¯ - ç±»å‹: {error_type}, æ¨¡å‹: {model_name}")
            logger.error(f"   é”™è¯¯è¯¦æƒ…: {error_msg}")
            logger.info(f"   ç³»ç»Ÿå°†å›é€€åˆ°ä¼ ç»Ÿæ‰§è¡Œæ–¹å¼")
            return True  # å›é€€åˆ°ä¼ ç»Ÿæ‰§è¡Œ
    
    def _check_ollama_service_health(self, base_url: str) -> bool:
        """
        æ£€æŸ¥OllamaæœåŠ¡å¥åº·çŠ¶æ€
        
        Args:
            base_url: OllamaæœåŠ¡åœ°å€
            
        Returns:
            bool: æœåŠ¡æ˜¯å¦å¥åº·
        """
        try:
            # å°è¯•è®¿é—®Ollama APIå¥åº·æ£€æŸ¥ç«¯ç‚¹
            health_url = f"{base_url.rstrip('/')}/api/tags"
            response = requests.get(health_url, timeout=5)
            
            if response.status_code == 200:
                logger.debug(f"âœ… OllamaæœåŠ¡å¥åº·æ£€æŸ¥é€šè¿‡: {base_url}")
                return True
            else:
                logger.warning(f"âš ï¸ OllamaæœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")
                return False
                
        except Exception as e:
            logger.warning(f"âŒ OllamaæœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def _log_provider_execution_path(self, model_name: str, provider_type: str, pipeline_type: str):
        """
        è®°å½•æä¾›å•†æ‰§è¡Œè·¯å¾„çš„è¯¦ç»†æ—¥å¿—
        
        Args:
            model_name: æ¨¡å‹åç§°
            provider_type: æä¾›å•†ç±»å‹ (ollama/openai)
            pipeline_type: ç®¡é“ç±»å‹
        """
        if provider_type == "ollama":
            logger.info(f"ğŸ¦™ Ollamaæ‰§è¡Œè·¯å¾„ - ç®¡é“: {pipeline_type}")
            logger.info(f"   æ¨¡å‹: {model_name}")
            logger.info(f"   æœåŠ¡åœ°å€: {self.config.ollama_base_url}")
            logger.info(f"   è¶…æ—¶è®¾ç½®: {self.config.ollama_timeout}ç§’")
            logger.info(f"   é‡è¯•è®¾ç½®: {self.config.ollama_max_retries}æ¬¡")
        elif provider_type == "openai":
            logger.info(f"ğŸ¤– OpenAIå…¼å®¹æ‰§è¡Œè·¯å¾„ - ç®¡é“: {pipeline_type}")
            logger.info(f"   æ¨¡å‹: {model_name}")
            logger.info(f"   ä½¿ç”¨Haystack OpenAIGenerator")
        else:
            logger.info(f"â“ æœªçŸ¥æä¾›å•†æ‰§è¡Œè·¯å¾„ - ç®¡é“: {pipeline_type}")
            logger.info(f"   æ¨¡å‹: {model_name}")
    
    def _implement_graceful_fallback(self, model_name: str, task_type: str, error: Exception) -> bool:
        """
        å®ç°ä¼˜é›…çš„å›é€€æœºåˆ¶
        
        Args:
            model_name: æ¨¡å‹åç§°
            task_type: ä»»åŠ¡ç±»å‹
            error: å‘ç”Ÿçš„é”™è¯¯
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸè®¾ç½®å›é€€
        """
        logger.warning(f"ğŸ”„ å¯åŠ¨ä¼˜é›…å›é€€æœºåˆ¶")
        logger.warning(f"   åŸå§‹æ¨¡å‹: {model_name}")
        logger.warning(f"   ä»»åŠ¡ç±»å‹: {task_type}")
        logger.warning(f"   é”™è¯¯åŸå› : {str(error)}")
        
        # è®°å½•å›é€€å†³ç­–
        if self._is_ollama_model(model_name):
            logger.info(f"   å›é€€ç­–ç•¥: Ollamaæ¨¡å‹ -> ä¼ ç»Ÿæ‰§è¡Œæ–¹å¼")
            logger.info(f"   å½±å“: å°†ä½¿ç”¨éHaystackç®¡é“æ‰§è¡Œä»»åŠ¡")
        else:
            logger.info(f"   å›é€€ç­–ç•¥: OpenAIå…¼å®¹æ¨¡å‹ -> ä¼ ç»Ÿæ‰§è¡Œæ–¹å¼")
            logger.info(f"   å½±å“: å°†è·³è¿‡Haystackç®¡é“ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
        
        # æä¾›ç”¨æˆ·å»ºè®®
        if "connection" in str(error).lower():
            logger.info(f"ğŸ’¡ å»ºè®®: æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒæœåŠ¡çŠ¶æ€")
        elif "timeout" in str(error).lower():
            logger.info(f"ğŸ’¡ å»ºè®®: å¢åŠ è¶…æ—¶æ—¶é—´æˆ–æ£€æŸ¥æœåŠ¡æ€§èƒ½")
        elif "not found" in str(error).lower():
            logger.info(f"ğŸ’¡ å»ºè®®: ç¡®è®¤æ¨¡å‹å·²æ­£ç¡®å®‰è£…å’Œé…ç½®")
        
        return True
    
    def _is_ollama_model(self, model_name: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦ä¸ºOllamaæ¨¡å‹"""
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„@ollamaåç¼€
        if "@ollama" in model_name:
            return True
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¸¸è§çš„Ollamaæ¨¡å‹åç§°æ¨¡å¼
        # Ollamaæ¨¡å‹é€šå¸¸ä½¿ç”¨ model:tag æ ¼å¼ï¼Œå¦‚ llama2:7b, qwen:32b ç­‰
        if ":" in model_name and "@" not in model_name:
            # å¸¸è§çš„Ollamaæ¨¡å‹å‰ç¼€
            ollama_model_prefixes = [
                "llama", "llama2", "llama3", "codellama", "vicuna", "alpaca",
                "mistral", "mixtral", "qwen", "qwen2", "deepseek", "yi",
                "gemma", "phi", "tinyllama", "orca", "wizard", "solar"
            ]
            
            model_base = model_name.split(":")[0].lower()
            return any(model_base.startswith(prefix) for prefix in ollama_model_prefixes)
        
        return False
    
    def _create_unified_generator(self, model_name: str, config: Dict[str, Any]) -> Any:
        """åˆ›å»ºUnifiedGeneratorå®ä¾‹ï¼ŒåŒ…å«å¢å¼ºçš„é”™è¯¯å¤„ç†"""
        if not UNIFIED_GENERATOR_AVAILABLE:
            logger.error("âŒ UnifiedGeneratorä¸å¯ç”¨ï¼Œæ— æ³•åˆ›å»ºOllamaç”Ÿæˆå™¨")
            return None
        
        try:
            # ç¡®å®šæä¾›å•†å’Œé…ç½®
            if self._is_ollama_model(model_name):
                provider = "ollama"
                # è·å–Ollamaç‰¹å®šé…ç½®
                ollama_config = self._get_ollama_config(model_name, config)
                clean_model_name = ollama_config["model"]
                
                # æ£€æŸ¥OllamaæœåŠ¡å¥åº·çŠ¶æ€
                if not self._check_ollama_service_health(ollama_config["base_url"]):
                    logger.warning(f"âš ï¸ OllamaæœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥ï¼Œä½†ä»å°è¯•åˆ›å»ºç”Ÿæˆå™¨")
                
                # è®°å½•è¯¦ç»†çš„é…ç½®ä¿¡æ¯
                logger.debug(f"ğŸ”§ åˆ›å»ºOllama UnifiedGeneratoré…ç½®:")
                logger.debug(f"   æ¨¡å‹: {clean_model_name}")
                logger.debug(f"   æœåŠ¡åœ°å€: {ollama_config['base_url']}")
                logger.debug(f"   è¶…æ—¶: {ollama_config['timeout']}ç§’")
                logger.debug(f"   æœ€å¤§é‡è¯•: {ollama_config['max_retries']}æ¬¡")
                logger.debug(f"   é‡è¯•å»¶è¿Ÿ: {ollama_config['retry_delay']}ç§’")
                
                # åˆ›å»ºUnifiedGeneratorå®ä¾‹ï¼Œä½¿ç”¨Ollamaé…ç½®
                generator = UnifiedGenerator(
                    model=clean_model_name,
                    provider=provider,
                    base_url=ollama_config["base_url"],
                    generation_kwargs=ollama_config["generation_kwargs"],
                    timeout=ollama_config["timeout"],
                    max_retries=ollama_config["max_retries"],
                    retry_delay=ollama_config["retry_delay"],
                    enable_monitoring=True
                )
                
                logger.info(f"âœ… æˆåŠŸåˆ›å»ºOllama UnifiedGenerator")
                logger.info(f"   æ¨¡å‹: {clean_model_name}")
                logger.info(f"   æœåŠ¡åœ°å€: {ollama_config['base_url']}")
                
            else:
                # éOllamaæ¨¡å‹ï¼Œæ£€æµ‹æä¾›å•†ç±»å‹
                if "@" in model_name:
                    clean_model_name, provider = model_name.split("@", 1)
                else:
                    clean_model_name = model_name
                    provider = "openai"  # é»˜è®¤ä¸ºOpenAIå…¼å®¹
                
                logger.debug(f"ğŸ”§ åˆ›å»º{provider}UnifiedGeneratoré…ç½®:")
                logger.debug(f"   æ¨¡å‹: {clean_model_name}")
                logger.debug(f"   æä¾›å•†: {provider}")
                
                # ä¸ºOpenAIå…¼å®¹æ¨¡å‹å‡†å¤‡é…ç½®
                generator_config = {
                    "generation_kwargs": {
                        "max_tokens": config.get("max_tokens", 2000),
                        "temperature": config.get("temperature", 0.1)
                    },
                    "timeout": config.get("timeout", 30.0),
                    "max_retries": config.get("max_retries", 3),
                    "enable_monitoring": True
                }
                
                # å¦‚æœæ˜¯OpenAIå…¼å®¹æ¨¡å‹ï¼Œæ·»åŠ APIé…ç½®
                if provider == "openai":
                    openai_config = self._get_openai_config(model_name)
                    if openai_config.get("api_key"):
                        generator_config["api_key"] = openai_config["api_key"]
                    if openai_config.get("base_url"):
                        generator_config["base_url"] = openai_config["base_url"]
                
                generator = UnifiedGenerator(
                    model=clean_model_name,
                    provider=provider,
                    **generator_config
                )
                
                logger.info(f"âœ… æˆåŠŸåˆ›å»º{provider}UnifiedGeneratorï¼Œæ¨¡å‹: {clean_model_name}")
            
            return generator
            
        except Exception as e:
            # ä½¿ç”¨å¢å¼ºçš„é”™è¯¯å¤„ç†
            should_fallback = self._handle_ollama_error(e, model_name, "generator_creation")
            if should_fallback:
                logger.info(f"ğŸ”„ UnifiedGeneratoråˆ›å»ºå¤±è´¥ï¼Œç³»ç»Ÿå°†å›é€€åˆ°ä¼ ç»Ÿæ‰§è¡Œæ–¹å¼")
            return None
    
    def build_security_scan_pipeline(self, task_config: Dict[str, Any] = None) -> Pipeline:
        """æ„å»ºå®‰å…¨æ‰«æç®¡é“"""
        if not HAYSTACK_AVAILABLE:
            return None
        
        # ä½¿ç”¨ä»»åŠ¡ç‰¹å®šé…ç½®æˆ–é»˜è®¤é…ç½®
        config = task_config or {
            "model_name": self.config.model_name,
            "max_tokens": self.config.max_tokens,
            "temperature": self.config.temperature
        }
        
        # æ£€æŸ¥æ¨¡å‹ç±»å‹å¹¶é€‰æ‹©åˆé€‚çš„ç”Ÿæˆå™¨
        model_name = config["model_name"]
        is_ollama = self._is_ollama_model(model_name)
        is_openai_compatible = self._is_openai_compatible_model(model_name)
        
        if not is_ollama and not is_openai_compatible:
            logger.info(f"æ¨¡å‹ {model_name} æ—¢ä¸æ˜¯Ollamaæ¨¡å‹ä¹Ÿä¸å…¼å®¹OpenAI APIï¼Œå°†è·³è¿‡Haystackç®¡é“æ„å»º")
            return None
        
        # å®‰å…¨æ‰«ææç¤ºæ¨¡æ¿
        security_prompt_template = """
        å¯¹ä»¥ä¸‹ä»£ç è¿›è¡Œå®‰å…¨æ¼æ´åˆ†æï¼š
        
        ä»£ç å†…å®¹ï¼š
        {{ code_content }}
        
        æ–‡ä»¶è·¯å¾„ï¼š{{ file_path }}
        ç¼–ç¨‹è¯­è¨€ï¼š{{ language }}
        
        ç›¸å…³çŸ¥è¯†åº“ä¿¡æ¯ï¼š
        {{ knowledge_context }}
        
        è¯·æ£€æŸ¥ä»¥ä¸‹å®‰å…¨é—®é¢˜ï¼š
        1. SQLæ³¨å…¥æ¼æ´
        2. XSSè·¨ç«™è„šæœ¬
        3. å‘½ä»¤æ³¨å…¥
        4. è·¯å¾„éå†
        5. æƒé™ç»•è¿‡
        6. æ•æ„Ÿä¿¡æ¯æ³„éœ²
        7. ä¸å®‰å…¨çš„åŠ å¯†
        8. è¾“å…¥éªŒè¯ç¼ºå¤±
        
        å¯¹æ¯ä¸ªå‘ç°çš„é—®é¢˜ï¼Œè¯·æä¾›ï¼š
        - æ¼æ´ç±»å‹
        - ä¸¥é‡ç¨‹åº¦ï¼ˆcritical/high/medium/lowï¼‰
        - å…·ä½“ä½ç½®ï¼ˆè¡Œå·ï¼‰
        - è¯¦ç»†æè¿°
        - ä¿®å¤å»ºè®®
        - ç½®ä¿¡åº¦è¯„åˆ†ï¼ˆ0-1ï¼‰
        
        è¿”å›JSONæ ¼å¼çš„ç»“æœã€‚
        """
        
        pipeline = Pipeline()
        
        # æç¤ºæ„å»ºç»„ä»¶
        pipeline.add_component("prompt_builder", PromptBuilder(
            template=security_prompt_template,
            required_variables=["code_content", "file_path", "language", "knowledge_context"]
        ))
        
        # ä½¿ç”¨UnifiedGeneratoræ”¯æŒæ‰€æœ‰æä¾›å•†
        try:
            # è®°å½•æ‰§è¡Œè·¯å¾„
            provider = "ollama" if is_ollama else "openai"
            self._log_provider_execution_path(model_name, provider, "security_scan")
            
            # è·å–å¹¶è®¾ç½®é…ç½®ï¼ˆå¯¹äºOpenAIå…¼å®¹æ¨¡å‹ï¼‰
            if not is_ollama:
                openai_config = self._get_openai_config(model_name)
            
            # åˆ›å»ºUnifiedGenerator
            generator = self._create_unified_generator(model_name, config)
            if generator is None:
                logger.error(f"âŒ æ— æ³•åˆ›å»ºç”Ÿæˆå™¨ï¼Œæ¨¡å‹: {model_name}")
                self._implement_graceful_fallback(model_name, "security_scan", Exception("Generator creation failed"))
                return None
            
            pipeline.add_component("llm", generator)
            logger.info(f"âœ… ä½¿ç”¨UnifiedGeneratoråˆ›å»ºå®‰å…¨æ‰«æç®¡é“ï¼Œæ¨¡å‹: {model_name} ({provider})")
            
        except Exception as e:
            logger.error(f"âŒ UnifiedGeneratoråˆ›å»ºå¤±è´¥: {e}")
            # å¯¹äºOllamaé”™è¯¯ï¼Œå°è¯•ç‰¹æ®Šå¤„ç†
            if is_ollama:
                should_fallback = self._handle_ollama_error(e, model_name, "pipeline_integration")
                if should_fallback:
                    self._implement_graceful_fallback(model_name, "security_scan", e)
                return None
            else:
                self._implement_graceful_fallback(model_name, "security_scan", e)
                return None
        
        pipeline.connect("prompt_builder", "llm")
        
        return pipeline
    
    def build_syntax_check_pipeline(self, task_config: Dict[str, Any] = None) -> Pipeline:
        """æ„å»ºè¯­æ³•æ£€æŸ¥ç®¡é“"""
        if not HAYSTACK_AVAILABLE:
            return None
        
        # ä½¿ç”¨ä»»åŠ¡ç‰¹å®šé…ç½®æˆ–é»˜è®¤é…ç½®
        config = task_config or {
            "model_name": self.config.model_name,
            "max_tokens": self.config.max_tokens,
            "temperature": self.config.temperature
        }
        
        # æ£€æŸ¥æ¨¡å‹ç±»å‹å¹¶é€‰æ‹©åˆé€‚çš„ç”Ÿæˆå™¨
        model_name = config["model_name"]
        is_ollama = self._is_ollama_model(model_name)
        is_openai_compatible = self._is_openai_compatible_model(model_name)
        
        if not is_ollama and not is_openai_compatible:
            logger.info(f"æ¨¡å‹ {model_name} æ—¢ä¸æ˜¯Ollamaæ¨¡å‹ä¹Ÿä¸å…¼å®¹OpenAI APIï¼Œå°†è·³è¿‡Haystackç®¡é“æ„å»º")
            return None
        
        syntax_prompt_template = """
        åˆ†æä»¥ä¸‹ä»£ç çš„è¯­æ³•é—®é¢˜ï¼š
        
        ä»£ç å†…å®¹ï¼š
        {{ code_content }}
        
        æ–‡ä»¶è·¯å¾„ï¼š{{ file_path }}
        ç¼–ç¨‹è¯­è¨€ï¼š{{ language }}
        
        è¯·æ£€æŸ¥ï¼š
        1. è¯­æ³•é”™è¯¯
        2. ç¼©è¿›é—®é¢˜
        3. æ‹¬å·åŒ¹é…
        4. å˜é‡å£°æ˜
        5. å¯¼å…¥è¯­å¥
        
        è¿”å›JSONæ ¼å¼çš„ç»“æœï¼ŒåŒ…å«å‘ç°çš„é—®é¢˜åˆ—è¡¨ã€‚
        """
        
        pipeline = Pipeline()
        
        # æç¤ºæ„å»ºç»„ä»¶
        pipeline.add_component("prompt_builder", PromptBuilder(
            template=syntax_prompt_template,
            required_variables=["code_content", "file_path", "language"]
        ))
        
        # ä½¿ç”¨UnifiedGeneratoræ”¯æŒæ‰€æœ‰æä¾›å•†
        try:
            # è®°å½•æ‰§è¡Œè·¯å¾„
            provider = "ollama" if is_ollama else "openai"
            self._log_provider_execution_path(model_name, provider, "syntax_check")
            
            # è·å–å¹¶è®¾ç½®é…ç½®ï¼ˆå¯¹äºOpenAIå…¼å®¹æ¨¡å‹ï¼‰
            if not is_ollama:
                openai_config = self._get_openai_config(model_name)
            
            # åˆ›å»ºUnifiedGenerator
            generator = self._create_unified_generator(model_name, config)
            if generator is None:
                logger.error(f"âŒ æ— æ³•åˆ›å»ºç”Ÿæˆå™¨ï¼Œæ¨¡å‹: {model_name}")
                self._implement_graceful_fallback(model_name, "syntax_check", Exception("Generator creation failed"))
                return None
            
            pipeline.add_component("llm", generator)
            logger.info(f"âœ… ä½¿ç”¨UnifiedGeneratoråˆ›å»ºè¯­æ³•æ£€æŸ¥ç®¡é“ï¼Œæ¨¡å‹: {model_name} ({provider})")
            
        except Exception as e:
            logger.error(f"âŒ UnifiedGeneratoråˆ›å»ºå¤±è´¥: {e}")
            # å¯¹äºOllamaé”™è¯¯ï¼Œå°è¯•ç‰¹æ®Šå¤„ç†
            if is_ollama:
                should_fallback = self._handle_ollama_error(e, model_name, "pipeline_integration")
                if should_fallback:
                    self._implement_graceful_fallback(model_name, "syntax_check", e)
                return None
            else:
                self._implement_graceful_fallback(model_name, "syntax_check", e)
                return None
        
        pipeline.connect("prompt_builder", "llm")
        return pipeline
    
    def build_logic_analysis_pipeline(self, task_config: Dict[str, Any] = None) -> Pipeline:
        """æ„å»ºé€»è¾‘åˆ†æç®¡é“"""
        if not HAYSTACK_AVAILABLE:
            return None
        
        # ä½¿ç”¨ä»»åŠ¡ç‰¹å®šé…ç½®æˆ–é»˜è®¤é…ç½®
        config = task_config or {
            "model_name": self.config.model_name,
            "max_tokens": self.config.max_tokens,
            "temperature": self.config.temperature
        }
        
        # æ£€æŸ¥æ¨¡å‹ç±»å‹å¹¶é€‰æ‹©åˆé€‚çš„ç”Ÿæˆå™¨
        model_name = config["model_name"]
        is_ollama = self._is_ollama_model(model_name)
        is_openai_compatible = self._is_openai_compatible_model(model_name)
        
        if not is_ollama and not is_openai_compatible:
            logger.info(f"æ¨¡å‹ {model_name} æ—¢ä¸æ˜¯Ollamaæ¨¡å‹ä¹Ÿä¸å…¼å®¹OpenAI APIï¼Œå°†è·³è¿‡Haystackç®¡é“æ„å»º")
            return None
        
        logic_prompt_template = """
        åˆ†æä»¥ä¸‹ä»£ç çš„é€»è¾‘é—®é¢˜ï¼š
        
        ä»£ç å†…å®¹ï¼š
        {{ code_content }}
        
        æ–‡ä»¶è·¯å¾„ï¼š{{ file_path }}
        ç¼–ç¨‹è¯­è¨€ï¼š{{ language }}
        
        ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
        {{ context_info }}
        
        è¯·æ£€æŸ¥ï¼š
        1. é€»è¾‘é”™è¯¯
        2. æ­»ä»£ç 
        3. æ— é™å¾ªç¯
        4. ç©ºæŒ‡é’ˆå¼•ç”¨
        5. èµ„æºæ³„éœ²
        6. å¹¶å‘é—®é¢˜
        7. å¼‚å¸¸å¤„ç†
        8. è¾¹ç•Œæ¡ä»¶
        
        è¿”å›JSONæ ¼å¼çš„ç»“æœï¼ŒåŒ…å«å‘ç°çš„é€»è¾‘é—®é¢˜ã€‚
        """
        
        pipeline = Pipeline()
        
        # æç¤ºæ„å»ºç»„ä»¶
        pipeline.add_component("prompt_builder", PromptBuilder(
            template=logic_prompt_template,
            required_variables=["code_content", "file_path", "language", "context_info"]
        ))
        
        # ä½¿ç”¨UnifiedGeneratoræ”¯æŒæ‰€æœ‰æä¾›å•†
        try:
            # è®°å½•æ‰§è¡Œè·¯å¾„
            provider = "ollama" if is_ollama else "openai"
            self._log_provider_execution_path(model_name, provider, "logic_analysis")
            
            # è·å–å¹¶è®¾ç½®é…ç½®ï¼ˆå¯¹äºOpenAIå…¼å®¹æ¨¡å‹ï¼‰
            if not is_ollama:
                openai_config = self._get_openai_config(model_name)
            
            # åˆ›å»ºUnifiedGenerator
            generator = self._create_unified_generator(model_name, config)
            if generator is None:
                logger.error(f"âŒ æ— æ³•åˆ›å»ºç”Ÿæˆå™¨ï¼Œæ¨¡å‹: {model_name}")
                self._implement_graceful_fallback(model_name, "logic_analysis", Exception("Generator creation failed"))
                return None
            
            pipeline.add_component("llm", generator)
            logger.info(f"âœ… ä½¿ç”¨UnifiedGeneratoråˆ›å»ºé€»è¾‘åˆ†æç®¡é“ï¼Œæ¨¡å‹: {model_name} ({provider})")
            
        except Exception as e:
            logger.error(f"âŒ UnifiedGeneratoråˆ›å»ºå¤±è´¥: {e}")
            # å¯¹äºOllamaé”™è¯¯ï¼Œå°è¯•ç‰¹æ®Šå¤„ç†
            if is_ollama:
                should_fallback = self._handle_ollama_error(e, model_name, "pipeline_integration")
                if should_fallback:
                    self._implement_graceful_fallback(model_name, "logic_analysis", e)
                return None
            else:
                self._implement_graceful_fallback(model_name, "logic_analysis", e)
                return None
        
        pipeline.connect("prompt_builder", "llm")
        return pipeline
    
    def build_dependency_analysis_pipeline(self, task_config: Dict[str, Any] = None) -> Pipeline:
        """æ„å»ºä¾èµ–åˆ†æç®¡é“"""
        if not HAYSTACK_AVAILABLE:
            return None
        
        # ä½¿ç”¨ä»»åŠ¡ç‰¹å®šé…ç½®æˆ–é»˜è®¤é…ç½®
        config = task_config or {
            "model_name": self.config.model_name,
            "max_tokens": self.config.max_tokens,
            "temperature": self.config.temperature
        }
        
        # æ£€æŸ¥æ¨¡å‹ç±»å‹å¹¶é€‰æ‹©åˆé€‚çš„ç”Ÿæˆå™¨
        model_name = config["model_name"]
        is_ollama = self._is_ollama_model(model_name)
        is_openai_compatible = self._is_openai_compatible_model(model_name)
        
        if not is_ollama and not is_openai_compatible:
            logger.info(f"æ¨¡å‹ {model_name} æ—¢ä¸æ˜¯Ollamaæ¨¡å‹ä¹Ÿä¸å…¼å®¹OpenAI APIï¼Œå°†è·³è¿‡Haystackç®¡é“æ„å»º")
            return None
        
        dependency_prompt_template = """
        åˆ†æä»¥ä¸‹ä»£ç çš„ä¾èµ–å…³ç³»é—®é¢˜ï¼š
        
        ä»£ç å†…å®¹ï¼š
        {{ code_content }}
        
        æ–‡ä»¶è·¯å¾„ï¼š{{ file_path }}
        ç¼–ç¨‹è¯­è¨€ï¼š{{ language }}
        
        ä¾èµ–ä¿¡æ¯ï¼š
        {{ dependency_info }}
        
        è¯·æ£€æŸ¥ï¼š
        1. è¿‡æ—¶çš„ä¾èµ–
        2. å®‰å…¨æ¼æ´çš„ä¾èµ–
        3. å¾ªç¯ä¾èµ–
        4. æœªä½¿ç”¨çš„ä¾èµ–
        5. ç‰ˆæœ¬å†²çª
        6. è®¸å¯è¯é—®é¢˜
        
        è¿”å›JSONæ ¼å¼çš„ç»“æœã€‚
        """
        
        pipeline = Pipeline()
        
        # æç¤ºæ„å»ºç»„ä»¶
        pipeline.add_component("prompt_builder", PromptBuilder(
            template=dependency_prompt_template,
            required_variables=["code_content", "file_path", "language", "dependency_info"]
        ))
        
        # ä½¿ç”¨UnifiedGeneratoræ”¯æŒæ‰€æœ‰æä¾›å•†
        try:
            # è®°å½•æ‰§è¡Œè·¯å¾„
            provider = "ollama" if is_ollama else "openai"
            self._log_provider_execution_path(model_name, provider, "dependency_analysis")
            
            # è·å–å¹¶è®¾ç½®é…ç½®ï¼ˆå¯¹äºOpenAIå…¼å®¹æ¨¡å‹ï¼‰
            if not is_ollama:
                openai_config = self._get_openai_config(model_name)
            
            # åˆ›å»ºUnifiedGenerator
            generator = self._create_unified_generator(model_name, config)
            if generator is None:
                logger.error(f"âŒ æ— æ³•åˆ›å»ºç”Ÿæˆå™¨ï¼Œæ¨¡å‹: {model_name}")
                self._implement_graceful_fallback(model_name, "dependency_analysis", Exception("Generator creation failed"))
                return None
            
            pipeline.add_component("llm", generator)
            logger.info(f"âœ… ä½¿ç”¨UnifiedGeneratoråˆ›å»ºä¾èµ–åˆ†æç®¡é“ï¼Œæ¨¡å‹: {model_name} ({provider})")
            
        except Exception as e:
            logger.error(f"âŒ UnifiedGeneratoråˆ›å»ºå¤±è´¥: {e}")
            # å¯¹äºOllamaé”™è¯¯ï¼Œå°è¯•ç‰¹æ®Šå¤„ç†
            if is_ollama:
                should_fallback = self._handle_ollama_error(e, model_name, "pipeline_integration")
                if should_fallback:
                    self._implement_graceful_fallback(model_name, "dependency_analysis", e)
                return None
            else:
                self._implement_graceful_fallback(model_name, "dependency_analysis", e)
                return None
        
        pipeline.connect("prompt_builder", "llm")
        return pipeline

class HaystackAIOrchestrator:
    """åŸºäºHaystack-AIçš„ä¸»ç¼–æ’å™¨ - å±‚çº§RAGæ¶æ„çš„æ ¸å¿ƒç¼–æ’ç»„ä»¶"""
    
    def __init__(self, workers: int = None, pipeline_config: HaystackPipelineConfig = None):
        """åˆå§‹åŒ–ç¼–æ’å™¨"""
        # ä»é…ç½®è·å–é»˜è®¤å€¼
        haystack_config = Config.get_hierarchical_layer_config("haystack")
        
        if workers is None:
            self.workers = haystack_config.max_workers if haystack_config else 10
        else:
            self.workers = workers
            
        if pipeline_config is None:
            self.pipeline_config = HaystackPipelineConfig.from_config()
        else:
            self.pipeline_config = pipeline_config
        
        # åˆå§‹åŒ–å„å±‚ç»„ä»¶
        self.txtai_retriever = TxtaiRetriever()
        self.r2r_enhancer = R2REnhancer()
        self.self_rag_validator = SelfRAGValidator()
        
        # åˆå§‹åŒ–ä»»åŠ¡åˆ†è§£å™¨ã€å¹¶è¡Œæ‰§è¡Œå™¨å’Œç»“æœæ•´åˆå™¨
        self.task_decomposer = TaskDecomposer()
        self.parallel_executor = ParallelProcessingManager(max_workers=workers)
        self.task_scheduler = TaskScheduler()
        self.result_integrator = ResultIntegrator()
        
        # åˆå§‹åŒ–Haystackç®¡é“æ„å»ºå™¨
        self.pipeline_builder = HaystackPipelineBuilder(self.pipeline_config)
        
        # æ„å»ºå„ç§åˆ†æç®¡é“
        self.pipelines = {}
        if HAYSTACK_AVAILABLE:
            self._build_pipelines()
        
        # æ€§èƒ½ç›‘æ§
        self.performance_metrics = {
            "tasks_completed": 0,
            "total_execution_time": 0.0,
            "layer_performance": {
                "haystack": {"calls": 0, "total_time": 0.0},
                "txtai": {"calls": 0, "total_time": 0.0},
                "r2r": {"calls": 0, "total_time": 0.0},
                "self_rag": {"calls": 0, "total_time": 0.0}
            }
        }
        
        # å…¼å®¹æ€§æ”¯æŒ
        self.agents = {}
        self.code_units = []
        
        logger.info(f"Haystack-AIç¼–æ’å™¨åˆå§‹åŒ–å®Œæˆï¼Œå·¥ä½œçº¿ç¨‹æ•°: {self.workers}, Haystackå¯ç”¨: {HAYSTACK_AVAILABLE}")
    
    def _get_task_specific_config(self, task_type: TaskType) -> Dict[str, Any]:
        """è·å–ä»»åŠ¡ç‰¹å®šçš„é…ç½®"""
        # ä»ç»Ÿä¸€æ¨¡å‹é…ç½®è·å–æ¨¡å‹
        model_name = Config.get_task_model(task_type.value)
        
        logger.info(f"ğŸš€ Haystackç¼–æ’å±‚ - ä»»åŠ¡ {task_type.value} ä½¿ç”¨æ¨¡å‹: {model_name}")
        
        # è·å–Haystackå±‚é…ç½®
        try:
            # ç›´æ¥ä»hierarchical_rag_modelsè·å–é…ç½®
            haystack_config = Config.hierarchical_rag_models.haystack
            task_models = haystack_config.get("task_models", {})
            
            if task_type.value in task_models:
                # ä½¿ç”¨ä»»åŠ¡ç‰¹å®šæ¨¡å‹
                task_model = task_models[task_type.value]
                config = {
                    "model_name": task_model,
                    "max_tokens": self.pipeline_config.max_tokens,
                    "temperature": self.pipeline_config.temperature,
                    "top_k": self.pipeline_config.top_k,
                }
                logger.debug(f"Haystackä»»åŠ¡ {task_type.value} é…ç½®: {config}")
                return config
        except Exception as e:
            logger.warning(f"è·å–Haystackä»»åŠ¡é…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        
        # å›é€€åˆ°é»˜è®¤é…ç½®
        config = {
            "model_name": model_name,
            "max_tokens": self.pipeline_config.max_tokens,
            "temperature": self.pipeline_config.temperature,
            "top_k": self.pipeline_config.top_k,
            "similarity_threshold": self.pipeline_config.similarity_threshold,
            "enable_embeddings": self.pipeline_config.enable_embeddings,
            "enable_ranking": self.pipeline_config.enable_ranking
        }
        logger.debug(f"Haystackä»»åŠ¡ {task_type.value} ä½¿ç”¨é»˜è®¤é…ç½®: {config}")
        return config
    
    def _build_pipelines(self):
        """æ„å»ºHaystackåˆ†æç®¡é“"""
        try:
            self.pipelines = {}
            
            # ä¸ºæ¯ç§ä»»åŠ¡ç±»å‹æ„å»ºç®¡é“ï¼Œä½¿ç”¨ä»»åŠ¡ç‰¹å®šé…ç½®
            for task_type in TaskType:
                task_config = self._get_task_specific_config(task_type)
                
                if task_type == TaskType.SYNTAX_CHECK:
                    pipeline = self.pipeline_builder.build_syntax_check_pipeline(task_config)
                elif task_type == TaskType.SECURITY_SCAN:
                    pipeline = self.pipeline_builder.build_security_scan_pipeline(task_config)
                elif task_type == TaskType.LOGIC_ANALYSIS:
                    pipeline = self.pipeline_builder.build_logic_analysis_pipeline(task_config)
                elif task_type == TaskType.DEPENDENCY_ANALYSIS:
                    pipeline = self.pipeline_builder.build_dependency_analysis_pipeline(task_config)
                else:
                    continue
                
                if pipeline:
                    self.pipelines[task_type] = pipeline
                    logger.debug(f"æˆåŠŸæ„å»º {task_type.value} ç®¡é“")
                else:
                    logger.info(f"è·³è¿‡ {task_type.value} ç®¡é“æ„å»ºï¼ˆæ¨¡å‹ä¸å…¼å®¹æˆ–å…¶ä»–åŸå› ï¼‰")
            
            if self.pipelines:
                logger.info(f"Haystackåˆ†æç®¡é“æ„å»ºå®Œæˆï¼Œå…±æ„å»º {len(self.pipelines)} ä¸ªç®¡é“")
            else:
                logger.info("æœªæ„å»ºä»»ä½•Haystackç®¡é“ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼æ‰§è¡Œä»»åŠ¡")
            
        except Exception as e:
            logger.error(f"æ„å»ºHaystackç®¡é“å¤±è´¥: {e}")
            self.pipelines = {}
    
    async def orchestrate_audit(self, source_files: List[SourceFile]) -> AuditResult:
        """ä¸»ç¼–æ’æµç¨‹ - æ‰§è¡Œå®Œæ•´çš„å±‚çº§RAGå®¡è®¡"""
        start_time = time.time()
        logger.info(f"ğŸš€ å¼€å§‹Haystack-AIå±‚çº§RAGå®¡è®¡ï¼Œæ–‡ä»¶æ•°: {len(source_files)}")
        
        try:
            # 1. ä»»åŠ¡åˆ†è§£
            task_collection = await self._decompose_audit_tasks(source_files)
            logger.info(f"ğŸ“‹ ä»»åŠ¡åˆ†è§£å®Œæˆï¼Œç”Ÿæˆ {len(task_collection)} ä¸ªå®¡è®¡ä»»åŠ¡")
            
            # 2. å¹¶è¡Œæ‰§è¡Œå„ç±»ä»»åŠ¡ï¼ˆä½¿ç”¨Haystackç®¡é“ï¼‰
            task_results = await self._execute_tasks_parallel(task_collection.tasks)
            logger.info(f"âš¡ å¹¶è¡Œä»»åŠ¡æ‰§è¡Œå®Œæˆï¼Œè·å¾— {len(task_results)} ä¸ªç»“æœ")
            
            # 3. æ”¶é›†æ‰€æœ‰æ¼æ´
            all_vulnerabilities = []
            for result in task_results:
                all_vulnerabilities.extend(result.vulnerabilities)
            
            # 4. txtaiå±‚ï¼šçŸ¥è¯†æ£€ç´¢å¢å¼º
            enhanced_vulnerabilities = await self._apply_txtai_enhancement(all_vulnerabilities)
            logger.info(f"ğŸ” txtaiçŸ¥è¯†æ£€ç´¢å®Œæˆï¼Œå¢å¼ºäº† {len(enhanced_vulnerabilities)} ä¸ªæ¼æ´")
            
            # 5. R2Rå±‚ï¼šä¸Šä¸‹æ–‡å¢å¼º
            context_enhanced_vulnerabilities = await self._apply_r2r_enhancement(
                enhanced_vulnerabilities, source_files
            )
            logger.info(f"ğŸ”— R2Rä¸Šä¸‹æ–‡å¢å¼ºå®Œæˆï¼Œå¤„ç†äº† {len(context_enhanced_vulnerabilities)} ä¸ªæ¼æ´")
            
            # 6. Self-RAGå±‚ï¼šéªŒè¯ä¸è¿‡æ»¤
            validated_vulnerabilities = await self._apply_self_rag_validation(
                context_enhanced_vulnerabilities
            )
            logger.info(f"âœ… Self-RAGéªŒè¯å®Œæˆï¼ŒéªŒè¯äº† {len(validated_vulnerabilities)} ä¸ªæ¼æ´")
            
            # 7. ç»“æœæ•´åˆ
            audit_result = await self._integrate_results(
                validated_vulnerabilities, task_results, start_time
            )
            
            # 8. æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            self._update_performance_metrics(audit_result.processing_time)
            
            logger.info(f"ğŸ‰ Haystack-AIå±‚çº§RAGå®¡è®¡å®Œæˆï¼Œè€—æ—¶: {audit_result.processing_time:.2f}ç§’")
            logger.info(f"ğŸ“Š å‘ç°æ¼æ´: {len(audit_result.vulnerabilities)}ï¼Œç½®ä¿¡åº¦: {audit_result.confidence_score:.2f}")
            
            # 9. è¾“å‡ºæ¨¡å‹ä½¿ç”¨ç»Ÿè®¡
            try:
                from auditluma.monitoring.model_usage_logger import model_usage_logger
                logger.info("ğŸ“Š ç”Ÿæˆæ¨¡å‹ä½¿ç”¨ç»Ÿè®¡æ‘˜è¦...")
                model_usage_logger.print_session_summary()
            except Exception as e:
                logger.warning(f"ç”Ÿæˆæ¨¡å‹ä½¿ç”¨ç»Ÿè®¡å¤±è´¥: {e}")
            
            return audit_result
            
        except Exception as e:
            logger.error(f"âŒ Haystack-AIç¼–æ’è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise
    
    async def _decompose_audit_tasks(self, source_files: List[SourceFile]) -> TaskCollection:
        """ä»»åŠ¡åˆ†è§£"""
        return await self.task_decomposer.decompose_audit_tasks(source_files)
    
    async def _execute_tasks_parallel(self, tasks: List[AuditTask]) -> List[TaskResult]:
        """å¹¶è¡Œæ‰§è¡Œä»»åŠ¡ - ä½¿ç”¨Haystackç®¡é“"""
        if not tasks:
            return []
        
        # è°ƒåº¦ä»»åŠ¡æ‰§è¡Œé¡ºåº
        scheduled_tasks = self.task_scheduler.schedule_tasks(
            TaskCollection(tasks=tasks), strategy="hybrid"
        )
        
        # åˆ›å»ºä»»åŠ¡æ‰§è¡Œå™¨ï¼ˆä½¿ç”¨Haystackç®¡é“ï¼‰
        async def haystack_task_executor(task: AuditTask):
            """Haystackä»»åŠ¡æ‰§è¡Œå™¨"""
            try:
                start_time = time.time()
                
                # ä½¿ç”¨å¯¹åº”çš„Haystackç®¡é“æ‰§è¡Œä»»åŠ¡
                vulnerabilities = await self._execute_with_haystack_pipeline(task)
                
                execution_time = time.time() - start_time
                confidence = self._calculate_task_confidence(task, vulnerabilities)
                
                # æ›´æ–°Haystackå±‚æ€§èƒ½æŒ‡æ ‡
                self.performance_metrics["layer_performance"]["haystack"]["calls"] += 1
                self.performance_metrics["layer_performance"]["haystack"]["total_time"] += execution_time
                
                return TaskResult(
                    task_id=task.id,
                    task_type=task.task_type,
                    vulnerabilities=vulnerabilities,
                    execution_time=execution_time,
                    confidence=confidence,
                    metadata={
                        **task.metadata,
                        "haystack_pipeline_used": True,
                        "pipeline_type": task.task_type.value
                    }
                )
            except Exception as e:
                logger.error(f"Haystackä»»åŠ¡æ‰§è¡Œå¤±è´¥: {task.id}, {e}")
                return TaskResult(
                    task_id=task.id,
                    task_type=task.task_type,
                    vulnerabilities=[],
                    execution_time=0.0,
                    confidence=0.0,
                    metadata={"error": str(e), "haystack_pipeline_used": False}
                )
        
        # ä½¿ç”¨å¹¶è¡Œæ‰§è¡Œå¼•æ“æ‰§è¡Œä»»åŠ¡
        execution_result = await self.parallel_executor.execute_tasks(
            scheduled_tasks, haystack_task_executor
        )
        
        # è½¬æ¢æ‰§è¡Œç»“æœä¸ºTaskResultåˆ—è¡¨
        task_results = []
        for task_execution in execution_result.task_executions:
            if task_execution.result and isinstance(task_execution.result, TaskResult):
                task_execution.result.execution_time = task_execution.execution_time
                task_results.append(task_execution.result)
            else:
                # åˆ›å»ºå¤±è´¥çš„TaskResult
                task_result = TaskResult(
                    task_id=task_execution.task.id,
                    task_type=task_execution.task.task_type,
                    vulnerabilities=[],
                    execution_time=task_execution.execution_time,
                    confidence=0.0,
                    metadata={
                        "status": task_execution.status.value,
                        "error": str(task_execution.error) if task_execution.error else None,
                        "haystack_pipeline_used": False
                    }
                )
                task_results.append(task_result)
        
        logger.info(f"Haystackå¹¶è¡Œæ‰§è¡Œç»Ÿè®¡: æˆåŠŸ {execution_result.successful_tasks}, "
                   f"å¤±è´¥ {execution_result.failed_tasks}")
        
        return task_results
    
    async def _execute_with_haystack_pipeline(self, task: AuditTask) -> List[VulnerabilityResult]:
        """ä½¿ç”¨Haystackç®¡é“æ‰§è¡Œä»»åŠ¡"""
        if not HAYSTACK_AVAILABLE or not self.pipelines:
            logger.info(f"Haystackä¸å¯ç”¨æˆ–ç®¡é“ä¸ºç©ºï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹å¼æ‰§è¡Œä»»åŠ¡: {task.id}")
            return await self._execute_traditional_task(task)
        
        try:
            pipeline = self.pipelines.get(task.task_type)
            if not pipeline:
                logger.warning(f"æœªæ‰¾åˆ°ä»»åŠ¡ç±»å‹ {task.task_type} çš„Haystackç®¡é“ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹å¼")
                return await self._execute_traditional_task(task)
            
            vulnerabilities = []
            
            # ä¸ºæ¯ä¸ªæºæ–‡ä»¶æ‰§è¡Œç®¡é“
            for source_file in task.source_files:
                try:
                    # å‡†å¤‡ç®¡é“è¾“å…¥
                    pipeline_input = {
                        "code_content": source_file.content,
                        "file_path": str(source_file.path),
                        "language": getattr(source_file, 'language', 'unknown'),
                        "task_type": task.task_type.value
                    }
                    
                    # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
                    if task.task_type == TaskType.SECURITY_SCAN:
                        knowledge_context = await self._get_knowledge_context(source_file)
                        pipeline_input["knowledge_context"] = knowledge_context
                    elif task.task_type == TaskType.LOGIC_ANALYSIS:
                        context_info = await self._get_context_info(source_file, task.code_units)
                        pipeline_input["context_info"] = context_info
                    elif task.task_type == TaskType.DEPENDENCY_ANALYSIS:
                        dependency_info = await self._get_dependency_info(source_file)
                        pipeline_input["dependency_info"] = dependency_info
                    
                    # è·å–ä»»åŠ¡é…ç½®ä»¥è®°å½•ä½¿ç”¨çš„æ¨¡å‹
                    task_config = self._get_task_specific_config(task.task_type)
                    model_name = task_config.get("model_name", "unknown")
                    
                    logger.info(f"ğŸš€ Haystackç¼–æ’å±‚ - æ‰§è¡Œ {task.task_type.value} ç®¡é“ï¼Œæ–‡ä»¶: {source_file.path}")
                    logger.info(f"ğŸš€ Haystackç¼–æ’å±‚ - ä½¿ç”¨æ¨¡å‹: {model_name}")
                    logger.debug(f"ç®¡é“è¾“å…¥å‚æ•°: {list(pipeline_input.keys())}")
                    
                    # æ‰§è¡Œç®¡é“ï¼ˆæ·»åŠ è¶…æ—¶ä¿æŠ¤ï¼‰
                    try:
                        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡ŒåŒæ­¥ç®¡é“è°ƒç”¨ï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
                        import concurrent.futures
                        with concurrent.futures.ThreadPoolExecutor() as executor:
                            future = executor.submit(pipeline.run, pipeline_input)
                            result = await asyncio.get_event_loop().run_in_executor(
                                None, lambda: future.result(timeout=30)  # 30ç§’è¶…æ—¶
                            )
                    except concurrent.futures.TimeoutError:
                        logger.error(f"Haystackç®¡é“æ‰§è¡Œè¶…æ—¶: {task.task_type.value}, æ–‡ä»¶: {source_file.path}")
                        continue
                    except Exception as pipeline_error:
                        logger.error(f"Haystackç®¡é“æ‰§è¡Œå¼‚å¸¸: {task.task_type.value}, æ–‡ä»¶: {source_file.path}, é”™è¯¯: {pipeline_error}")
                        continue
                    
                    logger.info(f"âœ… Haystackç¼–æ’å±‚ - {task.task_type.value} ç®¡é“æ‰§è¡Œå®Œæˆï¼Œæ¨¡å‹: {model_name}")
                    
                    # è§£æç»“æœ
                    file_vulnerabilities = await self._parse_pipeline_result(
                        result, source_file, task.task_type
                    )
                    vulnerabilities.extend(file_vulnerabilities)
                    
                    logger.info(f"âœ… Haystackç¼–æ’å±‚ - æ–‡ä»¶ {source_file.path} å‘ç° {len(file_vulnerabilities)} ä¸ªæ¼æ´")
                    
                except Exception as e:
                    logger.error(f"Haystackç®¡é“æ‰§è¡Œå¤±è´¥ï¼Œæ–‡ä»¶: {source_file.path}, é”™è¯¯: {e}")
                    continue
            
            return vulnerabilities
            
        except Exception as e:
            logger.error(f"Haystackç®¡é“æ‰§è¡Œå‡ºé”™: {e}")
            return await self._execute_traditional_task(task)
    
    async def _execute_traditional_task(self, task: AuditTask) -> List[VulnerabilityResult]:
        """ä¼ ç»Ÿä»»åŠ¡æ‰§è¡Œæ–¹å¼ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        vulnerabilities = []
        
        for code_unit in task.code_units:
            if task.task_type == TaskType.SYNTAX_CHECK:
                if await self._has_syntax_issues(code_unit):
                    vuln = VulnerabilityResult(
                        id=f"syntax_{uuid.uuid4().hex[:8]}",
                        vulnerability_type="Syntax Error",
                        severity="low",
                        description="ä»£ç è¯­æ³•é—®é¢˜",
                        file_path=str(code_unit.source_file.path),
                        start_line=code_unit.start_line,
                        end_line=code_unit.end_line,
                        snippet=code_unit.content[:200],
                        confidence=0.9
                    )
                    vulnerabilities.append(vuln)
            
            elif task.task_type == TaskType.SECURITY_SCAN:
                security_vulns = await self._basic_security_scan(code_unit)
                vulnerabilities.extend(security_vulns)
        
        return vulnerabilities
    
    async def _get_knowledge_context(self, source_file: SourceFile) -> str:
        """è·å–çŸ¥è¯†åº“ä¸Šä¸‹æ–‡"""
        try:
            # æ·»åŠ è¶…æ—¶é˜²æ­¢å¡ä½
            knowledge_info = await asyncio.wait_for(
                self.txtai_retriever.retrieve_vulnerability_info(
                    "security_scan", source_file.content[:500]
                ),
                timeout=10.0  # 10ç§’è¶…æ—¶
            )
            
            context_parts = []
            if knowledge_info.best_practices:
                context_parts.append("æœ€ä½³å®è·µ:")
                context_parts.extend(knowledge_info.best_practices[:3])
            
            return "\n".join(context_parts)
            
        except asyncio.TimeoutError:
            logger.warning("è·å–çŸ¥è¯†åº“ä¸Šä¸‹æ–‡è¶…æ—¶")
            return "çŸ¥è¯†åº“ä¸Šä¸‹æ–‡è·å–è¶…æ—¶"
        except Exception as e:
            logger.warning(f"è·å–çŸ¥è¯†åº“ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return "æ— å¯ç”¨çŸ¥è¯†åº“ä¸Šä¸‹æ–‡"
    
    async def _get_context_info(self, source_file: SourceFile, code_units: List[CodeUnit]) -> str:
        """è·å–ä»£ç ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        try:
            context_parts = [f"æ–‡ä»¶: {source_file.path}"]
            
            functions = [unit for unit in code_units if unit.type == "function"]
            if functions:
                context_parts.append("å‡½æ•°åˆ—è¡¨:")
                for func in functions[:5]:
                    context_parts.append(f"- {func.name} (è¡Œ {func.start_line}-{func.end_line})")
            
            return "\n".join(context_parts)
            
        except Exception as e:
            logger.warning(f"è·å–ä¸Šä¸‹æ–‡ä¿¡æ¯å¤±è´¥: {e}")
            return f"æ–‡ä»¶: {source_file.path}"
    
    async def _get_dependency_info(self, source_file: SourceFile) -> str:
        """è·å–ä¾èµ–ä¿¡æ¯"""
        try:
            content = source_file.content
            dependencies = []
            
            # Python imports
            import_lines = [line.strip() for line in content.split('\n') 
                          if line.strip().startswith(('import ', 'from '))]
            dependencies.extend(import_lines[:10])
            
            if dependencies:
                return "ä¾èµ–å…³ç³»:\n" + "\n".join(dependencies)
            else:
                return "æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„ä¾èµ–å…³ç³»"
                
        except Exception as e:
            logger.warning(f"è·å–ä¾èµ–ä¿¡æ¯å¤±è´¥: {e}")
            return "ä¾èµ–ä¿¡æ¯ä¸å¯ç”¨"
    
    async def _parse_pipeline_result(self, result: Dict[str, Any], 
                                   source_file: SourceFile, 
                                   task_type: TaskType) -> List[VulnerabilityResult]:
        """è§£æHaystackç®¡é“ç»“æœ"""
        vulnerabilities = []
        
        try:
            # ä»ç®¡é“ç»“æœä¸­æå–ç”Ÿæˆçš„æ–‡æœ¬
            generated_text = ""
            if "llm" in result and "replies" in result["llm"]:
                generated_text = result["llm"]["replies"][0] if result["llm"]["replies"] else ""
            
            if not generated_text:
                logger.warning(f"Haystackç®¡é“æœªè¿”å›æœ‰æ•ˆç»“æœï¼Œä»»åŠ¡ç±»å‹: {task_type}")
                return vulnerabilities
            
            # å°è¯•è§£æJSONç»“æœ
            try:
                json_start = generated_text.find('{')
                json_end = generated_text.rfind('}') + 1
                
                if json_start >= 0 and json_end > json_start:
                    json_text = generated_text[json_start:json_end]
                    parsed_result = json.loads(json_text)
                    
                    vulnerabilities = self._extract_vulnerabilities_from_parsed_result(
                        parsed_result, source_file, task_type
                    )
                else:
                    vulnerabilities = self._extract_vulnerabilities_from_text(
                        generated_text, source_file, task_type
                    )
                    
            except json.JSONDecodeError:
                vulnerabilities = self._extract_vulnerabilities_from_text(
                    generated_text, source_file, task_type
                )
            
        except Exception as e:
            logger.error(f"è§£æHaystackç®¡é“ç»“æœå¤±è´¥: {e}")
        
        return vulnerabilities
    
    def _extract_vulnerabilities_from_parsed_result(self, parsed_result: Dict[str, Any], 
                                                  source_file: SourceFile, 
                                                  task_type: TaskType) -> List[VulnerabilityResult]:
        """ä»è§£æçš„JSONç»“æœä¸­æå–æ¼æ´"""
        vulnerabilities = []
        
        try:
            issues = []
            if "issues" in parsed_result:
                issues = parsed_result["issues"]
            elif "vulnerabilities" in parsed_result:
                issues = parsed_result["vulnerabilities"]
            elif isinstance(parsed_result, list):
                issues = parsed_result
            
            for issue in issues:
                if isinstance(issue, dict):
                    vuln = VulnerabilityResult(
                        id=f"{task_type.value}_{uuid.uuid4().hex[:8]}",
                        vulnerability_type=issue.get("type", f"{task_type.value}_issue"),
                        severity=issue.get("severity", "medium").lower(),
                        description=issue.get("description", ""),
                        file_path=str(source_file.path),
                        start_line=issue.get("line", 1),
                        end_line=issue.get("end_line", issue.get("line", 1)),
                        snippet=issue.get("code", ""),
                        confidence=float(issue.get("confidence", 0.7)),
                        recommendation=issue.get("fix", issue.get("recommendation", ""))
                    )
                    vulnerabilities.append(vuln)
                    
        except Exception as e:
            logger.error(f"ä»JSONç»“æœæå–æ¼æ´å¤±è´¥: {e}")
        
        return vulnerabilities
    
    def _extract_vulnerabilities_from_text(self, text: str, 
                                         source_file: SourceFile, 
                                         task_type: TaskType) -> List[VulnerabilityResult]:
        """ä»æ–‡æœ¬ç»“æœä¸­æå–æ¼æ´"""
        vulnerabilities = []
        
        try:
            lines = text.split('\n')
            current_issue = {}
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                if any(keyword in line.lower() for keyword in ['vulnerability', 'issue', 'problem', 'error']):
                    if current_issue:
                        vuln = self._create_vulnerability_from_text_issue(
                            current_issue, source_file, task_type
                        )
                        if vuln:
                            vulnerabilities.append(vuln)
                    
                    current_issue = {"description": line}
                
                elif current_issue:
                    current_issue["description"] = current_issue.get("description", "") + " " + line
            
            if current_issue:
                vuln = self._create_vulnerability_from_text_issue(
                    current_issue, source_file, task_type
                )
                if vuln:
                    vulnerabilities.append(vuln)
                    
        except Exception as e:
            logger.error(f"ä»æ–‡æœ¬ç»“æœæå–æ¼æ´å¤±è´¥: {e}")
        
        return vulnerabilities
    
    def _create_vulnerability_from_text_issue(self, issue: Dict[str, str], 
                                            source_file: SourceFile, 
                                            task_type: TaskType) -> Optional[VulnerabilityResult]:
        """ä»æ–‡æœ¬é—®é¢˜åˆ›å»ºæ¼æ´å¯¹è±¡"""
        try:
            description = issue.get("description", "")
            
            severity = "medium"
            if any(word in description.lower() for word in ['critical', 'severe']):
                severity = "critical"
            elif any(word in description.lower() for word in ['high', 'important']):
                severity = "high"
            elif any(word in description.lower() for word in ['low', 'minor']):
                severity = "low"
            
            import re
            line_match = re.search(r'line\s+(\d+)', description, re.IGNORECASE)
            line_number = int(line_match.group(1)) if line_match else 1
            
            return VulnerabilityResult(
                id=f"{task_type.value}_{uuid.uuid4().hex[:8]}",
                vulnerability_type=f"{task_type.value}_issue",
                severity=severity,
                description=description,
                file_path=str(source_file.path),
                start_line=line_number,
                end_line=line_number,
                snippet="",
                confidence=0.6
            )
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæ¼æ´å¯¹è±¡å¤±è´¥: {e}")
            return None   
 # å±‚çº§RAGå¤„ç†æ–¹æ³•
    async def _apply_txtai_enhancement(self, vulnerabilities: List[VulnerabilityResult]) -> List[VulnerabilityResult]:
        """åº”ç”¨txtaiå±‚çŸ¥è¯†æ£€ç´¢å¢å¼º"""
        start_time = time.time()
        
        enhanced_vulnerabilities = []
        for vuln in vulnerabilities:
            try:
                knowledge_info = await self.txtai_retriever.retrieve_vulnerability_info(
                    vuln.vulnerability_type, vuln.snippet
                )
                
                enhanced_vuln = await self.txtai_retriever.enhance_vulnerability(
                    vuln, knowledge_info
                )
                enhanced_vulnerabilities.append(enhanced_vuln)
                
            except Exception as e:
                logger.warning(f"txtaiå¢å¼ºå¤±è´¥: {vuln.id}, {e}")
                enhanced_vulnerabilities.append(vuln)
        
        execution_time = time.time() - start_time
        self.performance_metrics["layer_performance"]["txtai"]["calls"] += 1
        self.performance_metrics["layer_performance"]["txtai"]["total_time"] += execution_time
        
        return enhanced_vulnerabilities
    
    async def _apply_r2r_enhancement(self, vulnerabilities: List[VulnerabilityResult], 
                                   source_files: List[SourceFile]) -> List[VulnerabilityResult]:
        """åº”ç”¨R2Rå±‚ä¸Šä¸‹æ–‡å¢å¼º"""
        start_time = time.time()
        
        global_context = await self.r2r_enhancer.build_global_context(source_files)
        
        enhanced_vulnerabilities = []
        for vuln in vulnerabilities:
            try:
                enhanced_context = await self.r2r_enhancer.enhance_context(
                    vuln, global_context
                )
                
                enhanced_vuln = await self.r2r_enhancer.apply_context_enhancement(
                    vuln, enhanced_context
                )
                enhanced_vulnerabilities.append(enhanced_vuln)
                
            except Exception as e:
                logger.warning(f"R2Rå¢å¼ºå¤±è´¥: {vuln.id}, {e}")
                enhanced_vulnerabilities.append(vuln)
        
        execution_time = time.time() - start_time
        self.performance_metrics["layer_performance"]["r2r"]["calls"] += 1
        self.performance_metrics["layer_performance"]["r2r"]["total_time"] += execution_time
        
        return enhanced_vulnerabilities
    
    async def _apply_self_rag_validation(self, vulnerabilities: List[VulnerabilityResult]) -> List[VulnerabilityResult]:
        """åº”ç”¨Self-RAGå±‚éªŒè¯ä¸è¿‡æ»¤"""
        start_time = time.time()
        
        validated_vulnerabilities = []
        for vuln in vulnerabilities:
            try:
                validation_result = await self.self_rag_validator.validate_vulnerability(vuln)
                
                if validation_result.is_valid:
                    vuln.confidence = validation_result.confidence_score
                    vuln.validation_metadata = validation_result.metadata
                    validated_vulnerabilities.append(vuln)
                else:
                    logger.debug(f"æ¼æ´è¢«Self-RAGè¿‡æ»¤: {vuln.id}")
                
            except Exception as e:
                logger.warning(f"Self-RAGéªŒè¯å¤±è´¥: {vuln.id}, {e}")
                validated_vulnerabilities.append(vuln)
        
        execution_time = time.time() - start_time
        self.performance_metrics["layer_performance"]["self_rag"]["calls"] += 1
        self.performance_metrics["layer_performance"]["self_rag"]["total_time"] += execution_time
        
        return validated_vulnerabilities
    
    async def _integrate_results(self, vulnerabilities: List[VulnerabilityResult], 
                               task_results: List[TaskResult], start_time: float) -> AuditResult:
        """ç»“æœæ•´åˆ"""
        processing_time = time.time() - start_time
        
        integration_result = await self.result_integrator.integrate_results(
            task_results, ConflictResolutionStrategy.CONSENSUS
        )
        
        final_vulnerabilities = integration_result.integrated_vulnerabilities
        
        if final_vulnerabilities:
            confidence_score = sum(v.confidence for v in final_vulnerabilities) / len(final_vulnerabilities)
        else:
            confidence_score = 1.0
        
        execution_summary = {
            "total_tasks": len(task_results),
            "successful_tasks": len([r for r in task_results if not r.metadata.get("error")]),
            "total_vulnerabilities": len(final_vulnerabilities),
            "original_vulnerabilities": len(vulnerabilities),
            "duplicate_count": integration_result.duplicate_count,
            "conflict_count": integration_result.conflict_count,
            "processing_time": processing_time,
            "integration_time": integration_result.processing_time,
            "layer_performance": self.performance_metrics["layer_performance"],
            "task_breakdown": {
                task_type.value: len([r for r in task_results if r.task_type == task_type])
                for task_type in TaskType
            },
            "quality_metrics": integration_result.quality_metrics,
            "integration_metadata": integration_result.integration_metadata,
            "haystack_enabled": HAYSTACK_AVAILABLE,
            "haystack_pipelines_used": sum(1 for r in task_results if r.metadata.get("haystack_pipeline_used", False))
        }
        
        return AuditResult(
            vulnerabilities=final_vulnerabilities,
            task_results=task_results,
            execution_summary=execution_summary,
            confidence_score=confidence_score,
            processing_time=processing_time
        )
    
    # è¾…åŠ©æ–¹æ³•
    def _calculate_task_confidence(self, task: AuditTask, vulnerabilities: List[VulnerabilityResult]) -> float:
        """è®¡ç®—ä»»åŠ¡ç½®ä¿¡åº¦"""
        if not vulnerabilities:
            return 1.0
        
        base_confidence = 0.8
        
        type_multipliers = {
            TaskType.SYNTAX_CHECK: 0.95,
            TaskType.LOGIC_ANALYSIS: 0.85,
            TaskType.SECURITY_SCAN: 0.80,
            TaskType.DEPENDENCY_ANALYSIS: 0.90
        }
        
        multiplier = type_multipliers.get(task.task_type, 0.8)
        
        # å¦‚æœä½¿ç”¨äº†Haystackç®¡é“ï¼Œæé«˜ç½®ä¿¡åº¦
        if HAYSTACK_AVAILABLE and self.pipelines.get(task.task_type):
            multiplier *= 1.1
        
        return min(1.0, base_confidence * multiplier)
    
    async def _has_syntax_issues(self, code_unit: CodeUnit) -> bool:
        """æ£€æŸ¥ä»£ç å•å…ƒæ˜¯å¦æœ‰è¯­æ³•é—®é¢˜"""
        content = code_unit.content
        
        syntax_issues = [
            'SyntaxError',
            'IndentationError',
            'TabError',
            'unexpected EOF',
            'invalid syntax'
        ]
        
        return any(issue in content for issue in syntax_issues)
    
    async def _basic_security_scan(self, code_unit: CodeUnit) -> List[VulnerabilityResult]:
        """åŸºç¡€å®‰å…¨æ‰«æ"""
        vulnerabilities = []
        content = code_unit.content.lower()
        
        security_patterns = {
            "sql injection": ["select", "insert", "update", "delete", "drop"],
            "xss": ["<script", "javascript:", "eval("],
            "command injection": ["system(", "exec(", "shell_exec"],
            "path traversal": ["../", "..\\", "path.join"]
        }
        
        for vuln_type, patterns in security_patterns.items():
            for pattern in patterns:
                if pattern in content:
                    vuln = VulnerabilityResult(
                        id=f"security_{uuid.uuid4().hex[:8]}",
                        vulnerability_type=vuln_type,
                        severity="medium",
                        description=f"æ£€æµ‹åˆ°æ½œåœ¨çš„{vuln_type}é—®é¢˜",
                        file_path=str(code_unit.source_file.path),
                        start_line=code_unit.start_line,
                        end_line=code_unit.end_line,
                        snippet=code_unit.content[:200],
                        confidence=0.6
                    )
                    vulnerabilities.append(vuln)
                    break
        
        return vulnerabilities
    
    def _update_performance_metrics(self, processing_time: float):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        self.performance_metrics["tasks_completed"] += 1
        self.performance_metrics["total_execution_time"] += processing_time
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        total_tasks = self.performance_metrics["tasks_completed"]
        if total_tasks == 0:
            return {"message": "å°šæœªæ‰§è¡Œä»»ä½•ä»»åŠ¡"}
        
        avg_execution_time = self.performance_metrics["total_execution_time"] / total_tasks
        
        layer_summary = {}
        for layer, metrics in self.performance_metrics["layer_performance"].items():
            if metrics["calls"] > 0:
                layer_summary[layer] = {
                    "calls": metrics["calls"],
                    "avg_time": metrics["total_time"] / metrics["calls"],
                    "total_time": metrics["total_time"]
                }
        
        return {
            "total_tasks_completed": total_tasks,
            "average_execution_time": avg_execution_time,
            "total_execution_time": self.performance_metrics["total_execution_time"],
            "layer_performance": layer_summary,
            "haystack_enabled": HAYSTACK_AVAILABLE,
            "pipelines_available": len(self.pipelines) if self.pipelines else 0
        } 
   # ==================== å…¼å®¹æ€§æ¥å£ ====================
    # ä»¥ä¸‹æ–¹æ³•æä¾›ä¸ç°æœ‰AgentOrchestratorçš„å…¼å®¹æ€§
    
    async def initialize_agents(self) -> None:
        """åˆå§‹åŒ–æ™ºèƒ½ä½“ - å…¼å®¹æ€§æ–¹æ³•"""
        logger.info("Haystack-AIç¼–æ’å™¨ï¼šæ™ºèƒ½ä½“åˆå§‹åŒ–ï¼ˆå…¼å®¹æ€§æ¨¡å¼ï¼‰")
        self.agents = {
            "haystack_ai_orchestrator": self,
            "txtai_retriever": self.txtai_retriever,
            "r2r_enhancer": self.r2r_enhancer,
            "self_rag_validator": self.self_rag_validator,
            "haystack_pipelines": self.pipelines
        }
        logger.info(f"Haystack-AIç¼–æ’å™¨ï¼šå·²åˆå§‹åŒ– {len(self.agents)} ä¸ªç»„ä»¶")
    
    async def extract_code_units(self, source_files: List[SourceFile]) -> List[CodeUnit]:
        """æå–ä»£ç å•å…ƒ - å…¼å®¹æ€§æ–¹æ³•"""
        logger.info(f"Haystack-AIç¼–æ’å™¨ï¼šæå–ä»£ç å•å…ƒï¼Œæ–‡ä»¶æ•°: {len(source_files)}")
        
        try:
            from auditluma.parsers.code_parser import CodeParser
            
            code_units = []
            parser = CodeParser()
            
            for source_file in source_files:
                try:
                    file_units = await parser.parse_file_async(source_file)
                    code_units.extend(file_units)
                except Exception as e:
                    logger.warning(f"è§£ææ–‡ä»¶å¤±è´¥: {source_file.path}, {e}")
            
            self.code_units = code_units
            return code_units
            
        except ImportError:
            logger.warning("ä»£ç è§£æå™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–çš„ä»£ç å•å…ƒæå–")
            return self._simple_extract_code_units(source_files)
    
    def _simple_extract_code_units(self, source_files: List[SourceFile]) -> List[CodeUnit]:
        """ç®€åŒ–çš„ä»£ç å•å…ƒæå–"""
        code_units = []
        
        for source_file in source_files:
            import hashlib
            unit = CodeUnit(
                id=f"file_{hashlib.md5(str(source_file.path).encode()).hexdigest()[:8]}",
                name=source_file.path.name,
                type="file",
                content=source_file.content,
                source_file=source_file,
                start_line=1,
                end_line=len(source_file.content.splitlines())
            )
            code_units.append(unit)
        
        self.code_units = code_units
        return code_units
    
    async def run_security_analysis(self, source_files: List[SourceFile], 
                                   skip_cross_file: bool = False, 
                                   enhanced_analysis: bool = False) -> List[VulnerabilityResult]:
        """è¿è¡Œå®‰å…¨åˆ†æ - å…¼å®¹æ€§æ–¹æ³•ï¼Œä½¿ç”¨Haystack-AIå±‚çº§RAGæ¶æ„"""
        logger.info(f"Haystack-AIç¼–æ’å™¨ï¼šå¼€å§‹å±‚çº§RAGå®‰å…¨åˆ†æï¼Œæ–‡ä»¶æ•°: {len(source_files)}")
        
        try:
            audit_result = await self.orchestrate_audit(source_files)
            logger.info(f"Haystack-AIç¼–æ’å™¨ï¼šå±‚çº§RAGåˆ†æå®Œæˆï¼Œå‘ç°æ¼æ´: {len(audit_result.vulnerabilities)}")
            return audit_result.vulnerabilities
            
        except Exception as e:
            logger.error(f"Haystack-AIç¼–æ’å™¨ï¼šå®‰å…¨åˆ†æå¤±è´¥: {e}")
            return []
    
    async def run_code_structure_analysis(self, code_units: List[CodeUnit]) -> Dict[str, Any]:
        """è¿è¡Œä»£ç ç»“æ„åˆ†æ - å…¼å®¹æ€§æ–¹æ³•"""
        logger.info(f"Haystack-AIç¼–æ’å™¨ï¼šä»£ç ç»“æ„åˆ†æï¼Œä»£ç å•å…ƒæ•°: {len(code_units)}")
        
        structure_info = {
            "total_units": len(code_units),
            "unit_types": {},
            "file_distribution": {},
            "complexity_metrics": {},
            "haystack_enabled": HAYSTACK_AVAILABLE
        }
        
        for unit in code_units:
            unit_type = getattr(unit, 'type', 'unknown')
            structure_info["unit_types"][unit_type] = structure_info["unit_types"].get(unit_type, 0) + 1
            
            file_path = str(unit.source_file.path)
            structure_info["file_distribution"][file_path] = structure_info["file_distribution"].get(file_path, 0) + 1
        
        logger.info(f"Haystack-AIç¼–æ’å™¨ï¼šç»“æ„åˆ†æå®Œæˆï¼Œå•å…ƒç±»å‹: {len(structure_info['unit_types'])}")
        return structure_info
    
    async def generate_remediations(self, vulnerabilities: List[VulnerabilityResult]) -> Dict[str, Any]:
        """ç”Ÿæˆä¿®å¤å»ºè®® - å…¼å®¹æ€§æ–¹æ³•"""
        logger.info(f"Haystack-AIç¼–æ’å™¨ï¼šç”Ÿæˆä¿®å¤å»ºè®®ï¼Œæ¼æ´æ•°: {len(vulnerabilities)}")
        
        if not vulnerabilities:
            return {
                "summary": "æœªå‘ç°éœ€è¦ä¿®å¤çš„æ¼æ´",
                "remediation_count": 0,
                "remediations": []
            }
        
        remediations = []
        
        for vuln in vulnerabilities:
            try:
                remediation_info = await self.txtai_retriever.get_remediation_suggestions(
                    vuln.vulnerability_type, vuln.description
                )
                
                remediation = {
                    "vulnerability_id": vuln.id,
                    "vulnerability_type": vuln.vulnerability_type,
                    "suggestions": remediation_info.get("suggestions", []),
                    "best_practices": remediation_info.get("best_practices", []),
                    "code_examples": remediation_info.get("code_examples", []),
                    "haystack_enhanced": HAYSTACK_AVAILABLE
                }
                remediations.append(remediation)
                
            except Exception as e:
                logger.warning(f"ç”Ÿæˆä¿®å¤å»ºè®®å¤±è´¥: {vuln.id}, {e}")
                remediation = {
                    "vulnerability_id": vuln.id,
                    "vulnerability_type": vuln.vulnerability_type,
                    "suggestions": [f"è¯·ä¿®å¤ {vuln.vulnerability_type} æ¼æ´"],
                    "best_practices": ["éµå¾ªå®‰å…¨ç¼–ç è§„èŒƒ"],
                    "code_examples": [],
                    "haystack_enhanced": False
                }
                remediations.append(remediation)
        
        return {
            "summary": f"ä¸º {len(vulnerabilities)} ä¸ªæ¼æ´ç”Ÿæˆäº†ä¿®å¤å»ºè®®",
            "remediation_count": len(remediations),
            "remediations": remediations,
            "haystack_enabled": HAYSTACK_AVAILABLE
        }
    
    async def run_analysis(self, source_files: List[SourceFile]) -> List[VulnerabilityResult]:
        """è¿è¡Œåˆ†æ - å…¼å®¹æ€§æ–¹æ³•"""
        logger.info(f"Haystack-AIç¼–æ’å™¨ï¼šè¿è¡Œå®Œæ•´åˆ†æï¼Œæ–‡ä»¶æ•°: {len(source_files)}")
        return await self.run_security_analysis(source_files)
    
    async def generate_summary(self, vulnerabilities: List[VulnerabilityResult], 
                             assessment: Dict[str, Any] = None) -> str:
        """ç”Ÿæˆæ‘˜è¦ - å…¼å®¹æ€§æ–¹æ³•"""
        if not vulnerabilities:
            return "æœªå‘ç°å®‰å…¨æ¼æ´ã€‚"
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç±»
        severity_counts = {}
        for vuln in vulnerabilities:
            severity = getattr(vuln, 'severity', 'unknown')
            severity_counts[severity] = severity_counts.get(severity, 0) + 1
        
        # æŒ‰ç±»å‹åˆ†ç±»
        type_counts = {}
        for vuln in vulnerabilities:
            vuln_type = vuln.vulnerability_type
            type_counts[vuln_type] = type_counts.get(vuln_type, 0) + 1
        
        # ç”Ÿæˆæ‘˜è¦
        summary_parts = [
            f"ğŸ” Haystack-AIå±‚çº§RAGå®‰å…¨åˆ†ææ‘˜è¦",
            f"ğŸ“Š å‘ç°æ¼æ´æ€»æ•°: {len(vulnerabilities)}",
            "",
            "ğŸ“ˆ ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ:"
        ]
        
        for severity, count in sorted(severity_counts.items()):
            summary_parts.append(f"  - {severity}: {count}")
        
        summary_parts.extend([
            "",
            "ğŸ·ï¸ æ¼æ´ç±»å‹åˆ†å¸ƒ:"
        ])
        
        for vuln_type, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
            summary_parts.append(f"  - {vuln_type}: {count}")
        
        # æ·»åŠ æ€§èƒ½ä¿¡æ¯
        perf_summary = self.get_performance_summary()
        if "total_tasks_completed" in perf_summary:
            summary_parts.extend([
                "",
                "âš¡ æ€§èƒ½æŒ‡æ ‡:",
                f"  - å®Œæˆä»»åŠ¡æ•°: {perf_summary['total_tasks_completed']}",
                f"  - å¹³å‡æ‰§è¡Œæ—¶é—´: {perf_summary.get('average_execution_time', 0):.2f}ç§’",
                f"  - Haystackç®¡é“: {'å¯ç”¨' if perf_summary.get('haystack_enabled') else 'ç¦ç”¨'}",
                f"  - å¯ç”¨ç®¡é“æ•°: {perf_summary.get('pipelines_available', 0)}"
            ])
        
        return "\n".join(summary_parts)
    
    async def generate_audit_report(self, audit_result: AuditResult) -> str:
        """ç”Ÿæˆè¯¦ç»†çš„å®¡è®¡æŠ¥å‘Š"""
        try:
            integration_result_mock = type('IntegrationResult', (), {
                'integrated_vulnerabilities': audit_result.vulnerabilities,
                'duplicate_count': audit_result.execution_summary.get('duplicate_count', 0),
                'conflict_count': audit_result.execution_summary.get('conflict_count', 0),
                'clusters': [],
                'integration_metadata': audit_result.execution_summary.get('integration_metadata', {}),
                'quality_metrics': audit_result.execution_summary.get('quality_metrics', {}),
                'processing_time': audit_result.processing_time
            })()
            
            report = await self.result_integrator.generate_audit_report(
                integration_result_mock, audit_result.task_results
            )
            
            # æ ¼å¼åŒ–æŠ¥å‘Š
            report_text = f"""
{report.title} (Haystack-AIå¢å¼ºç‰ˆ)
{'=' * (len(report.title) + 15)}

ç”Ÿæˆæ—¶é—´: {report.generated_at}
æŠ¥å‘ŠID: {report.report_id}
å¤„ç†æ—¶é—´: {report.processing_time:.2f}ç§’
Haystack-AI: {'å¯ç”¨' if HAYSTACK_AVAILABLE else 'ç¦ç”¨'}
ç®¡é“æ•°é‡: {len(self.pipelines) if self.pipelines else 0}

{report.summary}

"""
            
            for section in report.sections:
                report_text += f"""
{section.title}
{'-' * len(section.title)}
{section.content}

"""
            
            if report.recommendations:
                report_text += """
å»ºè®®
----
"""
                for i, rec in enumerate(report.recommendations, 1):
                    report_text += f"{i}. {rec}\n"
            
            # æ·»åŠ Haystack-AIç‰¹å®šä¿¡æ¯
            if HAYSTACK_AVAILABLE:
                report_text += f"""

Haystack-AIæŠ€æœ¯ç»†èŠ‚
------------------
â€¢ ä½¿ç”¨çš„ç®¡é“ç±»å‹: {list(self.pipelines.keys()) if self.pipelines else 'æ— '}
â€¢ ç®¡é“æ‰§è¡ŒæˆåŠŸç‡: {audit_result.execution_summary.get('haystack_pipelines_used', 0)}/{audit_result.execution_summary.get('total_tasks', 0)}
â€¢ å±‚çº§RAGæ¶æ„: txtai â†’ R2R â†’ Self-RAG â†’ ç»“æœæ•´åˆ
â€¢ æ™ºèƒ½å†²çªè§£å†³: å…±è¯†ç®—æ³•
â€¢ è‡ªåŠ¨å»é‡: {audit_result.execution_summary.get('duplicate_count', 0)} ä¸ªé‡å¤é¡¹
"""
            
            return report_text
            
        except Exception as e:
            logger.error(f"ç”ŸæˆHaystack-AIå®¡è®¡æŠ¥å‘Šå¤±è´¥: {e}")
            return self.generate_summary(audit_result.vulnerabilities, audit_result.execution_summary)