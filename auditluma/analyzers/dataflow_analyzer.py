"""
æ•°æ®æµåˆ†æå™¨ - è¿½è¸ªæ•°æ®åœ¨å‡½æ•°é—´çš„æµåŠ¨è·¯å¾„å’Œæ±¡ç‚¹åˆ†æ
"""

import ast
import networkx as nx
from typing import Dict, List, Set, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum

from loguru import logger

from auditluma.models.code import SourceFile, CodeUnit
from .global_context_analyzer import CodeEntity


class TaintLevel(Enum):
    """æ±¡ç‚¹çº§åˆ«"""
    SAFE = "safe"           # å®‰å…¨çš„
    SANITIZED = "sanitized" # å·²æ¶ˆæ¯’çš„
    TAINTED = "tainted"     # æœ‰æ±¡ç‚¹çš„
    DANGEROUS = "dangerous" # å±é™©çš„


@dataclass
class TaintSource:
    """æ±¡ç‚¹æº"""
    entity_name: str
    source_type: str  # "user_input", "file_input", "network_input", etc.
    confidence: float = 1.0


@dataclass
class TaintSink:
    """æ±¡ç‚¹æ±‡ç‚¹"""
    entity_name: str
    sink_type: str  # "sql_query", "command_exec", "file_write", etc.
    danger_level: str = "high"


@dataclass
class DataFlowPath:
    """æ•°æ®æµè·¯å¾„"""
    source: TaintSource
    sink: TaintSink
    path: List[str]
    taint_level: TaintLevel
    sanitization_points: List[str]
    risk_score: float


class DataFlowAnalyzer:
    """æ•°æ®æµåˆ†æå™¨ - è¿½è¸ªæ•°æ®æµåŠ¨å’Œæ±¡ç‚¹ä¼ æ’­"""
    
    def __init__(self, global_context: Dict[str, Any]):
        self.global_context = global_context
        self.call_graph = global_context["call_graph"]
        self.entities = global_context["entities"]
        
        # æ±¡ç‚¹åˆ†æç›¸å…³
        self.taint_sources: List[TaintSource] = []
        self.taint_sinks: List[TaintSink] = []
        self.taint_propagation_graph = nx.DiGraph()
        
        # æ•°æ®æµè·¯å¾„ç¼“å­˜
        self.analyzed_paths: Dict[Tuple[str, str], DataFlowPath] = {}
        
        # åˆå§‹åŒ–æ±¡ç‚¹æºå’Œæ±‡ç‚¹
        self._initialize_taint_analysis()
    
    def _initialize_taint_analysis(self):
        """åˆå§‹åŒ–æ±¡ç‚¹åˆ†æ - è¯†åˆ«æ±¡ç‚¹æºå’Œæ±‡ç‚¹"""
        logger.debug("åˆå§‹åŒ–æ±¡ç‚¹åˆ†æ...")
        
        # å®šä¹‰æ±¡ç‚¹æºæ¨¡å¼
        source_patterns = {
            'user_input': [
                r'request\.',
                r'input\s*\(',
                r'raw_input\s*\(',
                r'sys\.argv',
                r'os\.environ',
                r'form\.',
                r'args\.',
                r'json\.',
                r'params\.',
                r'cookies\.',
                r'headers\.'
            ],
            'file_input': [
                r'open\s*\(',
                r'read\s*\(',
                r'readline\s*\(',
                r'readlines\s*\(',
                r'file\s*\(',
                r'csv\.reader',
                r'json\.load'
            ],
            'network_input': [
                r'requests\.',
                r'urllib\.',
                r'socket\.',
                r'http\.',
                r'ftp\.',
                r'urlopen'
            ]
        }
        
        # å®šä¹‰æ±‡ç‚¹æ¨¡å¼
        sink_patterns = {
            'sql_query': [
                r'execute\s*\(',
                r'query\s*\(',
                r'cursor\.',
                r'SELECT\s+.*FROM',
                r'INSERT\s+INTO',
                r'UPDATE\s+.*SET',
                r'DELETE\s+FROM'
            ],
            'command_exec': [
                r'os\.system\s*\(',
                r'subprocess\.',
                r'eval\s*\(',
                r'exec\s*\(',
                r'shell=True',
                r'popen\s*\('
            ],
            'file_write': [
                r'write\s*\(',
                r'writelines\s*\(',
                r'open\s*\(.*["\']w',
                r'open\s*\(.*["\']a'
            ],
            'template_render': [
                r'render\s*\(',
                r'template\.',
                r'jinja',
                r'{% .*%}',
                r'{{ .*}}'
            ],
            'response_output': [
                r'response\.',
                r'HttpResponse',
                r'return.*render',
                r'print\s*\(',
                r'write\s*\('
            ]
        }
        
        # è¯†åˆ«æ±¡ç‚¹æº
        for source_type, patterns in source_patterns.items():
            entities = self._find_entities_with_patterns(patterns)
            for entity_name in entities:
                source = TaintSource(
                    entity_name=entity_name,
                    source_type=source_type,
                    confidence=0.9
                )
                self.taint_sources.append(source)
        
        # è¯†åˆ«æ±‡ç‚¹
        for sink_type, patterns in sink_patterns.items():
            entities = self._find_entities_with_patterns(patterns)
            for entity_name in entities:
                danger_level = "high" if sink_type in ['sql_query', 'command_exec'] else "medium"
                sink = TaintSink(
                    entity_name=entity_name,
                    sink_type=sink_type,
                    danger_level=danger_level
                )
                self.taint_sinks.append(sink)
        
        logger.debug(f"è¯†åˆ«äº† {len(self.taint_sources)} ä¸ªæ±¡ç‚¹æºå’Œ {len(self.taint_sinks)} ä¸ªæ±‡ç‚¹")
    
    def analyze_data_flows(self) -> List[DataFlowPath]:
        """åˆ†ææ•°æ®æµè·¯å¾„"""
        logger.info("ğŸ” å¼€å§‹æ•°æ®æµåˆ†æ...")
        
        dangerous_paths = []
        
        # åˆ†ææ¯ä¸ªæ±¡ç‚¹æºåˆ°æ±‡ç‚¹çš„è·¯å¾„
        for source in self.taint_sources:
            for sink in self.taint_sinks:
                path_key = (source.entity_name, sink.entity_name)
                
                # æ£€æŸ¥ç¼“å­˜
                if path_key in self.analyzed_paths:
                    dangerous_paths.append(self.analyzed_paths[path_key])
                    continue
                
                # æŸ¥æ‰¾è·¯å¾„
                paths = self._find_all_paths(source.entity_name, sink.entity_name)
                
                for path in paths:
                    # åˆ†æè·¯å¾„çš„æ±¡ç‚¹ä¼ æ’­
                    flow_path = self._analyze_path_taint_propagation(source, sink, path)
                    
                    if flow_path and flow_path.risk_score > 0.3:
                        dangerous_paths.append(flow_path)
                        self.analyzed_paths[path_key] = flow_path
        
        # æŒ‰é£é™©è¯„åˆ†æ’åº
        dangerous_paths.sort(key=lambda x: x.risk_score, reverse=True)
        
        logger.info(f"âœ… æ•°æ®æµåˆ†æå®Œæˆï¼Œå‘ç° {len(dangerous_paths)} æ¡å±é™©è·¯å¾„")
        
        return dangerous_paths
    
    def _find_entities_with_patterns(self, patterns: List[str]) -> List[str]:
        """æŸ¥æ‰¾åŒ…å«ç‰¹å®šæ¨¡å¼çš„å®ä½“"""
        import re
        matching_entities = []
        
        for entity_name, entity in self.entities.items():
            if entity.ast_node:
                try:
                    # å°è¯•è·å–ä»£ç å­—ç¬¦ä¸²
                    import astor
                    code_str = astor.to_source(entity.ast_node)
                except ImportError:
                    code_str = str(entity.ast_node)
                
                for pattern in patterns:
                    if re.search(pattern, code_str, re.IGNORECASE):
                        matching_entities.append(entity_name)
                        break
        
        return matching_entities
    
    def _find_all_paths(self, source: str, sink: str, max_length: int = 10) -> List[List[str]]:
        """æŸ¥æ‰¾ä»æºåˆ°æ±‡ç‚¹çš„æ‰€æœ‰è·¯å¾„"""
        try:
            # ä½¿ç”¨è°ƒç”¨å›¾æŸ¥æ‰¾è·¯å¾„
            if nx.has_path(self.call_graph, source, sink):
                all_paths = list(nx.all_simple_paths(
                    self.call_graph, source, sink, cutoff=max_length
                ))
                return all_paths[:5]  # é™åˆ¶è·¯å¾„æ•°é‡
        except nx.NetworkXError:
            pass
        
        return []
    
    def _analyze_path_taint_propagation(self, source: TaintSource, sink: TaintSink, path: List[str]) -> Optional[DataFlowPath]:
        """åˆ†æè·¯å¾„ä¸Šçš„æ±¡ç‚¹ä¼ æ’­"""
        if len(path) < 2:
            return None
        
        # åˆå§‹æ±¡ç‚¹çº§åˆ«åŸºäºæºç±»å‹
        current_taint = TaintLevel.TAINTED
        if source.source_type == "user_input":
            current_taint = TaintLevel.DANGEROUS
        
        sanitization_points = []
        
        # åˆ†æè·¯å¾„ä¸Šæ¯ä¸ªèŠ‚ç‚¹çš„æ±¡ç‚¹ä¼ æ’­
        for i, node in enumerate(path[1:-1], 1):  # è·³è¿‡æºå’Œæ±‡ç‚¹
            node_effect = self._analyze_node_taint_effect(node)
            
            if node_effect == "sanitize":
                current_taint = TaintLevel.SANITIZED
                sanitization_points.append(node)
            elif node_effect == "propagate":
                # æ±¡ç‚¹ç»§ç»­ä¼ æ’­
                pass
            elif node_effect == "amplify":
                if current_taint == TaintLevel.TAINTED:
                    current_taint = TaintLevel.DANGEROUS
        
        # è®¡ç®—é£é™©è¯„åˆ†
        risk_score = self._calculate_risk_score(source, sink, current_taint, path)
        
        return DataFlowPath(
            source=source,
            sink=sink,
            path=path,
            taint_level=current_taint,
            sanitization_points=sanitization_points,
            risk_score=risk_score
        )
    
    def _analyze_node_taint_effect(self, node: str) -> str:
        """åˆ†æèŠ‚ç‚¹å¯¹æ±¡ç‚¹çš„å½±å“"""
        if node not in self.entities:
            return "propagate"
        
        entity = self.entities[node]
        
        if entity.ast_node:
            try:
                import astor
                code_str = astor.to_source(entity.ast_node).lower()
            except ImportError:
                code_str = str(entity.ast_node).lower()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ¶ˆæ¯’æ“ä½œ
            sanitization_keywords = [
                'escape', 'sanitize', 'clean', 'validate', 'filter',
                'quote', 'encode', 'htmlentities', 'htmlspecialchars',
                'strip_tags', 'bleach', 'purify'
            ]
            
            for keyword in sanitization_keywords:
                if keyword in code_str:
                    return "sanitize"
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ”¾å¤§é£é™©çš„æ“ä½œ
            amplify_keywords = [
                'eval', 'exec', 'compile', 'format', 'template'
            ]
            
            for keyword in amplify_keywords:
                if keyword in code_str:
                    return "amplify"
        
        return "propagate"
    
    def _calculate_risk_score(self, source: TaintSource, sink: TaintSink, 
                            taint_level: TaintLevel, path: List[str]) -> float:
        """è®¡ç®—é£é™©è¯„åˆ†"""
        base_score = 0.0
        
        # åŸºäºæ±¡ç‚¹æºç±»å‹çš„åŸºç¡€åˆ†æ•°
        source_scores = {
            'user_input': 0.8,
            'file_input': 0.6,
            'network_input': 0.7
        }
        base_score += source_scores.get(source.source_type, 0.5)
        
        # åŸºäºæ±‡ç‚¹ç±»å‹çš„åˆ†æ•°
        sink_scores = {
            'sql_query': 0.9,
            'command_exec': 1.0,
            'file_write': 0.6,
            'template_render': 0.7,
            'response_output': 0.5
        }
        base_score += sink_scores.get(sink.sink_type, 0.3)
        
        # åŸºäºæ±¡ç‚¹çº§åˆ«çš„è°ƒæ•´
        taint_multipliers = {
            TaintLevel.SAFE: 0.0,
            TaintLevel.SANITIZED: 0.2,
            TaintLevel.TAINTED: 0.7,
            TaintLevel.DANGEROUS: 1.0
        }
        base_score *= taint_multipliers[taint_level]
        
        # åŸºäºè·¯å¾„é•¿åº¦çš„è°ƒæ•´ï¼ˆè·¯å¾„è¶ŠçŸ­é£é™©è¶Šé«˜ï¼‰
        path_factor = max(0.3, 1.0 - (len(path) - 2) * 0.1)
        base_score *= path_factor
        
        # åŸºäºæºçš„å¯ä¿¡åº¦
        base_score *= source.confidence
        
        return min(1.0, base_score)
    
    def get_critical_data_flows(self, min_risk_score: float = 0.7) -> List[DataFlowPath]:
        """è·å–é«˜é£é™©æ•°æ®æµ"""
        all_flows = self.analyze_data_flows()
        return [flow for flow in all_flows if flow.risk_score >= min_risk_score]
    
    def get_sanitization_coverage(self) -> Dict[str, Any]:
        """è·å–æ¶ˆæ¯’è¦†ç›–ç‡ç»Ÿè®¡"""
        all_flows = self.analyze_data_flows()
        
        total_flows = len(all_flows)
        sanitized_flows = len([f for f in all_flows if f.taint_level == TaintLevel.SANITIZED])
        
        coverage_by_source = {}
        for source_type in ['user_input', 'file_input', 'network_input']:
            type_flows = [f for f in all_flows if f.source.source_type == source_type]
            type_sanitized = [f for f in type_flows if f.taint_level == TaintLevel.SANITIZED]
            
            coverage_by_source[source_type] = {
                'total': len(type_flows),
                'sanitized': len(type_sanitized),
                'coverage_rate': len(type_sanitized) / len(type_flows) if type_flows else 0
            }
        
        return {
            'overall_coverage': sanitized_flows / total_flows if total_flows else 0,
            'total_flows': total_flows,
            'sanitized_flows': sanitized_flows,
            'coverage_by_source': coverage_by_source
        }
    
    def visualize_data_flow(self, flow_path: DataFlowPath) -> Dict[str, Any]:
        """å¯è§†åŒ–æ•°æ®æµè·¯å¾„"""
        nodes = []
        edges = []
        
        # åˆ›å»ºèŠ‚ç‚¹
        for i, node in enumerate(flow_path.path):
            node_type = "source" if i == 0 else "sink" if i == len(flow_path.path) - 1 else "intermediate"
            is_sanitization = node in flow_path.sanitization_points
            
            nodes.append({
                'id': node,
                'label': node.split("::")[-1] if "::" in node else node,
                'type': node_type,
                'sanitization': is_sanitization,
                'entity': self.entities.get(node, {})
            })
        
        # åˆ›å»ºè¾¹
        for i in range(len(flow_path.path) - 1):
            edges.append({
                'source': flow_path.path[i],
                'target': flow_path.path[i + 1],
                'type': 'data_flow'
            })
        
        return {
            'nodes': nodes,
            'edges': edges,
            'flow_info': {
                'source_type': flow_path.source.source_type,
                'sink_type': flow_path.sink.sink_type,
                'taint_level': flow_path.taint_level.value,
                'risk_score': flow_path.risk_score,
                'sanitization_points': len(flow_path.sanitization_points)
            }
        } 