"""
å…¨å±€ä¸Šä¸‹æ–‡åˆ†æå™¨ - æ„å»ºé¡¹ç›®çº§åˆ«çš„ä»£ç å…³ç³»å›¾å’Œä¸Šä¸‹æ–‡ä¿¡æ¯
"""

import ast
import hashlib
import networkx as nx
from typing import Dict, List, Set, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum
from pathlib import Path

from loguru import logger

from auditluma.models.code import SourceFile, CodeUnit, FileType
from auditluma.parsers.code_parser import extract_code_units
from auditluma.rag.enhanced_self_rag import enhanced_self_rag as self_rag


class AnalysisLevel(Enum):
    """åˆ†æå±‚æ¬¡æšä¸¾"""
    SYNTAX = "syntax"           # è¯­æ³•çº§åˆ«
    SEMANTIC = "semantic"       # è¯­ä¹‰çº§åˆ«  
    DATAFLOW = "dataflow"       # æ•°æ®æµçº§åˆ«
    CONTROL_FLOW = "control_flow"  # æ§åˆ¶æµçº§åˆ«
    GLOBAL = "global"           # å…¨å±€çº§åˆ«


@dataclass
class CodeEntity:
    """ä»£ç å®ä½“"""
    name: str
    type: str  # function, class, variable, etc.
    file_path: str
    start_line: int
    end_line: int
    ast_node: Optional[ast.AST] = None
    dependencies: Set[str] = None
    
    def __post_init__(self):
        if self.dependencies is None:
            self.dependencies = set()


@dataclass
class DataFlowEdge:
    """æ•°æ®æµè¾¹"""
    source: CodeEntity
    target: CodeEntity
    flow_type: str  # assignment, parameter, return, etc.
    confidence: float = 1.0


@dataclass
class CrossFileFlow:
    """è·¨æ–‡ä»¶æ•°æ®æµ"""
    source_file: str
    source_func: str
    target_file: str
    target_func: str
    flow_type: str  # "call", "import", "data"
    risk_level: str  # "high", "medium", "low"


class GlobalContextAnalyzer:
    """å…¨å±€ä¸Šä¸‹æ–‡åˆ†æå™¨ - æ„å»ºé¡¹ç›®çº§åˆ«çš„ä»£ç å…³ç³»å›¾"""
    
    def __init__(self):
        self.call_graph = nx.DiGraph()
        self.data_flow_graph = nx.DiGraph()
        self.file_dependency_graph = nx.DiGraph()
        self.entities: Dict[str, CodeEntity] = {}
        self.cross_file_flows: List[CrossFileFlow] = []
        self.import_graph = nx.DiGraph()
        self.file_functions: Dict[str, List[str]] = {}
        
    async def build_global_context(self, source_files: List[SourceFile]) -> Dict[str, Any]:
        """æ„å»ºå…¨å±€ä¸Šä¸‹æ–‡
        
        Args:
            source_files: æºæ–‡ä»¶åˆ—è¡¨
            
        Returns:
            å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯å­—å…¸
        """
        logger.info("ğŸ” å¼€å§‹æ„å»ºå…¨å±€ä¸Šä¸‹æ–‡...")
        
        # Self-RAGå¢å¼ºï¼šæ£€ç´¢ç›¸å…³çš„ä»£ç åˆ†æçŸ¥è¯†
        self.use_self_rag = False
        try:
            if hasattr(self_rag, 'retrieve') and hasattr(self_rag, 'embedder') and hasattr(self_rag, 'vector_store'):
                self.use_self_rag = True
                logger.debug("ğŸ¤– å…¨å±€ä¸Šä¸‹æ–‡åˆ†æå™¨å¯ç”¨Self-RAGå¢å¼º")
        except Exception as e:
            logger.debug(f"Self-RAGåˆå§‹åŒ–æ£€æŸ¥å¤±è´¥: {e}")
        
        # 1. è§£ææ‰€æœ‰æ–‡ä»¶ï¼Œæ„å»ºå®ä½“å›¾
        await self._parse_all_files(source_files)
        
        # 2. æ„å»ºè°ƒç”¨å›¾
        self._build_call_graph()
        
        # 3. æ„å»ºæ•°æ®æµå›¾
        self._build_dataflow_graph()
        
        # 4. åˆ†æè·¨æ–‡ä»¶ä¾èµ–
        self._analyze_cross_file_dependencies()
        
        # 5. åˆ†æè·¨æ–‡ä»¶æ•°æ®æµ
        self._analyze_cross_file_flows()
        
        # 6. è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats = self._calculate_statistics()
        
        logger.info(f"âœ… å…¨å±€ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆ")
        logger.info(f"   - ä»£ç å®ä½“: {len(self.entities)}")
        logger.info(f"   - è°ƒç”¨å…³ç³»: {self.call_graph.number_of_edges()}")
        logger.info(f"   - æ•°æ®æµ: {self.data_flow_graph.number_of_edges()}")
        logger.info(f"   - è·¨æ–‡ä»¶æµ: {len(self.cross_file_flows)}")
        
        return {
            "entities": self.entities,
            "call_graph": self.call_graph,
            "data_flow_graph": self.data_flow_graph,
            "file_dependencies": self.file_dependency_graph,
            "cross_file_flows": self.cross_file_flows,
            "import_graph": self.import_graph,
            "statistics": stats
        }
    
    async def _parse_all_files(self, source_files: List[SourceFile]):
        """è§£ææ‰€æœ‰ä»£ç æ–‡ä»¶ï¼Œæå–å®ä½“ä¿¡æ¯"""
        logger.info(f"è§£æ {len(source_files)} ä¸ªæºæ–‡ä»¶...")
        
        for source_file in source_files:
            try:
                # æå–ä»£ç å•å…ƒ
                code_units = await extract_code_units(source_file)
                
                # å¤„ç†æ¯ä¸ªä»£ç å•å…ƒ
                for unit in code_units:
                    entity = self._code_unit_to_entity(unit)
                    self.entities[entity.name] = entity
                    
                    # è®°å½•æ–‡ä»¶ä¸­çš„å‡½æ•°
                    if entity.type == "function":
                        if entity.file_path not in self.file_functions:
                            self.file_functions[entity.file_path] = []
                        self.file_functions[entity.file_path].append(entity.name)
                
                # åˆ†ææ–‡ä»¶çº§åˆ«çš„å¯¼å…¥å…³ç³»
                if source_file.file_type == FileType.PYTHON:
                    self._analyze_python_imports(source_file)
                
            except Exception as e:
                logger.error(f"è§£ææ–‡ä»¶ {source_file.path} æ—¶å‡ºé”™: {e}")
    
    def _code_unit_to_entity(self, unit: CodeUnit) -> CodeEntity:
        """å°†CodeUnitè½¬æ¢ä¸ºCodeEntity"""
        # æ„å»ºå”¯ä¸€çš„å®ä½“åç§°
        entity_name = f"{unit.source_file.path}::{unit.name}"
        
        # å°è¯•è§£æASTèŠ‚ç‚¹ï¼ˆå¦‚æœæ˜¯Pythonä»£ç ï¼‰
        ast_node = None
        if unit.source_file.file_type == FileType.PYTHON:
            try:
                ast_node = ast.parse(unit.content)
            except Exception:
                # å¦‚æœæ— æ³•è§£ææ•´ä¸ªå†…å®¹ï¼Œå°è¯•è§£æä¸ºè¡¨è¾¾å¼
                try:
                    ast_node = ast.parse(unit.content, mode='exec')
                except Exception:
                    pass
        
        return CodeEntity(
            name=entity_name,
            type=unit.type,
            file_path=str(unit.source_file.path),
            start_line=unit.start_line,
            end_line=unit.end_line,
            ast_node=ast_node
        )
    
    def _analyze_python_imports(self, source_file: SourceFile):
        """åˆ†æPythonæ–‡ä»¶çš„å¯¼å…¥å…³ç³»"""
        try:
            tree = ast.parse(source_file.content)
            
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        self.import_graph.add_edge(
                            str(source_file.path), 
                            alias.name,
                            import_type="direct"
                        )
                        
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        self.import_graph.add_edge(
                            str(source_file.path),
                            node.module,
                            import_type="from"
                        )
                        
        except Exception as e:
            logger.debug(f"åˆ†æå¯¼å…¥å…³ç³»æ—¶å‡ºé”™ {source_file.path}: {e}")
    
    def _build_call_graph(self):
        """æ„å»ºå‡½æ•°è°ƒç”¨å›¾"""
        logger.debug("æ„å»ºè°ƒç”¨å›¾...")
        
        for entity_name, entity in self.entities.items():
            if entity.type == "function":
                # å°è¯•ä»åŸå§‹ä»£ç æ–‡ä»¶åˆ†æè°ƒç”¨å…³ç³»
                try:
                    with open(entity.file_path, 'r', encoding='utf-8') as f:
                        file_content = f.read()
                    
                    # è§£ææ•´ä¸ªæ–‡ä»¶çš„AST
                    file_tree = ast.parse(file_content)
                    
                    # æ‰¾åˆ°å½“å‰å‡½æ•°çš„å®šä¹‰
                    for node in ast.walk(file_tree):
                        if isinstance(node, ast.FunctionDef) and node.name == entity.name.split("::")[-1]:
                            # åˆ†æè¿™ä¸ªå‡½æ•°å†…çš„è°ƒç”¨
                            for call_node in ast.walk(node):
                                if isinstance(call_node, ast.Call):
                                    called_function = self._resolve_function_call(call_node, entity.file_path)
                                    if called_function and called_function in self.entities:
                                        self.call_graph.add_edge(
                                            entity_name, 
                                            called_function,
                                            call_type="function_call",
                                            line_number=getattr(call_node, 'lineno', 0)
                                        )
                                        logger.debug(f"æ·»åŠ è°ƒç”¨å…³ç³»: {entity_name} -> {called_function}")
                            break
                            
                except Exception as e:
                    logger.debug(f"åˆ†ææ–‡ä»¶è°ƒç”¨å…³ç³»å¤±è´¥ {entity.file_path}: {e}")
                    # å›é€€åˆ°ASTèŠ‚ç‚¹æ–¹æ³•
                    if entity.ast_node:
                        for node in ast.walk(entity.ast_node):
                            if isinstance(node, ast.Call):
                                called_function = self._resolve_function_call(node, entity.file_path)
                                if called_function and called_function in self.entities:
                                    self.call_graph.add_edge(
                                        entity_name, 
                                        called_function,
                                        call_type="function_call",
                                        line_number=getattr(node, 'lineno', 0)
                                    )
    
    def _build_dataflow_graph(self):
        """æ„å»ºæ•°æ®æµå›¾"""
        logger.debug("æ„å»ºæ•°æ®æµå›¾...")
        
        for entity_name, entity in self.entities.items():
            if entity.type == "function" and entity.ast_node:
                # åˆ†ææ•°æ®æµ
                dataflow_edges = self._analyze_function_dataflow(entity)
                for edge in dataflow_edges:
                    self.data_flow_graph.add_edge(
                        edge.source.name, 
                        edge.target.name,
                        flow_type=edge.flow_type,
                        confidence=edge.confidence
                    )
    
    def _analyze_cross_file_dependencies(self):
        """åˆ†æè·¨æ–‡ä»¶ä¾èµ–å…³ç³»"""
        logger.debug("åˆ†æè·¨æ–‡ä»¶ä¾èµ–...")
        
        # åŸºäºå¯¼å…¥å›¾æ„å»ºæ–‡ä»¶ä¾èµ–å…³ç³»
        for source_file, target_module in self.import_graph.edges():
            # å°è¯•å°†æ¨¡å—åæ˜ å°„åˆ°å®é™…æ–‡ä»¶
            target_file = self._resolve_module_to_file(target_module)
            if target_file:
                self.file_dependency_graph.add_edge(source_file, target_file)
    
    def _analyze_cross_file_flows(self):
        """åˆ†æè·¨æ–‡ä»¶æ•°æ®æµ"""
        logger.debug("åˆ†æè·¨æ–‡ä»¶æ•°æ®æµ...")
        
        # åŸºäºè°ƒç”¨å›¾å’Œæ–‡ä»¶ä¾èµ–æ„å»ºè·¨æ–‡ä»¶æµ
        for source_entity, target_entity in self.call_graph.edges():
            source_file = self.entities[source_entity].file_path
            target_file = self.entities[target_entity].file_path
            
            if source_file != target_file:
                # è¿™æ˜¯ä¸€ä¸ªè·¨æ–‡ä»¶è°ƒç”¨
                flow = CrossFileFlow(
                    source_file=source_file,
                    source_func=self.entities[source_entity].name.split("::")[-1],
                    target_file=target_file,
                    target_func=self.entities[target_entity].name.split("::")[-1],
                    flow_type="function_call",
                    risk_level=self._assess_flow_risk_level(source_entity, target_entity)
                )
                self.cross_file_flows.append(flow)
    
    def _resolve_function_call(self, call_node: ast.Call, current_file: str) -> Optional[str]:
        """è§£æå‡½æ•°è°ƒç”¨ï¼Œè¿”å›ç›®æ ‡å‡½æ•°çš„å®ä½“åç§°"""
        if isinstance(call_node.func, ast.Name):
            # æœ¬åœ°å‡½æ•°è°ƒç”¨
            func_name = call_node.func.id
            
            # é¦–å…ˆæ£€æŸ¥åŒæ–‡ä»¶å†…çš„å‡½æ•°
            local_name = f"{current_file}::{func_name}"
            if local_name in self.entities:
                return local_name
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¯¼å…¥çš„å‡½æ•° - æŸ¥æ‰¾æ‰€æœ‰æ–‡ä»¶ä¸­çš„åŒåå‡½æ•°
            for entity_name in self.entities:
                if entity_name.endswith(f"::{func_name}") and entity_name != local_name:
                    # æ£€æŸ¥æ˜¯å¦æœ‰å¯¼å…¥å…³ç³»
                    imported_module = entity_name.split("::")[0]
                    module_basename = Path(imported_module).stem
                    
                    # æ£€æŸ¥å¯¼å…¥å›¾ä¸­æ˜¯å¦æœ‰è¿™ä¸ªæ¨¡å—çš„å¯¼å…¥
                    for source, target in self.import_graph.edges():
                        if source == current_file and (target == module_basename or target == imported_module):
                            return entity_name
                            
        elif isinstance(call_node.func, ast.Attribute):
            # æ–¹æ³•è°ƒç”¨æˆ–æ¨¡å—å‡½æ•°è°ƒç”¨
            if isinstance(call_node.func.value, ast.Name):
                module_name = call_node.func.value.id
                func_name = call_node.func.attr
                
                # æŸ¥æ‰¾åŒ¹é…çš„å‡½æ•°
                for entity_name in self.entities:
                    if entity_name.endswith(f"::{func_name}"):
                        entity_file = entity_name.split("::")[0]
                        entity_module = Path(entity_file).stem
                        
                        # æ£€æŸ¥æ¨¡å—åæ˜¯å¦åŒ¹é…
                        if entity_module == module_name:
                            return entity_name
                        
                        # æ£€æŸ¥å¯¼å…¥å…³ç³»
                        for source, target in self.import_graph.edges():
                            if source == current_file and target == module_name:
                                # è¿›ä¸€æ­¥æ£€æŸ¥æ–‡ä»¶ååŒ¹é…
                                if entity_module == module_name or entity_file.endswith(f"{module_name}.py"):
                                    return entity_name
                        
        return None
    
    def _analyze_function_dataflow(self, entity: CodeEntity) -> List[DataFlowEdge]:
        """åˆ†æå‡½æ•°å†…æ•°æ®æµ"""
        edges = []
        # ç®€åŒ–ç‰ˆæœ¬ - å®é™…éœ€è¦æ›´ç²¾ç»†çš„æ•°æ®æµåˆ†æ
        # è¿™é‡Œå¯ä»¥æ‰©å±•å®ç°æ›´å¤æ‚çš„æ•°æ®æµè¿½è¸ª
        return edges
    
    def _resolve_module_to_file(self, module_name: str) -> Optional[str]:
        """å°†æ¨¡å—åè§£æä¸ºæ–‡ä»¶è·¯å¾„"""
        # ç®€åŒ–å®ç° - å¯ä»¥æ‰©å±•æ”¯æŒæ›´å¤æ‚çš„æ¨¡å—è§£æ
        for file_path in self.file_functions.keys():
            if module_name in file_path or file_path.endswith(f"{module_name}.py"):
                return file_path
        return None
    
    def _assess_flow_risk_level(self, source_entity: str, target_entity: str) -> str:
        """è¯„ä¼°æµçš„é£é™©çº§åˆ«"""
        source = self.entities[source_entity]
        target = self.entities[target_entity]
        
        # åŸºäºç®€å•è§„åˆ™è¯„ä¼°é£é™©
        risk_keywords = ['auth', 'login', 'password', 'token', 'admin', 'delete', 'execute', 'sql']
        
        source_content = source.name.lower()
        target_content = target.name.lower()
        
        if any(keyword in target_content for keyword in risk_keywords):
            return "high"
        elif any(keyword in source_content for keyword in risk_keywords):
            return "medium"
        else:
            return "low"
    
    def _calculate_statistics(self) -> Dict[str, Any]:
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_entities": len(self.entities),
            "total_files": len(set(entity.file_path for entity in self.entities.values())),
            "entity_types": {
                entity_type: len([e for e in self.entities.values() if e.type == entity_type])
                for entity_type in set(entity.type for entity in self.entities.values())
            },
            "call_relationships": self.call_graph.number_of_edges(),
            "data_flow_relationships": self.data_flow_graph.number_of_edges(),
            "file_dependencies": self.file_dependency_graph.number_of_edges(),
            "cross_file_flows": len(self.cross_file_flows),
            "import_relationships": self.import_graph.number_of_edges()
        }
    
    def get_entity_context(self, entity_name: str) -> Dict[str, Any]:
        """è·å–ç‰¹å®šå®ä½“çš„ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        if entity_name not in self.entities:
            return {}
        
        entity = self.entities[entity_name]
        
        # è·å–ä¾èµ–å’Œè¢«ä¾èµ–å…³ç³»
        dependencies = list(self.call_graph.successors(entity_name))
        dependents = list(self.call_graph.predecessors(entity_name))
        
        # è·å–ç›¸å…³çš„è·¨æ–‡ä»¶æµ
        related_flows = [
            flow for flow in self.cross_file_flows
            if entity.file_path in [flow.source_file, flow.target_file]
        ]
        
        return {
            "entity": entity,
            "dependencies": dependencies,
            "dependents": dependents,
            "related_flows": related_flows,
            "file_imports": list(self.import_graph.successors(entity.file_path)),
            "file_imported_by": list(self.import_graph.predecessors(entity.file_path))
        }
    
    def find_data_flow_paths(self, source_pattern: str, target_pattern: str) -> List[List[str]]:
        """æŸ¥æ‰¾ä»æºåˆ°ç›®æ ‡çš„æ•°æ®æµè·¯å¾„"""
        paths = []
        
        # æŸ¥æ‰¾åŒ¹é…æ¨¡å¼çš„å®ä½“
        source_entities = [
            name for name, entity in self.entities.items()
            if source_pattern.lower() in entity.name.lower()
        ]
        
        target_entities = [
            name for name, entity in self.entities.items()
            if target_pattern.lower() in entity.name.lower()
        ]
        
        # æŸ¥æ‰¾è·¯å¾„
        for source in source_entities:
            for target in target_entities:
                try:
                    if nx.has_path(self.call_graph, source, target):
                        path = nx.shortest_path(self.call_graph, source, target)
                        paths.append(path)
                except nx.NetworkXNoPath:
                    continue
        
        return paths 