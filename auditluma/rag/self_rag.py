"""
Self-RAG (è‡ªæ£€ç´¢å¢å¼ºç”Ÿæˆ) æ¨¡å—å®ç°
æä¾›ä»£ç å®¡è®¡è¿‡ç¨‹ä¸­çš„ä¸Šä¸‹æ–‡æ£€ç´¢å’ŒçŸ¥è¯†å¢å¼ºåŠŸèƒ½
"""

import os
from typing import List, Dict, Any, Optional, Tuple, Union
import json
import numpy as np
from pathlib import Path
import hashlib
from dataclasses import dataclass, field
import asyncio

from loguru import logger
from auditluma.config import Config

# å¯¼å…¥å¯é€‰ä¾èµ–ï¼Œå¦‚æœä¸å¯ç”¨åˆ™æä¾›ä¼˜é›…çš„é€€åŒ–
try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False
    logger.warning("FAISS æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€å•å‘é‡å­˜å‚¨")

try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    logger.warning("Tiktoken æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€å•åˆ†è¯å™¨")

from auditluma.models.code import SourceFile, CodeUnit


@dataclass
class Document:
    """è¡¨ç¤ºçŸ¥è¯†åº“ä¸­çš„æ–‡æ¡£"""
    id: str
    content: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    embedding: Optional[List[float]] = None


class SimpleEmbedder:
    """ä¸€ä¸ªç®€å•çš„åµŒå…¥æ¨¡å‹ï¼Œå½“æ²¡æœ‰æ›´é«˜çº§çš„æ¨¡å‹å¯ç”¨æ—¶ä½¿ç”¨"""
    def __init__(self, dimensions: int = 128):
        self.dimensions = dimensions
    
    def embed(self, text: str) -> List[float]:
        """ä»æ–‡æœ¬ç”Ÿæˆä¸€ä¸ªç®€å•çš„åµŒå…¥å‘é‡"""
        # ä½¿ç”¨æ–‡æœ¬çš„å“ˆå¸Œå€¼ç”Ÿæˆä¼ªéšæœºå‘é‡
        hash_obj = hashlib.md5(text.encode('utf-8'))
        seed = int(hash_obj.hexdigest(), 16) % (2**32)
        np.random.seed(seed)
        
        # å½’ä¸€åŒ–å‘é‡
        embedding = np.random.randn(self.dimensions).astype(np.float32)
        embedding = embedding / np.linalg.norm(embedding)
        
        return embedding.tolist()
    
    async def aembed(self, text: str) -> List[float]:
        """å¼‚æ­¥åµŒå…¥æ–¹æ³•ï¼ˆå®é™…ä¸Šå°±æ˜¯è°ƒç”¨åŒæ­¥æ–¹æ³•ï¼‰"""
        return self.embed(text)


class OpenAIEmbedder:
    """ä½¿ç”¨OpenAI APIçš„åµŒå…¥æ¨¡å‹"""
    def __init__(self, model_name: str = "text-embedding-3-small", provider: str = None):
        # ä»model_nameä¸­è§£ææ¨¡å‹åç§°å’Œæä¾›å•†ï¼ˆå¦‚æœä½¿ç”¨äº†model@provideræ ¼å¼ï¼‰
        parsed_model, parsed_provider = Config.parse_model_spec(model_name)
        
        # å¦‚æœä»model_nameè§£æå‡ºäº†æä¾›å•†ï¼Œä¼˜å…ˆä½¿ç”¨å®ƒ
        if parsed_provider:
            self.model_name = parsed_model
            provider = parsed_provider
            logger.info(f"ä»æ¨¡å‹è§„èŒƒ'{model_name}'ä¸­è§£æå‡ºæ¨¡å‹åç§°'{parsed_model}'å’Œæä¾›å•†'{parsed_provider}'")
        else:
            self.model_name = model_name
        
        # ä½¿ç”¨æŒ‡å®šçš„æä¾›å•†æˆ–é»˜è®¤æä¾›å•†
        provider = provider or Config.agent.default_provider
        provider_config = Config.get_llm_provider_config(provider)
        self.api_key = provider_config.api_key
        self.base_url = provider_config.base_url
        
        # åˆå§‹åŒ–APIå®¢æˆ·ç«¯ï¼Œç¦ç”¨è‡ªåŠ¨æ£€æµ‹åŠŸèƒ½
        import httpx
        from openai import AsyncOpenAI
        
        # åˆ›å»ºå¸¦æœ‰è¶…æ—¶è®¾ç½®çš„httpxå®¢æˆ·ç«¯
        timeout_settings = httpx.Timeout(
            connect=30.0,
            read=60.0,
            write=30.0,
            pool=15.0
        )
        http_client = httpx.AsyncClient(timeout=timeout_settings)
        
        # ç›´æ¥ä½¿ç”¨æŒ‡å®šæä¾›å•†åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼Œä¸é€šè¿‡init_llm_clientå‡½æ•°
        self.client = AsyncOpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
            http_client=http_client,
            max_retries=3
        )
        
        logger.info(f"åˆå§‹åŒ–åµŒå…¥æ¨¡å‹: {self.model_name}, ä½¿ç”¨æä¾›å•†: {provider}")
    
    async def aembed(self, text: str) -> List[float]:
        """å¼‚æ­¥ç”ŸæˆåµŒå…¥å‘é‡"""
        response = await self.client.embeddings.create(
            model=self.model_name,
            input=text
        )
        return response.data[0].embedding


class OllamaEmbedder:
    """ä½¿ç”¨Ollamaæœ¬åœ°APIçš„åµŒå…¥æ¨¡å‹"""
    def __init__(self, model_name: str = "mxbai-embed-large:latest", provider: str = None):
        # ä»model_nameä¸­è§£ææ¨¡å‹åç§°å’Œæä¾›å•†ï¼ˆå¦‚æœä½¿ç”¨äº†model@provideræ ¼å¼ï¼‰
        parsed_model, parsed_provider = Config.parse_model_spec(model_name)
        
        # å¦‚æœä»model_nameè§£æå‡ºäº†æä¾›å•†ï¼Œä¼˜å…ˆä½¿ç”¨å®ƒ
        if parsed_provider:
            self.model_name = parsed_model
            provider = parsed_provider
            logger.info(f"ä»æ¨¡å‹è§„èŒƒ'{model_name}'ä¸­è§£æå‡ºæ¨¡å‹åç§°'{parsed_model}'å’Œæä¾›å•†'{parsed_provider}'")
        else:
            self.model_name = model_name
        
        # ä½¿ç”¨æŒ‡å®šçš„æä¾›å•†æˆ–é»˜è®¤ä½¿ç”¨ollama_emdæä¾›å•†
        provider = provider or "ollama_emd"
        provider_config = Config.get_llm_provider_config(provider)

        # ä»é…ç½®ä¸­è·å–APIç«¯ç‚¹
        self.base_url = provider_config.base_url
        
        # åˆå§‹åŒ–APIå®¢æˆ·ç«¯
        import httpx
        
        # åˆ›å»ºå¸¦æœ‰è¶…æ—¶è®¾ç½®çš„httpxå®¢æˆ·ç«¯
        timeout_settings = httpx.Timeout(
            connect=30.0,
            read=60.0,
            write=30.0,
            pool=15.0
        )
        self.http_client = httpx.AsyncClient(timeout=timeout_settings)
        
        logger.info(f"åˆå§‹åŒ–OllamaåµŒå…¥æ¨¡å‹: {self.model_name}, APIåœ°å€: {self.base_url}")
    
    async def aembed(self, text: str) -> List[float]:
        """å¼‚æ­¥ç”ŸæˆåµŒå…¥å‘é‡ä¸”å¸¦æœ‰é‡è¯•æœºåˆ¶"""
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†æ¨¡æ‹Ÿæ¨¡å¼
        import os
        if os.environ.get("AUDITLUMA_MOCK_LLM", "").lower() in ["true", "1", "yes"]:
            logger.info("æ£€æµ‹åˆ°æ¨¡æ‹Ÿæ¨¡å¼å·²å¯ç”¨ï¼Œç”Ÿæˆæ¨¡æ‹ŸåµŒå…¥å‘é‡")
            # ç”Ÿæˆæ¨¡æ‹ŸåµŒå…¥å‘é‡
            import random
            random.seed(hash(text) % (2**32))  # ä½¿ç”¨æ–‡æœ¬å“ˆå¸Œä½œä¸ºç§å­ï¼Œç¡®ä¿ç›¸åŒæ–‡æœ¬å¾—åˆ°ç›¸åŒå‘é‡
            embedding = [random.uniform(-1, 1) for _ in range(1024)]
            # å½’ä¸€åŒ–å‘é‡
            magnitude = sum(x*x for x in embedding) ** 0.5
            if magnitude > 0:
                embedding = [x/magnitude for x in embedding]
            logger.info(f"ç”Ÿæˆæ¨¡æ‹ŸåµŒå…¥å‘é‡ï¼Œé•¿åº¦: {len(embedding)}")
            return embedding

        # Ollamaçš„embeddings APIæ ¼å¼
        payload = {
            "model": self.model_name,
            "input": text
        }
        
        # å°è¯•ä¸åŒçš„APIæ ¼å¼
        alternative_payloads = [
            # æ ‡å‡†æ ¼å¼
            {
                "model": self.model_name,
                "prompt": text
            },
            # å¤‡é€‰æ ¼å¼1ï¼Œä½¿ç”¨inputè€Œéprompt
            {
                "model": self.model_name,
                "input": text
            },
            # å¤‡é€‰æ ¼å¼2ï¼Œæ·»åŠ é¢å¤–å‚æ•°
            {
                "model": self.model_name,
                "prompt": text,
                "options": {"temperature": 0.0}
            }
        ]
        
        # æœ€å¤§é‡è¯•æ¬¡æ•°
        max_retries = 3
        retry_delay = 2  # åˆå§‹é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
        
        # é¦–å…ˆå°è¯•æ ‡å‡†æ ¼å¼
        current_payload = alternative_payloads[0]
        
        # å®ç°æŒ‡æ•°é€€é¿ï¼ˆexponential backoffï¼‰é‡è¯•æœºåˆ¶
        for attempt in range(max_retries):
            try:
                # å‘é€POSTè¯·æ±‚åˆ°Ollama API
                response = await self.http_client.post(
                    self.base_url,
                    json=current_payload,
                    timeout=30.0  # å¢åŠ è¶…æ—¶æ—¶é—´
                )
                
                # æ£€æŸ¥å“åº”çŠ¶æ€
                response.raise_for_status()
                response_data = response.json()
                
                # ä»å“åº”ä¸­æå–åµŒå…¥å‘é‡
                # Ollama APIè¿”å›çš„æ˜¯ {"embedding": [...]} æ ¼å¼
                if "embedding" in response_data:
                    logger.info(f"æˆåŠŸç”ŸæˆåµŒå…¥å‘é‡ï¼Œé•¿åº¦: {len(response_data['embedding'])}")
                    return response_data["embedding"]
                else:
                    logger.error(f"OllamaåµŒå…¥å“åº”æ ¼å¼é”™è¯¯: {response_data}")
                    raise ValueError("åµŒå…¥å“åº”ä¸­æ²¡æœ‰æ‰¾åˆ°embeddingå­—æ®µ")
                    
            except Exception as e:
                error_str = str(e)
                
                # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•æˆ–è€…å·²ç»å°è¯•äº†æ‰€æœ‰å¯é€‰æ ¼å¼
                if attempt == max_retries - 1:
                    logger.error(f"ç”ŸæˆOllamaåµŒå…¥æ—¶å‡ºé”™ (é‡è¯•{attempt+1}/{max_retries}): {e}")
                    
                    # æ£€æŸ¥é”™è¯¯ç±»å‹å¹¶æä¾›è¯¦ç»†ä¿¡æ¯
                    if "500" in error_str:
                        logger.error("æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: å¯èƒ½æ˜¯æ¨¡å‹ä¸å­˜åœ¨æˆ–é…ç½®é”™è¯¯ (å°è¯• 'ollama pull mxbai-embed-large')")
                    elif "503" in error_str:
                        logger.error("æœåŠ¡ä¸å¯ç”¨é”™è¯¯: è¯·ç¡®ä¿Ollamaæ­£åœ¨è¿è¡Œä¸”å·²åŠ è½½åµŒå…¥æ¨¡å‹")

                    # å¦‚æœè¿˜æœ‰å…¶ä»–å¯é€‰æ ¼å¼å¯ä»¥å°è¯•
                    payload_index = alternative_payloads.index(current_payload) + 1
                    if payload_index < len(alternative_payloads):
                        current_payload = alternative_payloads[payload_index]
                        logger.info(f"å°è¯•å¤‡é€‰APIæ ¼å¼ {payload_index}")
                        # é‡ç½®å°è¯•è®¡æ•°å™¨
                        attempt = 0
                        continue
                    else:
                        # æ‰€æœ‰æ ¼å¼å‡å·²å°è¯•å¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼ï¼ˆé»˜è®¤ä¸º falseï¼‰
                        import os
                        mock_mode = os.environ.get("AUDITLUMA_MOCK_LLM", "false").lower()
                        if mock_mode in ["true", "1", "yes"]:
                            logger.info("OllamaåµŒå…¥APIå¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹ŸåµŒå…¥å‘é‡")
                            # ç”Ÿæˆæ¨¡æ‹ŸåµŒå…¥å‘é‡
                            import random
                            random.seed(hash(text) % (2**32))  # ä½¿ç”¨æ–‡æœ¬å“ˆå¸Œä½œä¸ºç§å­ï¼Œç¡®ä¿ç›¸åŒæ–‡æœ¬å¾—åˆ°ç›¸åŒå‘é‡
                            embedding = [random.uniform(-1, 1) for _ in range(1024)]
                            # å½’ä¸€åŒ–å‘é‡
                            magnitude = sum(x*x for x in embedding) ** 0.5
                            if magnitude > 0:
                                embedding = [x/magnitude for x in embedding]
                            logger.info(f"ç”Ÿæˆæ¨¡æ‹ŸåµŒå…¥å‘é‡ï¼Œé•¿åº¦: {len(embedding)}")
                            return embedding
                        else:
                            # æ‰€æœ‰æ ¼å¼å‡å·²å°è¯•å¤±è´¥
                            raise
                
                # å¦åˆ™è®°å½•é”™è¯¯å¹¶å‡†å¤‡é‡è¯•
                logger.warning(f"ç”ŸæˆOllamaåµŒå…¥å¤±è´¥ (é‡è¯•{attempt+1}/{max_retries}): {e}")
                
                # ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•ï¼Œå¹¶ä¸”ä½¿ç”¨æŒ‡æ•°é€€é¿ç®—æ³•å¢åŠ é‡è¯•é—´éš”
                import asyncio
                await asyncio.sleep(retry_delay * (2 ** attempt))  # 2, 4, 8ç§’
                
                # æ ¹æ®é”™è¯¯ç±»å‹è°ƒæ•´ç­‰å¾…æ—¶é—´
                if "503" in error_str:
                    logger.info(f"æ£€æµ‹åˆ°503é”™è¯¯ï¼Œå¯èƒ½æ˜¯æ¨¡å‹æ­£åœ¨åŠ è½½ï¼Œç­‰å¾…é¢å¤–æ—¶é—´...")
                    await asyncio.sleep(5)  # é¢å¤–ç­‰å¾…5ç§’
                elif "500" in error_str:
                    logger.info(f"æ£€æµ‹åˆ°500é”™è¯¯ï¼Œå¯èƒ½æ˜¯è¯·æ±‚æ ¼å¼é”™è¯¯ï¼Œå°è¯•ä¸åŒæ ¼å¼...")
                    # å¦‚æœè¿ç»­å‡ºç°500é”™è¯¯ï¼Œå°è¯•åˆ‡æ¢è¯·æ±‚æ ¼å¼
                    payload_index = alternative_payloads.index(current_payload) + 1
                    if payload_index < len(alternative_payloads):
                        current_payload = alternative_payloads[payload_index]
                        logger.info(f"åˆ‡æ¢åˆ°å¤‡é€‰APIæ ¼å¼ {payload_index}")
                        # ä¸é‡ç½®å°è¯•è®¡æ•°å™¨ï¼Œç»§ç»­æ­£å¸¸é‡è¯•æµç¨‹


class SimpleVectorStore:
    """ä¸€ä¸ªç®€å•çš„å‘é‡å­˜å‚¨ï¼Œå½“FAISSä¸å¯ç”¨æ—¶ä½¿ç”¨"""
    def __init__(self):
        self.documents: List[Document] = []
    
    def add(self, documents: List[Document]) -> None:
        """æ·»åŠ æ–‡æ¡£åˆ°å­˜å‚¨"""
        self.documents.extend(documents)
    
    def search(self, query_embedding: List[float], k: int = 5) -> List[Tuple[Document, float]]:
        """æœç´¢æœ€ç›¸ä¼¼çš„æ–‡æ¡£"""
        if not self.documents:
            return []
        
        query_embedding_np = np.array(query_embedding, dtype=np.float32)
        
        results = []
        for doc in self.documents:
            if doc.embedding:
                doc_embedding_np = np.array(doc.embedding, dtype=np.float32)
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                similarity = np.dot(query_embedding_np, doc_embedding_np) / (
                    np.linalg.norm(query_embedding_np) * np.linalg.norm(doc_embedding_np)
                )
                results.append((doc, float(similarity)))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:k]
    
    def save(self, path: str) -> None:
        """ä¿å­˜å‘é‡å­˜å‚¨åˆ°æ–‡ä»¶"""
        data = {
            "documents": [
                {
                    "id": doc.id,
                    "content": doc.content,
                    "metadata": doc.metadata,
                    "embedding": doc.embedding
                }
                for doc in self.documents
            ]
        }
        
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def load(self, path: str) -> None:
        """ä»æ–‡ä»¶åŠ è½½å‘é‡å­˜å‚¨"""
        if not os.path.exists(path):
            logger.warning(f"å‘é‡å­˜å‚¨æ–‡ä»¶ä¸å­˜åœ¨: {path}")
            return
        
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        self.documents = [
            Document(
                id=doc["id"],
                content=doc["content"],
                metadata=doc["metadata"],
                embedding=doc["embedding"]
            )
            for doc in data["documents"]
        ]
        
        logger.info(f"å·²åŠ è½½ {len(self.documents)} ä¸ªæ–‡æ¡£åˆ°å‘é‡å­˜å‚¨")


class FAISSVectorStore:
    """ä½¿ç”¨FAISSçš„é«˜æ€§èƒ½å‘é‡å­˜å‚¨"""
    def __init__(self, dimensions: int = 1536):
        if not FAISS_AVAILABLE:
            raise ImportError("FAISS æœªå®‰è£…ï¼Œè¯·å®‰è£… faiss-cpu æˆ– faiss-gpu")
        
        self.dimensions = dimensions
        self.index = faiss.IndexFlatL2(dimensions)
        self.documents: List[Document] = []
        self.doc_ids_map: Dict[int, str] = {}  # æ˜ å°„FAISSç´¢å¼•åˆ°æ–‡æ¡£ID
    
    def add(self, documents: List[Document]) -> None:
        """æ·»åŠ æ–‡æ¡£åˆ°FAISSç´¢å¼•"""
        if not documents:
            return
        
        vectors = []
        for i, doc in enumerate(documents):
            if doc.embedding:
                vectors.append(np.array(doc.embedding, dtype=np.float32))
                self.doc_ids_map[len(self.documents) + i] = doc.id
        
        if vectors:
            vectors_np = np.vstack(vectors).astype(np.float32)
            self.index.add(vectors_np)
            self.documents.extend(documents)
            logger.debug(f"æ·»åŠ äº† {len(vectors)} ä¸ªå‘é‡åˆ°FAISSç´¢å¼•")
    
    def search(self, query_embedding: List[float], k: int = 5) -> List[Tuple[Document, float]]:
        """ä½¿ç”¨FAISSæœç´¢æœ€ç›¸ä¼¼çš„æ–‡æ¡£"""
        if self.index.ntotal == 0:
            return []
        
        query_embedding_np = np.array([query_embedding], dtype=np.float32)
        distances, indices = self.index.search(query_embedding_np, k)
        
        results = []
        for i, idx in enumerate(indices[0]):
            if idx != -1 and idx < len(self.documents):
                doc = self.documents[idx]
                # è½¬æ¢L2è·ç¦»ä¸ºç›¸ä¼¼åº¦åˆ†æ•° (è¶Šå°è¶Šç›¸ä¼¼ï¼Œæ‰€ä»¥ç”¨1å‡å»å½’ä¸€åŒ–å€¼)
                similarity = 1.0 - (distances[0][i] / 100.0)  # ç®€å•å½’ä¸€åŒ–
                # ç¡®ä¿åœ¨0-1èŒƒå›´å†…
                similarity = max(0.0, min(1.0, similarity))
                results.append((doc, float(similarity)))
        
        return results
    
    def save(self, path: str) -> None:
        """ä¿å­˜FAISSç´¢å¼•å’Œæ–‡æ¡£åˆ°æ–‡ä»¶"""
        # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        # ä¿å­˜FAISSç´¢å¼•
        index_path = f"{path}.index"
        faiss.write_index(self.index, index_path)
        
        # ä¿å­˜æ–‡æ¡£å’Œæ˜ å°„
        data = {
            "documents": [
                {
                    "id": doc.id,
                    "content": doc.content,
                    "metadata": doc.metadata,
                    # ä¸ä¿å­˜åµŒå…¥ï¼Œå› ä¸ºå®ƒä»¬å·²ç»åœ¨FAISSç´¢å¼•ä¸­
                }
                for doc in self.documents
            ],
            "doc_ids_map": self.doc_ids_map
        }
        
        with open(f"{path}.json", "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"å·²ä¿å­˜FAISSç´¢å¼•å’Œ {len(self.documents)} ä¸ªæ–‡æ¡£åˆ° {path}")
    
    def load(self, path: str) -> None:
        """ä»æ–‡ä»¶åŠ è½½FAISSç´¢å¼•å’Œæ–‡æ¡£"""
        index_path = f"{path}.index"
        data_path = f"{path}.json"
        
        if not os.path.exists(index_path) or not os.path.exists(data_path):
            logger.warning(f"FAISSç´¢å¼•æˆ–æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {path}")
            return
        
        # åŠ è½½FAISSç´¢å¼•
        self.index = faiss.read_index(index_path)
        
        # åŠ è½½æ–‡æ¡£å’Œæ˜ å°„
        with open(data_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        self.documents = [
            Document(
                id=doc["id"],
                content=doc["content"],
                metadata=doc["metadata"],
                embedding=None  # åµŒå…¥å­˜å‚¨åœ¨FAISSç´¢å¼•ä¸­
            )
            for doc in data["documents"]
        ]
        
        self.doc_ids_map = {int(k): v for k, v in data["doc_ids_map"].items()}
        
        logger.info(f"å·²åŠ è½½FAISSç´¢å¼•å’Œ {len(self.documents)} ä¸ªæ–‡æ¡£")


class SelfRAG:
    """Self-RAG å®ç°ï¼Œæä¾›è‡ªæ£€ç´¢å¢å¼ºç”ŸæˆåŠŸèƒ½"""
    def __init__(self):
        self.config = Config.self_rag
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        embedding_model = self.config.embedding_model
        
        try:
            # è§£æåµŒå…¥æ¨¡å‹åç§°å’Œæä¾›å•†
            model_name, provider = Config.parse_model_spec(embedding_model)
            
            # æ ¹æ®æä¾›å•†ç±»å‹é€‰æ‹©åˆé€‚çš„åµŒå…¥æ¨¡å‹
            if provider == "ollama_emd":
                # å°è¯•ä½¿ç”¨OllamaåµŒå…¥æ¨¡å‹ï¼Œä½†ä¹Ÿå‡†å¤‡äº†ç®€å•åµŒå…¥å™¨ä½œä¸ºåå¤‡
                self.embedder = OllamaEmbedder(model_name=model_name, provider=provider)
                # åˆ›å»ºåå¤‡åµŒå…¥å™¨
                self.fallback_embedder = SimpleEmbedder()
                logger.info(f"ğŸ“š ä¼ ç»ŸSelf-RAGç³»ç»Ÿ - ä½¿ç”¨OllamaåµŒå…¥æ¨¡å‹: {model_name} (å·²å‡†å¤‡åå¤‡åµŒå…¥å™¨)")
            else:
                # é»˜è®¤ä½¿ç”¨OpenAIå…¼å®¹çš„åµŒå…¥æ¨¡å‹
                self.embedder = OpenAIEmbedder(model_name=embedding_model)
                # ä»é…ç½®ä¸­è·å–æ¨¡å‹åç§°ï¼Œå¯èƒ½å·²åœ¨OpenAIEmbedderä¸­é€šè¿‡parse_model_specè§£æ
                model_name = self.embedder.model_name
                logger.info(f"ğŸ“š ä¼ ç»ŸSelf-RAGç³»ç»Ÿ - ä½¿ç”¨åµŒå…¥æ¨¡å‹: {model_name}")
        except Exception as e:
            logger.warning(f"æ— æ³•åˆå§‹åŒ–åµŒå…¥æ¨¡å‹: {e}")
            self.embedder = SimpleEmbedder()
            logger.info("å›é€€åˆ°ç®€å•åµŒå…¥æ¨¡å‹")
        
        # ç¡®å®šåµŒå…¥ç»´åº¦ - Ollama mxbai-embed-large ä½¿ç”¨1024ç»´ï¼ŒOpenAIé€šå¸¸æ˜¯1536ç»´
        embedding_dimensions = 1024 if provider == "ollama_emd" else 1536
        
        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
        if self.config.vector_store == "faiss" and FAISS_AVAILABLE:
            self.vector_store = FAISSVectorStore(dimensions=embedding_dimensions)
            logger.info(f"ä½¿ç”¨FAISSå‘é‡å­˜å‚¨ (ç»´åº¦: {embedding_dimensions})")
        else:
            self.vector_store = SimpleVectorStore()
            logger.info("ä½¿ç”¨ç®€å•å‘é‡å­˜å‚¨")
        
        # åŠ è½½æ•°æ®ç›®å½•
        data_dir = Path("./data/kb")
        os.makedirs(data_dir, exist_ok=True)
        
        # è·Ÿè¸ªå·²å¤„ç†çš„æ–‡ä»¶
        self.processed_files = set()
    
    async def add_source_file(self, file: SourceFile) -> None:
        """å°†æºæ–‡ä»¶æ·»åŠ åˆ°çŸ¥è¯†åº“"""
        if file.id in self.processed_files:
            logger.debug(f"æ–‡ä»¶å·²å­˜åœ¨äºçŸ¥è¯†åº“ä¸­: {file.id}")
            return
        
        # å°†æ–‡ä»¶åˆ†å—
        chunks = self._chunk_text(file.content, self.config.chunk_size, self.config.chunk_overlap)
        
        # åˆ›å»ºæ–‡æ¡£
        documents = []
        for i, chunk in enumerate(chunks):
            doc_id = f"{file.id}_{i}"
            doc = Document(
                id=doc_id,
                content=chunk,
                metadata={
                    "file_id": file.id,
                    "file_path": str(file.path),
                    "file_name": file.name,
                    "chunk_index": i,
                    "file_type": file.file_type,
                    "chunk_start": i * (self.config.chunk_size - self.config.chunk_overlap)
                }
            )
            documents.append(doc)
        
        # ä½¿ç”¨å¹¶å‘ç”ŸæˆåµŒå…¥
        async def generate_embedding(doc):
            try:
                # å°è¯•ä½¿ç”¨ä¸»è¦åµŒå…¥å™¨
                doc.embedding = await self.embedder.aembed(doc.content)
                return doc
            except Exception as e:
                logger.error(f"ä¸ºæ–‡æ¡£ç”ŸæˆåµŒå…¥æ—¶å‡ºé”™: {doc.id}, {e}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰åå¤‡åµŒå…¥å™¨å¯ç”¨
                if hasattr(self, "fallback_embedder") and self.fallback_embedder is not None:
                    try:
                        logger.warning(f"å°è¯•ä½¿ç”¨åå¤‡åµŒå…¥å™¨ç”ŸæˆåµŒå…¥: {doc.id}")
                        doc.embedding = await self.fallback_embedder.aembed(doc.content)
                        return doc
                    except Exception as fallback_error:
                        logger.error(f"åå¤‡åµŒå…¥å™¨ä¹Ÿå¤±è´¥: {fallback_error}")
                
                # å¦‚æœåå¤‡ä¹Ÿå¤±è´¥æˆ–æ²¡æœ‰åå¤‡ï¼Œè¿”å›None
                return None

        # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°é‡ï¼Œé¿å…è¿‡å¤šå¹¶å‘è¯·æ±‚
        semaphore = asyncio.Semaphore(10)  # æ§åˆ¶æœ€å¤§å¹¶å‘æ•°
        
        async def bounded_generate_embedding(doc):
            async with semaphore:
                return await generate_embedding(doc)
        
        # å¹¶å‘ç”Ÿæˆæ‰€æœ‰æ–‡æ¡£çš„åµŒå…¥
        embedding_tasks = [bounded_generate_embedding(doc) for doc in documents]
        embedded_docs = await asyncio.gather(*embedding_tasks)
        
        # è¿‡æ»¤å‡ºæˆåŠŸç”ŸæˆåµŒå…¥çš„æ–‡æ¡£
        valid_docs = [doc for doc in embedded_docs if doc and doc.embedding]
        
        # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
        if valid_docs:
            self.vector_store.add(valid_docs)
            self.processed_files.add(file.id)
            logger.info(f"å·²å°†æ–‡ä»¶æ·»åŠ åˆ°çŸ¥è¯†åº“: {file.id} ({len(valid_docs)}/{len(chunks)}ä¸ªå—)")
        else:
            logger.warning(f"æ–‡ä»¶ {file.id} æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„åµŒå…¥ï¼Œå°†ä½œä¸ºæ— åµŒå…¥æ–‡ä»¶è®°å½•")
            self.register_file_without_embedding(file)
    
    def register_file_without_embedding(self, file: SourceFile) -> None:
        """ä»…è®°å½•æ–‡ä»¶ä½†ä¸ç”ŸæˆåµŒå…¥
        
        åœ¨åµŒå…¥APIè°ƒç”¨å¤±è´¥æˆ–è¶…æ—¶çš„æƒ…å†µä¸‹ä½¿ç”¨ï¼Œå…è®¸ç³»ç»Ÿç»§ç»­åˆ†æè€Œä¸é˜»å¡
        
        Args:
            file: è¦è®°å½•çš„æºæ–‡ä»¶
        """
        if file.id in self.processed_files:
            return
            
        # åªå°†æ–‡ä»¶IDæ·»åŠ åˆ°å·²å¤„ç†æ–‡ä»¶é›†åˆï¼Œä¸åˆ›å»ºåµŒå…¥
        self.processed_files.add(file.id)
        logger.info(f"å·²è®°å½•æ–‡ä»¶(æ— åµŒå…¥): {file.id}")
    
    async def add_code_unit(self, unit: CodeUnit) -> None:
        """å°†ä»£ç å•å…ƒæ·»åŠ åˆ°çŸ¥è¯†åº“"""
        if unit.id in self.processed_files:
            return
        
        # åˆ›å»ºæ–‡æ¡£
        doc = Document(
            id=unit.id,
            content=unit.content,
            metadata={
                "unit_id": unit.id,
                "unit_name": unit.name,
                "unit_type": unit.type,
                "file_id": unit.source_file.id,
                "file_path": str(unit.source_file.path),
                "start_line": unit.start_line,
                "end_line": unit.end_line
            }
        )
        
        # ç”ŸæˆåµŒå…¥ï¼ˆåŠ å…¥é”™è¯¯å¤„ç†å’Œåå¤‡æœºåˆ¶ï¼‰
        try:
            # å°è¯•ä½¿ç”¨ä¸»è¦åµŒå…¥å™¨
            doc.embedding = await self.embedder.aembed(doc.content)
        except Exception as e:
            logger.error(f"ç”ŸæˆåµŒå…¥æ—¶å‡ºé”™: {unit.id}, {e}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰åå¤‡åµŒå…¥å™¨å¯ç”¨
            if hasattr(self, "fallback_embedder") and self.fallback_embedder is not None:
                try:
                    logger.warning(f"å°è¯•ä½¿ç”¨åå¤‡åµŒå…¥å™¨: {unit.id}")
                    doc.embedding = await self.fallback_embedder.aembed(doc.content)
                except Exception as fallback_error:
                    logger.error(f"åå¤‡åµŒå…¥å™¨ä¹Ÿå¤±è´¥: {fallback_error}")
                    # å¦‚æœåå¤‡ä¹Ÿå¤±è´¥ï¼Œä½¿ç”¨ç©ºå‘é‡æˆ–æŠ›å‡ºå¼‚å¸¸
                    raise
        
        # ç¡®ä¿åµŒå…¥å‘é‡å­˜åœ¨å†æ·»åŠ åˆ°å‘é‡å­˜å‚¨
        if doc.embedding is not None:
            # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
            try:
                self.vector_store.add([doc])
                self.processed_files.add(unit.id)
                logger.debug(f"æˆåŠŸæ·»åŠ ä»£ç å•å…ƒåˆ°çŸ¥è¯†åº“: {unit.id}")
            except Exception as store_error:
                logger.error(f"æ·»åŠ åˆ°å‘é‡å­˜å‚¨æ—¶å‡ºé”™: {unit.id}, {store_error}")
                raise
        else:
            # å¦‚æœæ²¡æœ‰åµŒå…¥å‘é‡ï¼Œä»ç„¶è®°å½•è¯¥æ–‡ä»¶ä¸ºå·²å¤„ç†
            self.processed_files.add(unit.id)
            logger.warning(f"æ·»åŠ äº†æ²¡æœ‰åµŒå…¥çš„ä»£ç å•å…ƒ: {unit.id}")
        
        logger.debug(f"å·²å°†ä»£ç å•å…ƒæ·»åŠ åˆ°çŸ¥è¯†åº“: {unit.id}")
    
    async def add_batch_code_units(self, units: List[CodeUnit], max_concurrency: int = 20) -> None:
        """æ‰¹é‡å°†ä»£ç å•å…ƒæ·»åŠ åˆ°çŸ¥è¯†åº“
        
        Args:
            units: è¦æ·»åŠ çš„ä»£ç å•å…ƒåˆ—è¡¨
            max_concurrency: æœ€å¤§å¹¶å‘æ•°
        """
        if not units:
            return
            
        # è¿‡æ»¤æ‰å·²å¤„ç†çš„å•å…ƒ
        units_to_process = [unit for unit in units if unit.id not in self.processed_files]
        
        if not units_to_process:
            logger.debug(f"æ²¡æœ‰æ–°çš„ä»£ç å•å…ƒéœ€è¦æ·»åŠ åˆ°çŸ¥è¯†åº“")
            return
            
        logger.info(f"æ‰¹é‡æ·»åŠ  {len(units_to_process)} ä¸ªä»£ç å•å…ƒåˆ°çŸ¥è¯†åº“")
        
        # é™åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(max_concurrency)
        
        async def process_unit(unit):
            async with semaphore:
                try:
                    await self.add_code_unit(unit)
                    return True
                except Exception as e:
                    logger.error(f"æ·»åŠ ä»£ç å•å…ƒåˆ°çŸ¥è¯†åº“æ—¶å‡ºé”™: {unit.id}, {e}")
                    return False
        
        # å¹¶å‘å¤„ç†æ‰€æœ‰å•å…ƒï¼Œä½†å…è®¸å¤±è´¥è€Œä¸ä¸­æ–­æ•´ä¸ªæ‰¹å¤„ç†
        tasks = [process_unit(unit) for unit in units_to_process]
        results = await asyncio.gather(*tasks, return_exceptions=False)
        
        # è®¡ç®—æˆåŠŸçš„å•å…ƒæ•°é‡ï¼ˆè¿”å› True çš„ç»“æœæ•°ï¼‰
        success_count = sum(1 for r in results if r is True)
        logger.info(f"æˆåŠŸæ·»åŠ  {success_count}/{len(units_to_process)} ä¸ªä»£ç å•å…ƒåˆ°çŸ¥è¯†åº“")
        
        # å³ä½¿æœ‰é”™è¯¯ä¹Ÿç»§ç»­å¤„ç†ï¼Œè€Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
    
    async def retrieve(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:
        """æ£€ç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„æ–‡æ¡£"""
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = await self.embedder.aembed(query)
        
        # æœç´¢ç›¸å…³æ–‡æ¡£
        results = self.vector_store.search(query_embedding, k=k)
        
        # è¿‡æ»¤ä½ç›¸ä¼¼åº¦ç»“æœ
        filtered_results = [(doc, score) for doc, score in results if score >= self.config.relevance_threshold]
        
        logger.debug(f"æ£€ç´¢åˆ° {len(filtered_results)}/{len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
        return filtered_results
    
    def save_knowledge_base(self, path: str = "./data/kb/knowledge_base") -> None:
        """ä¿å­˜çŸ¥è¯†åº“åˆ°æ–‡ä»¶"""
        self.vector_store.save(path)
        
        # ä¿å­˜å¤„ç†è¿‡çš„æ–‡ä»¶åˆ—è¡¨
        with open(f"{path}_processed.json", "w", encoding="utf-8") as f:
            json.dump(list(self.processed_files), f, ensure_ascii=False, indent=2)
        
        logger.info(f"å·²ä¿å­˜çŸ¥è¯†åº“åˆ° {path}")
    
    def load_knowledge_base(self, path: str = "./data/kb/knowledge_base") -> None:
        """ä»æ–‡ä»¶åŠ è½½çŸ¥è¯†åº“"""
        self.vector_store.load(path)
        
        # åŠ è½½å¤„ç†è¿‡çš„æ–‡ä»¶åˆ—è¡¨
        processed_path = f"{path}_processed.json"
        if os.path.exists(processed_path):
            with open(processed_path, "r", encoding="utf-8") as f:
                self.processed_files = set(json.load(f))
        
        logger.info(f"å·²åŠ è½½çŸ¥è¯†åº“ï¼Œ{len(self.processed_files)}ä¸ªå¤„ç†è¿‡çš„æ–‡ä»¶")
    
    def _chunk_text(self, text: str, chunk_size: int, chunk_overlap: int) -> List[str]:
        """å°†æ–‡æœ¬åˆ†å‰²æˆé‡å çš„å—"""
        if not text:
            return []
        
        # å¦‚æœæœ‰tiktokenï¼Œä½¿ç”¨å®ƒè¿›è¡Œåˆ†è¯
        if TIKTOKEN_AVAILABLE:
            tokenizer = tiktoken.get_encoding("cl100k_base")
            tokens = tokenizer.encode(text)
            chunks = []
            
            i = 0
            while i < len(tokens):
                # è·å–å½“å‰å—çš„ç»“æŸä½ç½®
                end = min(i + chunk_size, len(tokens))
                # è§£ç å½“å‰å—
                chunk = tokenizer.decode(tokens[i:end])
                chunks.append(chunk)
                # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªèµ·å§‹ä½ç½®ï¼Œè€ƒè™‘é‡å 
                i += (chunk_size - chunk_overlap)
            
            return chunks
        
        # ç®€å•æŒ‰å­—ç¬¦åˆ†å—
        chunks = []
        for i in range(0, len(text), chunk_size - chunk_overlap):
            chunks.append(text[i:i + chunk_size])
            if i + chunk_size >= len(text):
                break
        
        return chunks


# å…¨å±€Self-RAGå®ä¾‹
self_rag = SelfRAG()
