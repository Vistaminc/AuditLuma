"""
R2Rä»£ç ä¸Šä¸‹æ–‡å¢å¼ºå±‚ - å±‚çº§RAGæ¶æ„ç¬¬ä¸‰å±‚
è´Ÿè´£æ·±åº¦ä»£ç ç†è§£ä¸å…³ç³»åˆ†æ
"""

import asyncio
import networkx as nx
import time
from typing import List, Dict, Any, Optional, Set, Tuple
from dataclasses import dataclass, field
from collections import defaultdict, deque
import ast
import re
from pathlib import Path

from loguru import logger

from auditluma.config import Config
from auditluma.models.code import SourceFile, CodeUnit, VulnerabilityResult


@dataclass
class CallRelation:
    """è°ƒç”¨å…³ç³»"""
    caller: str
    callee: str
    call_type: str  # "direct", "indirect", "dynamic"
    confidence: float
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class DataFlow:
    """æ•°æ®æµä¿¡æ¯"""
    source: str
    sink: str
    flow_path: List[str]
    data_type: str
    taint_level: str  # "high", "medium", "low"
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ImpactScope:
    """å½±å“èŒƒå›´"""
    affected_functions: Set[str]
    affected_files: Set[str]
    propagation_depth: int
    risk_level: str  # "critical", "high", "medium", "low"
    propagation_paths: List[List[str]]


@dataclass
class EnhancedContext:
    """å¢å¼ºçš„ä»£ç ä¸Šä¸‹æ–‡"""
    call_chain: List[CallRelation]
    data_flow: List[DataFlow]
    impact_scope: ImpactScope
    semantic_context: Dict[str, Any]
    context_completeness: float  # 0.0 - 1.0


class CallGraphBuilder:
    """è°ƒç”¨å›¾æ„å»ºå™¨"""
    
    def __init__(self):
        self.call_graph = nx.DiGraph()
        self.function_definitions = {}  # function_name -> CodeUnit
        self.call_patterns = self._init_call_patterns()
    
    def _init_call_patterns(self) -> Dict[str, List[str]]:
        """åˆå§‹åŒ–è°ƒç”¨æ¨¡å¼"""
        return {
            "python": [
                r'(\w+)\s*\(',  # å‡½æ•°è°ƒç”¨
                r'(\w+)\.(\w+)\s*\(',  # æ–¹æ³•è°ƒç”¨
                r'(\w+)\.\w+\.\w+\s*\(',  # é“¾å¼è°ƒç”¨
            ],
            "javascript": [
                r'(\w+)\s*\(',
                r'(\w+)\.(\w+)\s*\(',
                r'await\s+(\w+)\s*\(',
            ],
            "java": [
                r'(\w+)\s*\(',
                r'(\w+)\.(\w+)\s*\(',
                r'this\.(\w+)\s*\(',
            ]
        }
    
    async def build_call_graph(self, source_files: List[SourceFile]) -> nx.DiGraph:
        """æ„å»ºè°ƒç”¨å›¾"""
        logger.info(f"å¼€å§‹æ„å»ºè°ƒç”¨å›¾ï¼Œæ–‡ä»¶æ•°: {len(source_files)}")
        
        # 1. æå–æ‰€æœ‰å‡½æ•°å®šä¹‰
        await self._extract_function_definitions(source_files)
        
        # 2. åˆ†æå‡½æ•°è°ƒç”¨å…³ç³»
        await self._analyze_function_calls(source_files)
        
        # 3. æ„å»ºå›¾ç»“æ„
        self._build_graph_structure()
        
        logger.info(f"è°ƒç”¨å›¾æ„å»ºå®Œæˆï¼ŒèŠ‚ç‚¹æ•°: {self.call_graph.number_of_nodes()}, è¾¹æ•°: {self.call_graph.number_of_edges()}")
        return self.call_graph
    
    async def _extract_function_definitions(self, source_files: List[SourceFile]):
        """æå–å‡½æ•°å®šä¹‰"""
        for source_file in source_files:
            try:
                # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©è§£ææ–¹æ³•
                if source_file.file_type == "python":
                    await self._extract_python_functions(source_file)
                elif source_file.file_type in ["javascript", "typescript"]:
                    await self._extract_js_functions(source_file)
                elif source_file.file_type == "java":
                    await self._extract_java_functions(source_file)
                else:
                    await self._extract_generic_functions(source_file)
                    
            except Exception as e:
                logger.warning(f"æå–å‡½æ•°å®šä¹‰å¤±è´¥: {source_file.path}, {e}")
    
    async def _extract_python_functions(self, source_file: SourceFile):
        """æå–Pythonå‡½æ•°å®šä¹‰"""
        try:
            tree = ast.parse(source_file.content)
            
            for node in ast.walk(tree):
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    func_name = f"{source_file.path}::{node.name}"
                    
                    # åˆ›å»ºä»£ç å•å…ƒ
                    code_unit = CodeUnit(
                        id=func_name,
                        name=node.name,
                        type="function",
                        content=ast.get_source_segment(source_file.content, node) or "",
                        start_line=node.lineno,
                        end_line=getattr(node, 'end_lineno', node.lineno),
                        source_file=source_file
                    )
                    
                    self.function_definitions[func_name] = code_unit
                    
        except SyntaxError as e:
            logger.warning(f"Pythonè¯­æ³•é”™è¯¯: {source_file.path}, {e}")
        except Exception as e:
            logger.warning(f"è§£æPythonæ–‡ä»¶å¤±è´¥: {source_file.path}, {e}")
    
    async def _extract_js_functions(self, source_file: SourceFile):
        """æå–JavaScript/TypeScriptå‡½æ•°å®šä¹‰"""
        content = source_file.content
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å‡½æ•°å®šä¹‰
        patterns = [
            r'function\s+(\w+)\s*\(',  # function declaration
            r'(\w+)\s*:\s*function\s*\(',  # object method
            r'(\w+)\s*=\s*function\s*\(',  # function expression
            r'(\w+)\s*=\s*\([^)]*\)\s*=>\s*{',  # arrow function
            r'async\s+function\s+(\w+)\s*\(',  # async function
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, content, re.MULTILINE)
            for match in matches:
                func_name = match.group(1)
                full_name = f"{source_file.path}::{func_name}"
                
                # ä¼°ç®—å‡½æ•°ä½ç½®
                start_pos = match.start()
                lines_before = content[:start_pos].count('\n')
                start_line = lines_before + 1
                
                code_unit = CodeUnit(
                    id=full_name,
                    name=func_name,
                    type="function",
                    content=self._extract_function_body(content, match.start()),
                    start_line=start_line,
                    end_line=start_line + 10,  # ä¼°ç®—
                    source_file=source_file
                )
                
                self.function_definitions[full_name] = code_unit
    
    async def _extract_java_functions(self, source_file: SourceFile):
        """æå–Javaæ–¹æ³•å®šä¹‰"""
        content = source_file.content
        
        # Javaæ–¹æ³•æ¨¡å¼
        method_pattern = r'(public|private|protected)?\s*(static)?\s*\w+\s+(\w+)\s*\([^)]*\)\s*{'
        
        matches = re.finditer(method_pattern, content, re.MULTILINE)
        for match in matches:
            method_name = match.group(3)
            full_name = f"{source_file.path}::{method_name}"
            
            start_pos = match.start()
            lines_before = content[:start_pos].count('\n')
            start_line = lines_before + 1
            
            code_unit = CodeUnit(
                id=full_name,
                name=method_name,
                type="method",
                content=self._extract_function_body(content, match.start()),
                start_line=start_line,
                end_line=start_line + 15,  # ä¼°ç®—
                source_file=source_file
            )
            
            self.function_definitions[full_name] = code_unit
    
    async def _extract_generic_functions(self, source_file: SourceFile):
        """é€šç”¨å‡½æ•°æå–"""
        content = source_file.content
        
        # é€šç”¨å‡½æ•°æ¨¡å¼
        patterns = [
            r'(\w+)\s*\([^)]*\)\s*{',  # åŸºæœ¬å‡½æ•°æ¨¡å¼
            r'def\s+(\w+)\s*\(',  # Python def
            r'function\s+(\w+)\s*\(',  # JavaScript function
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, content, re.MULTILINE)
            for match in matches:
                func_name = match.group(1)
                if func_name in ['if', 'for', 'while', 'switch']:  # æ’é™¤å…³é”®å­—
                    continue
                    
                full_name = f"{source_file.path}::{func_name}"
                
                start_pos = match.start()
                lines_before = content[:start_pos].count('\n')
                start_line = lines_before + 1
                
                code_unit = CodeUnit(
                    id=full_name,
                    name=func_name,
                    type="function",
                    content=self._extract_function_body(content, match.start()),
                    start_line=start_line,
                    end_line=start_line + 10,
                    source_file=source_file
                )
                
                self.function_definitions[full_name] = code_unit
    
    def _extract_function_body(self, content: str, start_pos: int, max_lines: int = 50) -> str:
        """æå–å‡½æ•°ä½“"""
        lines = content[start_pos:].split('\n')
        
        # ç®€å•çš„å¤§æ‹¬å·åŒ¹é…
        brace_count = 0
        function_lines = []
        
        for i, line in enumerate(lines):
            if i >= max_lines:
                break
                
            function_lines.append(line)
            brace_count += line.count('{') - line.count('}')
            
            if brace_count == 0 and i > 0:
                break
        
        return '\n'.join(function_lines)
    
    async def _analyze_function_calls(self, source_files: List[SourceFile]):
        """åˆ†æå‡½æ•°è°ƒç”¨å…³ç³»"""
        for source_file in source_files:
            try:
                file_type = source_file.file_type
                if file_type in self.call_patterns:
                    patterns = self.call_patterns[file_type]
                else:
                    patterns = self.call_patterns.get("python", [])  # é»˜è®¤ä½¿ç”¨Pythonæ¨¡å¼
                
                await self._find_function_calls(source_file, patterns)
                
            except Exception as e:
                logger.warning(f"åˆ†æå‡½æ•°è°ƒç”¨å¤±è´¥: {source_file.path}, {e}")
    
    async def _find_function_calls(self, source_file: SourceFile, patterns: List[str]):
        """æŸ¥æ‰¾å‡½æ•°è°ƒç”¨"""
        content = source_file.content
        
        for pattern in patterns:
            matches = re.finditer(pattern, content, re.MULTILINE)
            for match in matches:
                try:
                    # æå–è°ƒç”¨è€…å’Œè¢«è°ƒç”¨è€…
                    if match.lastindex == 1:  # ç®€å•å‡½æ•°è°ƒç”¨
                        callee = match.group(1)
                        caller = self._find_containing_function(source_file, match.start())
                    elif match.lastindex == 2:  # æ–¹æ³•è°ƒç”¨
                        obj = match.group(1)
                        method = match.group(2)
                        callee = f"{obj}.{method}"
                        caller = self._find_containing_function(source_file, match.start())
                    else:
                        continue
                    
                    if caller and callee:
                        self._add_call_relation(caller, callee, "direct", 0.9)
                        
                except Exception as e:
                    logger.debug(f"å¤„ç†å‡½æ•°è°ƒç”¨åŒ¹é…å¤±è´¥: {e}")
    
    def _find_containing_function(self, source_file: SourceFile, position: int) -> Optional[str]:
        """æŸ¥æ‰¾åŒ…å«æŒ‡å®šä½ç½®çš„å‡½æ•°"""
        lines_before = source_file.content[:position].count('\n')
        line_number = lines_before + 1
        
        # æŸ¥æ‰¾åŒ…å«è¯¥è¡Œçš„å‡½æ•°
        for func_name, code_unit in self.function_definitions.items():
            if (code_unit.source_file == source_file and 
                code_unit.start_line <= line_number <= code_unit.end_line):
                return func_name
        
        return None
    
    def _add_call_relation(self, caller: str, callee: str, call_type: str, confidence: float):
        """æ·»åŠ è°ƒç”¨å…³ç³»"""
        # è§£æå®Œæ•´çš„è¢«è°ƒç”¨è€…åç§°
        full_callee = self._resolve_function_name(callee, caller)
        
        if full_callee in self.function_definitions:
            relation = CallRelation(
                caller=caller,
                callee=full_callee,
                call_type=call_type,
                confidence=confidence
            )
            
            # æ·»åŠ åˆ°å›¾ä¸­
            self.call_graph.add_edge(caller, full_callee, relation=relation)
    
    def _resolve_function_name(self, callee: str, caller: str) -> str:
        """è§£æå‡½æ•°å…¨å"""
        # å¦‚æœå·²ç»æ˜¯å…¨åï¼Œç›´æ¥è¿”å›
        if "::" in callee:
            return callee
        
        # å°è¯•åœ¨åŒä¸€æ–‡ä»¶ä¸­æŸ¥æ‰¾
        caller_file = caller.split("::")[0] if "::" in caller else ""
        potential_name = f"{caller_file}::{callee}"
        
        if potential_name in self.function_definitions:
            return potential_name
        
        # åœ¨æ‰€æœ‰æ–‡ä»¶ä¸­æŸ¥æ‰¾
        for func_name in self.function_definitions:
            if func_name.endswith(f"::{callee}"):
                return func_name
        
        return callee  # è¿”å›åŸå
    
    def _build_graph_structure(self):
        """æ„å»ºå›¾ç»“æ„"""
        # æ·»åŠ æ‰€æœ‰å‡½æ•°èŠ‚ç‚¹
        for func_name, code_unit in self.function_definitions.items():
            self.call_graph.add_node(func_name, code_unit=code_unit)


class DataFlowAnalyzer:
    """æ•°æ®æµåˆ†æå™¨"""
    
    def __init__(self):
        self.taint_sources = self._init_taint_sources()
        self.taint_sinks = self._init_taint_sinks()
        self.sanitizers = self._init_sanitizers()
    
    def _init_taint_sources(self) -> Set[str]:
        """åˆå§‹åŒ–æ±¡ç‚¹æº"""
        return {
            # ç”¨æˆ·è¾“å…¥
            "input", "raw_input", "sys.argv", "request.form", "request.args",
            "request.json", "request.data", "request.files",
            # ç½‘ç»œè¾“å…¥
            "socket.recv", "urllib.request", "requests.get", "requests.post",
            # æ–‡ä»¶è¾“å…¥
            "open", "file.read", "os.environ"
        }
    
    def _init_taint_sinks(self) -> Set[str]:
        """åˆå§‹åŒ–æ±¡ç‚¹æ±‡èšç‚¹"""
        return {
            # SQLæ‰§è¡Œ
            "execute", "query", "cursor.execute", "db.query",
            # å‘½ä»¤æ‰§è¡Œ
            "os.system", "subprocess.call", "subprocess.run", "eval", "exec",
            # æ–‡ä»¶æ“ä½œ
            "open", "file.write", "os.remove", "shutil.rmtree",
            # ç½‘ç»œè¾“å‡º
            "response.write", "print", "render_template"
        }
    
    def _init_sanitizers(self) -> Set[str]:
        """åˆå§‹åŒ–å‡€åŒ–å‡½æ•°"""
        return {
            "escape", "quote", "sanitize", "validate", "filter",
            "html.escape", "urllib.parse.quote", "re.escape"
        }
    
    async def analyze_data_flow(self, call_graph: nx.DiGraph, 
                              vulnerability: VulnerabilityResult) -> List[DataFlow]:
        """åˆ†ææ•°æ®æµ"""
        logger.debug(f"å¼€å§‹æ•°æ®æµåˆ†æ: {vulnerability.id}")
        
        data_flows = []
        
        # æŸ¥æ‰¾æ¼æ´ç›¸å…³çš„å‡½æ•°
        vuln_function = self._find_vulnerability_function(call_graph, vulnerability)
        if not vuln_function:
            return data_flows
        
        # ä»æ±¡ç‚¹æºå¼€å§‹è¿½è¸ª
        for source in self.taint_sources:
            if source in vulnerability.snippet.lower():
                flows = await self._trace_taint_flow(call_graph, source, vuln_function)
                data_flows.extend(flows)
        
        return data_flows
    
    def _find_vulnerability_function(self, call_graph: nx.DiGraph, 
                                   vulnerability: VulnerabilityResult) -> Optional[str]:
        """æŸ¥æ‰¾æ¼æ´æ‰€åœ¨çš„å‡½æ•°"""
        for node in call_graph.nodes():
            code_unit = call_graph.nodes[node].get('code_unit')
            if (code_unit and 
                str(code_unit.source_file.path) == vulnerability.file_path and
                code_unit.start_line <= vulnerability.start_line <= code_unit.end_line):
                return node
        return None
    
    async def _trace_taint_flow(self, call_graph: nx.DiGraph, 
                              source: str, target: str) -> List[DataFlow]:
        """è¿½è¸ªæ±¡ç‚¹æµ"""
        flows = []
        
        try:
            # ä½¿ç”¨BFSæŸ¥æ‰¾ä»æºåˆ°ç›®æ ‡çš„è·¯å¾„
            if call_graph.has_node(target):
                # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„è·¯å¾„
                paths = self._find_taint_paths(call_graph, source, target)
                
                for path in paths:
                    # æ£€æŸ¥è·¯å¾„ä¸­æ˜¯å¦æœ‰å‡€åŒ–å‡½æ•°
                    taint_level = self._calculate_taint_level(path)
                    
                    flow = DataFlow(
                        source=source,
                        sink=target,
                        flow_path=path,
                        data_type="user_input",
                        taint_level=taint_level,
                        metadata={"path_length": len(path)}
                    )
                    flows.append(flow)
        
        except Exception as e:
            logger.warning(f"è¿½è¸ªæ±¡ç‚¹æµå¤±è´¥: {e}")
        
        return flows
    
    def _find_taint_paths(self, call_graph: nx.DiGraph, 
                         source: str, target: str, max_depth: int = 5) -> List[List[str]]:
        """æŸ¥æ‰¾æ±¡ç‚¹ä¼ æ’­è·¯å¾„"""
        paths = []
        
        # ä½¿ç”¨DFSæŸ¥æ‰¾è·¯å¾„
        def dfs(current: str, path: List[str], depth: int):
            if depth > max_depth:
                return
            
            if current == target:
                paths.append(path + [current])
                return
            
            if current in call_graph:
                for successor in call_graph.successors(current):
                    if successor not in path:  # é¿å…å¾ªç¯
                        dfs(successor, path + [current], depth + 1)
        
        # ä»åŒ…å«æºçš„å‡½æ•°å¼€å§‹æœç´¢
        for node in call_graph.nodes():
            code_unit = call_graph.nodes[node].get('code_unit')
            if code_unit and source in code_unit.content.lower():
                dfs(node, [], 0)
        
        return paths
    
    def _calculate_taint_level(self, path: List[str]) -> str:
        """è®¡ç®—æ±¡ç‚¹çº§åˆ«"""
        # æ£€æŸ¥è·¯å¾„ä¸­æ˜¯å¦æœ‰å‡€åŒ–å‡½æ•°
        for node in path:
            if any(sanitizer in node.lower() for sanitizer in self.sanitizers):
                return "low"
        
        # æ ¹æ®è·¯å¾„é•¿åº¦åˆ¤æ–­
        if len(path) <= 2:
            return "high"
        elif len(path) <= 4:
            return "medium"
        else:
            return "low"


class ImpactAnalyzer:
    """å½±å“é¢åˆ†æå™¨"""
    
    def __init__(self):
        self.risk_weights = {
            "critical_functions": 3.0,
            "public_apis": 2.5,
            "data_handlers": 2.0,
            "utility_functions": 1.0
        }
    
    async def analyze_impact_scope(self, call_graph: nx.DiGraph,
                                 vulnerability: VulnerabilityResult) -> ImpactScope:
        """åˆ†æå½±å“èŒƒå›´"""
        logger.debug(f"å¼€å§‹å½±å“é¢åˆ†æ: {vulnerability.id}")
        
        # æŸ¥æ‰¾æ¼æ´å‡½æ•°
        vuln_function = self._find_vulnerability_function(call_graph, vulnerability)
        if not vuln_function:
            return ImpactScope(
                affected_functions=set(),
                affected_files=set(),
                propagation_depth=0,
                risk_level="low",
                propagation_paths=[]
            )
        
        # åˆ†æå½±å“èŒƒå›´
        affected_functions = self._find_affected_functions(call_graph, vuln_function)
        affected_files = self._extract_affected_files(call_graph, affected_functions)
        propagation_paths = self._find_propagation_paths(call_graph, vuln_function)
        propagation_depth = max(len(path) for path in propagation_paths) if propagation_paths else 0
        risk_level = self._calculate_risk_level(affected_functions, propagation_depth)
        
        return ImpactScope(
            affected_functions=affected_functions,
            affected_files=affected_files,
            propagation_depth=propagation_depth,
            risk_level=risk_level,
            propagation_paths=propagation_paths
        )
    
    def _find_vulnerability_function(self, call_graph: nx.DiGraph,
                                   vulnerability: VulnerabilityResult) -> Optional[str]:
        """æŸ¥æ‰¾æ¼æ´å‡½æ•°"""
        for node in call_graph.nodes():
            code_unit = call_graph.nodes[node].get('code_unit')
            if (code_unit and 
                str(code_unit.source_file.path) == vulnerability.file_path and
                code_unit.start_line <= vulnerability.start_line <= code_unit.end_line):
                return node
        return None
    
    def _find_affected_functions(self, call_graph: nx.DiGraph, 
                               vuln_function: str, max_depth: int = 5) -> Set[str]:
        """æŸ¥æ‰¾å—å½±å“çš„å‡½æ•°"""
        affected = set()
        
        # å‘å‰ä¼ æ’­ï¼ˆè°ƒç”¨è€…ï¼‰
        predecessors = self._get_predecessors_bfs(call_graph, vuln_function, max_depth)
        affected.update(predecessors)
        
        # å‘åä¼ æ’­ï¼ˆè¢«è°ƒç”¨è€…ï¼‰
        successors = self._get_successors_bfs(call_graph, vuln_function, max_depth)
        affected.update(successors)
        
        affected.add(vuln_function)
        return affected
    
    def _get_predecessors_bfs(self, call_graph: nx.DiGraph, 
                            start: str, max_depth: int) -> Set[str]:
        """BFSè·å–å‰é©±èŠ‚ç‚¹"""
        visited = set()
        queue = deque([(start, 0)])
        
        while queue:
            node, depth = queue.popleft()
            if depth >= max_depth or node in visited:
                continue
                
            visited.add(node)
            
            for predecessor in call_graph.predecessors(node):
                if predecessor not in visited:
                    queue.append((predecessor, depth + 1))
        
        visited.discard(start)
        return visited
    
    def _get_successors_bfs(self, call_graph: nx.DiGraph, 
                          start: str, max_depth: int) -> Set[str]:
        """BFSè·å–åç»§èŠ‚ç‚¹"""
        visited = set()
        queue = deque([(start, 0)])
        
        while queue:
            node, depth = queue.popleft()
            if depth >= max_depth or node in visited:
                continue
                
            visited.add(node)
            
            for successor in call_graph.successors(node):
                if successor not in visited:
                    queue.append((successor, depth + 1))
        
        visited.discard(start)
        return visited
    
    def _extract_affected_files(self, call_graph: nx.DiGraph, 
                              affected_functions: Set[str]) -> Set[str]:
        """æå–å—å½±å“çš„æ–‡ä»¶"""
        affected_files = set()
        
        for func_name in affected_functions:
            if "::" in func_name:
                file_path = func_name.split("::")[0]
                affected_files.add(file_path)
        
        return affected_files
    
    def _find_propagation_paths(self, call_graph: nx.DiGraph, 
                              vuln_function: str, max_paths: int = 10) -> List[List[str]]:
        """æŸ¥æ‰¾ä¼ æ’­è·¯å¾„"""
        paths = []
        
        # æŸ¥æ‰¾ä»æ¼æ´å‡½æ•°å¼€å§‹çš„è·¯å¾„
        def dfs(current: str, path: List[str], depth: int):
            if depth > 5 or len(paths) >= max_paths:
                return
            
            path = path + [current]
            
            # å¦‚æœæ²¡æœ‰åç»§èŠ‚ç‚¹ï¼Œè¿™æ˜¯ä¸€æ¡å®Œæ•´è·¯å¾„
            successors = list(call_graph.successors(current))
            if not successors:
                paths.append(path)
                return
            
            for successor in successors:
                if successor not in path:  # é¿å…å¾ªç¯
                    dfs(successor, path, depth + 1)
        
        dfs(vuln_function, [], 0)
        return paths
    
    def _calculate_risk_level(self, affected_functions: Set[str], 
                            propagation_depth: int) -> str:
        """è®¡ç®—é£é™©çº§åˆ«"""
        # åŸºç¡€é£é™©åˆ†æ•°
        base_score = len(affected_functions) * 0.1 + propagation_depth * 0.2
        
        # æ ¹æ®å‡½æ•°ç±»å‹è°ƒæ•´æƒé‡
        weighted_score = base_score
        for func_name in affected_functions:
            if self._is_critical_function(func_name):
                weighted_score += self.risk_weights["critical_functions"]
            elif self._is_public_api(func_name):
                weighted_score += self.risk_weights["public_apis"]
            elif self._is_data_handler(func_name):
                weighted_score += self.risk_weights["data_handlers"]
        
        # ç¡®å®šé£é™©çº§åˆ«
        if weighted_score >= 8.0:
            return "critical"
        elif weighted_score >= 5.0:
            return "high"
        elif weighted_score >= 2.0:
            return "medium"
        else:
            return "low"
    
    def _is_critical_function(self, func_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå…³é”®å‡½æ•°"""
        critical_keywords = ["auth", "login", "password", "admin", "root", "exec", "eval"]
        return any(keyword in func_name.lower() for keyword in critical_keywords)
    
    def _is_public_api(self, func_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå…¬å…±API"""
        api_keywords = ["api", "endpoint", "route", "handler", "controller"]
        return any(keyword in func_name.lower() for keyword in api_keywords)
    
    def _is_data_handler(self, func_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ•°æ®å¤„ç†å‡½æ•°"""
        data_keywords = ["process", "parse", "validate", "sanitize", "transform"]
        return any(keyword in func_name.lower() for keyword in data_keywords)


class ContextExpander:
    """ä¸Šä¸‹æ–‡æ‰©å±•å™¨"""
    
    def __init__(self):
        self.semantic_analyzers = {
            "variable_usage": self._analyze_variable_usage,
            "control_flow": self._analyze_control_flow,
            "error_handling": self._analyze_error_handling,
            "security_patterns": self._analyze_security_patterns
        }
    
    async def expand_semantic_context(self, vulnerability: VulnerabilityResult,
                                    global_context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰©å±•è¯­ä¹‰ä¸Šä¸‹æ–‡"""
        semantic_context = {}
        
        # å¹¶è¡Œæ‰§è¡Œå„ç§è¯­ä¹‰åˆ†æ
        tasks = []
        for analyzer_name, analyzer_func in self.semantic_analyzers.items():
            task = asyncio.create_task(
                analyzer_func(vulnerability, global_context),
                name=analyzer_name
            )
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æ”¶é›†ç»“æœ
        for i, (analyzer_name, result) in enumerate(zip(self.semantic_analyzers.keys(), results)):
            if isinstance(result, Exception):
                logger.warning(f"è¯­ä¹‰åˆ†æå¤±è´¥ {analyzer_name}: {result}")
                semantic_context[analyzer_name] = {}
            else:
                semantic_context[analyzer_name] = result
        
        return semantic_context
    
    async def _analyze_variable_usage(self, vulnerability: VulnerabilityResult,
                                    global_context: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå˜é‡ä½¿ç”¨"""
        # æå–ä»£ç ç‰‡æ®µä¸­çš„å˜é‡
        variables = self._extract_variables(vulnerability.snippet)
        
        # åˆ†æå˜é‡çš„å®šä¹‰å’Œä½¿ç”¨
        usage_analysis = {}
        for var in variables:
            usage_analysis[var] = {
                "defined": self._is_variable_defined(var, vulnerability.snippet),
                "used": self._count_variable_usage(var, vulnerability.snippet),
                "type": self._infer_variable_type(var, vulnerability.snippet)
            }
        
        return {"variables": usage_analysis}
    
    async def _analyze_control_flow(self, vulnerability: VulnerabilityResult,
                                  global_context: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ§åˆ¶æµ"""
        snippet = vulnerability.snippet
        
        control_structures = {
            "conditionals": len(re.findall(r'\b(if|elif|else)\b', snippet)),
            "loops": len(re.findall(r'\b(for|while)\b', snippet)),
            "try_catch": len(re.findall(r'\b(try|except|catch|finally)\b', snippet)),
            "returns": len(re.findall(r'\breturn\b', snippet))
        }
        
        return {"control_flow": control_structures}
    
    async def _analyze_error_handling(self, vulnerability: VulnerabilityResult,
                                    global_context: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æé”™è¯¯å¤„ç†"""
        snippet = vulnerability.snippet
        
        error_handling = {
            "has_try_catch": bool(re.search(r'\b(try|except|catch)\b', snippet)),
            "has_error_logging": bool(re.search(r'\b(log|print|console)\b.*error', snippet, re.IGNORECASE)),
            "has_validation": bool(re.search(r'\b(validate|check|verify)\b', snippet, re.IGNORECASE)),
            "error_propagation": bool(re.search(r'\b(raise|throw)\b', snippet))
        }
        
        return {"error_handling": error_handling}
    
    async def _analyze_security_patterns(self, vulnerability: VulnerabilityResult,
                                       global_context: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå®‰å…¨æ¨¡å¼"""
        snippet = vulnerability.snippet.lower()
        
        security_patterns = {
            "input_validation": bool(re.search(r'\b(validate|sanitize|escape|filter)\b', snippet)),
            "authentication": bool(re.search(r'\b(auth|login|password|token)\b', snippet)),
            "authorization": bool(re.search(r'\b(permission|role|access|allow|deny)\b', snippet)),
            "encryption": bool(re.search(r'\b(encrypt|decrypt|hash|crypto)\b', snippet)),
            "sql_injection_protection": bool(re.search(r'\b(prepare|bind|param)\b', snippet)),
            "xss_protection": bool(re.search(r'\b(escape|encode|sanitize)\b.*html', snippet))
        }
        
        return {"security_patterns": security_patterns}
    
    def _extract_variables(self, code: str) -> List[str]:
        """æå–ä»£ç ä¸­çš„å˜é‡"""
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å˜é‡å
        variables = re.findall(r'\b([a-zA-Z_][a-zA-Z0-9_]*)\b', code)
        
        # è¿‡æ»¤å…³é”®å­—
        keywords = {'if', 'else', 'for', 'while', 'def', 'class', 'import', 'return', 'try', 'except'}
        variables = [var for var in variables if var not in keywords]
        
        return list(set(variables))  # å»é‡
    
    def _is_variable_defined(self, var: str, code: str) -> bool:
        """æ£€æŸ¥å˜é‡æ˜¯å¦è¢«å®šä¹‰"""
        patterns = [
            rf'\b{var}\s*=',  # èµ‹å€¼
            rf'\bdef\s+{var}\s*\(',  # å‡½æ•°å®šä¹‰
            rf'\bclass\s+{var}\b',  # ç±»å®šä¹‰
            rf'\bfor\s+{var}\b',  # forå¾ªç¯å˜é‡
        ]
        
        return any(re.search(pattern, code) for pattern in patterns)
    
    def _count_variable_usage(self, var: str, code: str) -> int:
        """è®¡ç®—å˜é‡ä½¿ç”¨æ¬¡æ•°"""
        return len(re.findall(rf'\b{var}\b', code))
    
    def _infer_variable_type(self, var: str, code: str) -> str:
        """æ¨æ–­å˜é‡ç±»å‹"""
        # ç®€å•çš„ç±»å‹æ¨æ–­
        if re.search(rf'{var}\s*=\s*\d+', code):
            return "integer"
        elif re.search(rf'{var}\s*=\s*["\']', code):
            return "string"
        elif re.search(rf'{var}\s*=\s*\[', code):
            return "list"
        elif re.search(rf'{var}\s*=\s*\{{', code):
            return "dict"
        else:
            return "unknown"


class R2REnhancer:
    """R2Rä»£ç ä¸Šä¸‹æ–‡å¢å¼ºå™¨ - å±‚çº§RAGæ¶æ„ç¬¬ä¸‰å±‚æ ¸å¿ƒç»„ä»¶"""
    
    def __init__(self):
        """åˆå§‹åŒ–R2Rå¢å¼ºå™¨"""
        # è·å–R2Rå±‚çš„æ¨¡å‹é…ç½®
        self.r2r_models = Config.get_r2r_models()
        self.context_model = self.r2r_models.get("context_model", "gpt-3.5-turbo@openai")
        self.enhancement_model = self.r2r_models.get("enhancement_model", "gpt-3.5-turbo@openai")
        
        logger.info(f"R2Rå¢å¼ºå™¨ä½¿ç”¨æ¨¡å‹ - ä¸Šä¸‹æ–‡: {self.context_model}, å¢å¼º: {self.enhancement_model}")
        
        self.call_graph_builder = CallGraphBuilder()
        self.dataflow_analyzer = DataFlowAnalyzer()
        self.impact_analyzer = ImpactAnalyzer()
        self.context_expander = ContextExpander()
        
        # ç¼“å­˜
        self.global_context_cache = {}
        
        logger.info("R2Rä¸Šä¸‹æ–‡å¢å¼ºå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def get_context_model(self) -> str:
        """è·å–ä¸Šä¸‹æ–‡åˆ†ææ¨¡å‹"""
        return self.context_model
    
    def get_enhancement_model(self) -> str:
        """è·å–å¢å¼ºæ¨¡å‹"""
        return self.enhancement_model
    
    async def _call_context_model(self, prompt: str, **kwargs) -> str:
        """è°ƒç”¨ä¸Šä¸‹æ–‡åˆ†ææ¨¡å‹"""
        start_time = time.time()
        
        try:
            from auditluma.utils import init_llm_client
            from auditluma.monitoring.model_usage_logger import model_usage_logger
            
            logger.info(f"ğŸ”— R2Rå¢å¼ºå±‚ - è°ƒç”¨ä¸Šä¸‹æ–‡åˆ†ææ¨¡å‹: {self.context_model}")
            logger.debug(f"ä¸Šä¸‹æ–‡åˆ†ææç¤ºé•¿åº¦: {len(prompt)} å­—ç¬¦")
            
            # ä½¿ç”¨é…ç½®çš„ä¸Šä¸‹æ–‡æ¨¡å‹
            llm_client = init_llm_client(self.context_model)
            response = await llm_client.generate_async(prompt, **kwargs)
            
            execution_time = time.time() - start_time
            
            # è®°å½•æ¨¡å‹ä½¿ç”¨
            model_usage_logger.log_model_usage(
                layer="r2r",
                component="R2REnhancer",
                model_name=self.context_model,
                operation="context_analysis",
                input_size=len(prompt),
                output_size=len(response),
                execution_time=execution_time,
                success=True
            )
            
            logger.info(f"âœ… R2Rå¢å¼ºå±‚ - ä¸Šä¸‹æ–‡æ¨¡å‹ {self.context_model} è°ƒç”¨æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
            return response
            
        except Exception as e:
            execution_time = time.time() - start_time
            
            # è®°å½•å¤±è´¥çš„æ¨¡å‹ä½¿ç”¨
            from auditluma.monitoring.model_usage_logger import model_usage_logger
            model_usage_logger.log_model_usage(
                layer="r2r",
                component="R2REnhancer",
                model_name=self.context_model,
                operation="context_analysis",
                input_size=len(prompt),
                output_size=0,
                execution_time=execution_time,
                success=False,
                error_message=str(e)
            )
            
            logger.error(f"âŒ R2Rå¢å¼ºå±‚ - è°ƒç”¨ä¸Šä¸‹æ–‡æ¨¡å‹ {self.context_model} å¤±è´¥: {e}")
            return ""
    
    async def _call_enhancement_model(self, prompt: str, **kwargs) -> str:
        """è°ƒç”¨å¢å¼ºæ¨¡å‹"""
        start_time = time.time()
        
        try:
            from auditluma.utils import init_llm_client
            from auditluma.monitoring.model_usage_logger import model_usage_logger
            
            logger.info(f"ğŸ”— R2Rå¢å¼ºå±‚ - è°ƒç”¨å¢å¼ºæ¨¡å‹: {self.enhancement_model}")
            logger.debug(f"å¢å¼ºæç¤ºé•¿åº¦: {len(prompt)} å­—ç¬¦")
            
            # ä½¿ç”¨é…ç½®çš„å¢å¼ºæ¨¡å‹
            llm_client = init_llm_client(self.enhancement_model)
            response = await llm_client.generate_async(prompt, **kwargs)
            
            execution_time = time.time() - start_time
            
            # è®°å½•æ¨¡å‹ä½¿ç”¨
            model_usage_logger.log_model_usage(
                layer="r2r",
                component="R2REnhancer",
                model_name=self.enhancement_model,
                operation="enhancement",
                input_size=len(prompt),
                output_size=len(response),
                execution_time=execution_time,
                success=True
            )
            
            logger.info(f"âœ… R2Rå¢å¼ºå±‚ - å¢å¼ºæ¨¡å‹ {self.enhancement_model} è°ƒç”¨æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
            return response
            
        except Exception as e:
            execution_time = time.time() - start_time
            
            # è®°å½•å¤±è´¥çš„æ¨¡å‹ä½¿ç”¨
            from auditluma.monitoring.model_usage_logger import model_usage_logger
            model_usage_logger.log_model_usage(
                layer="r2r",
                component="R2REnhancer",
                model_name=self.enhancement_model,
                operation="enhancement",
                input_size=len(prompt),
                output_size=0,
                execution_time=execution_time,
                success=False,
                error_message=str(e)
            )
            
            logger.error(f"âŒ R2Rå¢å¼ºå±‚ - è°ƒç”¨å¢å¼ºæ¨¡å‹ {self.enhancement_model} å¤±è´¥: {e}")
            return ""
    
    async def build_global_context(self, source_files: List[SourceFile]) -> Dict[str, Any]:
        """æ„å»ºå…¨å±€ä¸Šä¸‹æ–‡"""
        logger.info(f"å¼€å§‹æ„å»ºå…¨å±€ä¸Šä¸‹æ–‡ï¼Œæ–‡ä»¶æ•°: {len(source_files)}")
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._generate_cache_key(source_files)
        if cache_key in self.global_context_cache:
            logger.info("ä½¿ç”¨ç¼“å­˜çš„å…¨å±€ä¸Šä¸‹æ–‡")
            return self.global_context_cache[cache_key]
        
        # æ„å»ºè°ƒç”¨å›¾
        call_graph = await self.call_graph_builder.build_call_graph(source_files)
        
        # æ„å»ºå…¨å±€ä¸Šä¸‹æ–‡
        global_context = {
            "call_graph": call_graph,
            "source_files": source_files,
            "function_definitions": self.call_graph_builder.function_definitions,
            "file_count": len(source_files),
            "function_count": len(self.call_graph_builder.function_definitions),
            "call_relationships": call_graph.number_of_edges()
        }
        
        # ç¼“å­˜ç»“æœ
        self.global_context_cache[cache_key] = global_context
        
        logger.info(f"å…¨å±€ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆï¼Œå‡½æ•°æ•°: {global_context['function_count']}, è°ƒç”¨å…³ç³»: {global_context['call_relationships']}")
        return global_context
    
    async def enhance_context(self, vulnerability: VulnerabilityResult,
                            global_context: Dict[str, Any]) -> EnhancedContext:
        """å¢å¼ºä»£ç ä¸Šä¸‹æ–‡ - ä¸»è¦æ¥å£æ–¹æ³•"""
        logger.debug(f"å¼€å§‹å¢å¼ºä¸Šä¸‹æ–‡: {vulnerability.id}")
        
        call_graph = global_context.get("call_graph")
        if not call_graph:
            logger.warning("å…¨å±€ä¸Šä¸‹æ–‡ä¸­ç¼ºå°‘è°ƒç”¨å›¾")
            return self._create_empty_context()
        
        try:
            # å¹¶è¡Œæ‰§è¡Œå„ç§åˆ†æ
            tasks = [
                self._trace_call_dependencies(vulnerability, call_graph),
                self.dataflow_analyzer.analyze_data_flow(call_graph, vulnerability),
                self.impact_analyzer.analyze_impact_scope(call_graph, vulnerability),
                self.context_expander.expand_semantic_context(vulnerability, global_context)
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†ç»“æœ
            call_chain = results[0] if not isinstance(results[0], Exception) else []
            data_flow = results[1] if not isinstance(results[1], Exception) else []
            impact_scope = results[2] if not isinstance(results[2], Exception) else ImpactScope(
                affected_functions=set(), affected_files=set(), propagation_depth=0, 
                risk_level="low", propagation_paths=[]
            )
            semantic_context = results[3] if not isinstance(results[3], Exception) else {}
            
            # è®¡ç®—ä¸Šä¸‹æ–‡å®Œæ•´æ€§
            context_completeness = self._calculate_context_completeness(
                call_chain, data_flow, impact_scope, semantic_context
            )
            
            enhanced_context = EnhancedContext(
                call_chain=call_chain,
                data_flow=data_flow,
                impact_scope=impact_scope,
                semantic_context=semantic_context,
                context_completeness=context_completeness
            )
            
            logger.debug(f"ä¸Šä¸‹æ–‡å¢å¼ºå®Œæˆ: {vulnerability.id}, å®Œæ•´æ€§: {context_completeness:.2f}")
            return enhanced_context
            
        except Exception as e:
            logger.error(f"ä¸Šä¸‹æ–‡å¢å¼ºå¤±è´¥: {vulnerability.id}, {e}")
            return self._create_empty_context()
    
    async def _trace_call_dependencies(self, vulnerability: VulnerabilityResult,
                                     call_graph: nx.DiGraph) -> List[CallRelation]:
        """è¿½è¸ªè°ƒç”¨ä¾èµ–å…³ç³»"""
        call_relations = []
        
        # æŸ¥æ‰¾æ¼æ´æ‰€åœ¨çš„å‡½æ•°
        vuln_function = self._find_vulnerability_function(call_graph, vulnerability)
        if not vuln_function:
            return call_relations
        
        # è·å–è°ƒç”¨å…³ç³»
        for predecessor in call_graph.predecessors(vuln_function):
            edge_data = call_graph.get_edge_data(predecessor, vuln_function)
            if edge_data and 'relation' in edge_data:
                call_relations.append(edge_data['relation'])
        
        for successor in call_graph.successors(vuln_function):
            edge_data = call_graph.get_edge_data(vuln_function, successor)
            if edge_data and 'relation' in edge_data:
                call_relations.append(edge_data['relation'])
        
        return call_relations
    
    def _find_vulnerability_function(self, call_graph: nx.DiGraph,
                                   vulnerability: VulnerabilityResult) -> Optional[str]:
        """æŸ¥æ‰¾æ¼æ´æ‰€åœ¨çš„å‡½æ•°"""
        for node in call_graph.nodes():
            code_unit = call_graph.nodes[node].get('code_unit')
            if (code_unit and 
                str(code_unit.source_file.path) == vulnerability.file_path and
                code_unit.start_line <= vulnerability.start_line <= code_unit.end_line):
                return node
        return None
        
        # æ·»åŠ æ•°æ®æµä¿¡æ¯
        if enhanced_context.data_flow:
            flow_info = f"\næ•°æ®æµåˆ†æ: æ£€æµ‹åˆ° {len(enhanced_context.data_flow)} ä¸ªæ•°æ®æµè·¯å¾„"
            high_risk_flows = [f for f in enhanced_context.data_flow if f.taint_level == "high"]
            if high_risk_flows:
                flow_info += f"ï¼Œå…¶ä¸­ {len(high_risk_flows)} ä¸ªä¸ºé«˜é£é™©è·¯å¾„"
            enhanced_description += flow_info
        
        # æ·»åŠ å½±å“é¢ä¿¡æ¯
        impact_info = f"\nå½±å“é¢åˆ†æ: å½±å“ {len(enhanced_context.impact_scope.affected_functions)} ä¸ªå‡½æ•°ï¼Œ" \
                     f"{len(enhanced_context.impact_scope.affected_files)} ä¸ªæ–‡ä»¶ï¼Œ" \
                     f"ä¼ æ’­æ·±åº¦ {enhanced_context.impact_scope.propagation_depth}ï¼Œ" \
                     f"é£é™©çº§åˆ« {enhanced_context.impact_scope.risk_level}"
        enhanced_description += impact_info
        
        vulnerability.description = enhanced_description
        
        # æ›´æ–°å…ƒæ•°æ®
        if not hasattr(vulnerability, 'metadata'):
            vulnerability.metadata = {}
        
        vulnerability.metadata.update({
            "r2r_enhanced": True,
            "context_completeness": enhanced_context.context_completeness,
            "call_chain_length": len(enhanced_context.call_chain),
            "data_flow_count": len(enhanced_context.data_flow),
            "impact_scope": {
                "affected_functions": len(enhanced_context.impact_scope.affected_functions),
                "affected_files": len(enhanced_context.impact_scope.affected_files),
                "risk_level": enhanced_context.impact_scope.risk_level
            },
            "semantic_analysis": enhanced_context.semantic_context
        })
        
        return vulnerability
    
    async def _trace_call_dependencies(self, vulnerability: VulnerabilityResult,
                                     call_graph: nx.DiGraph) -> List[CallRelation]:
        """è¿½è¸ªè°ƒç”¨ä¾èµ–å…³ç³»"""
        call_relations = []
        
        # æŸ¥æ‰¾æ¼æ´å‡½æ•°
        vuln_function = self._find_vulnerability_function(call_graph, vulnerability)
        if not vuln_function:
            return call_relations
        
        # è·å–ç›´æ¥è°ƒç”¨å…³ç³»
        for predecessor in call_graph.predecessors(vuln_function):
            edge_data = call_graph.get_edge_data(predecessor, vuln_function)
            if edge_data and 'relation' in edge_data:
                call_relations.append(edge_data['relation'])
        
        for successor in call_graph.successors(vuln_function):
            edge_data = call_graph.get_edge_data(vuln_function, successor)
            if edge_data and 'relation' in edge_data:
                call_relations.append(edge_data['relation'])
        
        return call_relations
    
    def _find_vulnerability_function(self, call_graph: nx.DiGraph,
                                   vulnerability: VulnerabilityResult) -> Optional[str]:
        """æŸ¥æ‰¾æ¼æ´æ‰€åœ¨çš„å‡½æ•°"""
        for node in call_graph.nodes():
            code_unit = call_graph.nodes[node].get('code_unit')
            if (code_unit and 
                str(code_unit.source_file.path) == vulnerability.file_path and
                code_unit.start_line <= vulnerability.start_line <= code_unit.end_line):
                return node
        return None
    
    def _calculate_context_completeness(self, call_chain: List[CallRelation],
                                      data_flow: List[DataFlow],
                                      impact_scope: ImpactScope,
                                      semantic_context: Dict[str, Any]) -> float:
        """è®¡ç®—ä¸Šä¸‹æ–‡å®Œæ•´æ€§"""
        completeness_factors = []
        
        # è°ƒç”¨é“¾å®Œæ•´æ€§
        if call_chain:
            call_completeness = min(1.0, len(call_chain) / 5.0)  # å‡è®¾5ä¸ªè°ƒç”¨å…³ç³»ä¸ºå®Œæ•´
            completeness_factors.append(call_completeness)
        
        # æ•°æ®æµå®Œæ•´æ€§
        if data_flow:
            flow_completeness = min(1.0, len(data_flow) / 3.0)  # å‡è®¾3ä¸ªæ•°æ®æµä¸ºå®Œæ•´
            completeness_factors.append(flow_completeness)
        
        # å½±å“é¢å®Œæ•´æ€§
        if impact_scope.affected_functions:
            impact_completeness = min(1.0, len(impact_scope.affected_functions) / 10.0)
            completeness_factors.append(impact_completeness)
        
        # è¯­ä¹‰åˆ†æå®Œæ•´æ€§
        semantic_completeness = len(semantic_context) / 4.0  # 4ä¸ªåˆ†æå™¨
        completeness_factors.append(semantic_completeness)
        
        # è®¡ç®—å¹³å‡å®Œæ•´æ€§
        if completeness_factors:
            return sum(completeness_factors) / len(completeness_factors)
        else:
            return 0.0
    
    def _create_empty_context(self) -> EnhancedContext:
        """åˆ›å»ºç©ºçš„å¢å¼ºä¸Šä¸‹æ–‡"""
        return EnhancedContext(
            call_chain=[],
            data_flow=[],
            impact_scope=ImpactScope(
                affected_functions=set(),
                affected_files=set(),
                propagation_depth=0,
                risk_level="low",
                propagation_paths=[]
            ),
            semantic_context={},
            context_completeness=0.0
        )
    
    def _generate_cache_key(self, source_files: List[SourceFile]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        
        # ä½¿ç”¨æ–‡ä»¶è·¯å¾„å’Œä¿®æ”¹æ—¶é—´ç”Ÿæˆç¼“å­˜é”®
        file_info = []
        for source_file in source_files:
            file_info.append(f"{source_file.path}:{len(source_file.content)}")
        
        cache_content = "|".join(sorted(file_info))
        return hashlib.md5(cache_content.encode()).hexdigest()
    
    def clear_cache(self):
        """æ¸…ç†ç¼“å­˜"""
        self.global_context_cache.clear()
        logger.info("R2Rç¼“å­˜å·²æ¸…ç†")